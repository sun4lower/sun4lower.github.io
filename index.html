<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>Sunflower</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="本博客旨在分享学习Spark等大数据技术过程中的点点滴滴，提供一个交流学习的平台">
<meta property="og:type" content="website">
<meta property="og:title" content="Sunflower">
<meta property="og:url" content="http://www.sun4lower.cn/index.html">
<meta property="og:site_name" content="Sunflower">
<meta property="og:description" content="本博客旨在分享学习Spark等大数据技术过程中的点点滴滴，提供一个交流学习的平台">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sunflower">
<meta name="twitter:description" content="本博客旨在分享学习Spark等大数据技术过程中的点点滴滴，提供一个交流学习的平台">
    

    

    
        <link rel="icon" href="/sunflower.ico" />
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?4d423c10c1a9ce21e23383733c383235";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>

    


</head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Sunflower</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/.">Home</a>
                
                    <a class="main-nav-link" href="/archives">Archives</a>
                
                    <a class="main-nav-link" href="/categories">Categories</a>
                
                    <a class="main-nav-link" href="/tags">Tags</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/css/images/avatar.png" />
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/.">Home</a></td>
                
                    <td><a class="main-nav-link" href="/archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/css/images/avatar.png" />
            <h2 id="name">Sunflower</h2>
            <h3 id="title">Studying Spark &amp; Bigdata</h3>
            <span id="location"><i class="fa fa-map-marker"></i>ShangHai, China</span>
            <a id="follow" target="_blank" href="https://github.com/sun4lower/">FOLLOW</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                6
                <span>posts</span>
            </div>
            <div class="article-info-block">
                23
                <span>tags</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="http://github.com/sun4lower" target="_blank" title="github" class=tooltip>
                            <i class="fa fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="http://wpa.qq.com/msgrd?v=3&uin=1054844760&site=qq&menu=yes" target="_blank" title="qq" class=tooltip>
                            <i class="fa fa-qq"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="http://stackoverflow.com/users/7587826/sunflower" target="_blank" title="stack-overflow" class=tooltip>
                            <i class="fa fa-stack-overflow"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="mailto:sun4lower@163.com" target="_blank" title="send" class=tooltip>
                            <i class="fa fa-send"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main">
    <article id="post-sc-sparkshell" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2017/03/02/sc-sparkshell/">Spark-Core源码精读(2)、spark-shell(spark-submit)流程详解</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2017/03/02/sc-sparkshell/">
            <time datetime="2017-03-02T05:28:17.000Z" itemprop="datePublished">2017-03-02</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/sparkc/">spark-core</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/REPL/">REPL</a>, <a class="tag-link" href="/tags/SparkContext/">SparkContext</a>, <a class="tag-link" href="/tags/SqlContext/">SqlContext</a>, <a class="tag-link" href="/tags/spark/">spark</a>, <a class="tag-link" href="/tags/spark-class/">spark-class</a>, <a class="tag-link" href="/tags/sparkc/">spark-core</a>, <a class="tag-link" href="/tags/spark-shell/">spark-shell</a>, <a class="tag-link" href="/tags/spark-submit/">spark-submit</a>, <a class="tag-link" href="/tags/bigdata/">大数据</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>本文将解读使用spark-shell的方式进入REPL的具体流程。</p>
<p><strong>注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。</strong></p>
<h2 id="shell部分"><a href="#shell部分" class="headerlink" title="shell部分"></a>shell部分</h2><p>下面我们来看一下当我们输入 spark-shell –master spark://master:7077时具体的执行流程，首先当然是看一下spark-shell.sh的源码，我们只选取了相对比较重要的部分：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">##检测有没有设置SPARK_HOME环境变量，如果没有进行设置</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">export</span> SPARK_HOME=<span class="string">"<span class="variable">$(cd "`dirname "$0"`"/..; pwd)</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"><span class="comment">##...</span></div><div class="line"><span class="keyword">function</span> <span class="function"><span class="title">main</span></span>() &#123;</div><div class="line">  <span class="keyword">if</span> <span class="variable">$cygwin</span>; <span class="keyword">then</span></div><div class="line">    <span class="comment"># Workaround for issue involving JLine and Cygwin</span></div><div class="line">    <span class="comment"># (see http://sourceforge.net/p/jline/bugs/40/).</span></div><div class="line">    <span class="comment"># If you're using the Mintty terminal emulator in Cygwin, may need to set the</span></div><div class="line">    <span class="comment"># "Backspace sends ^H" setting in "Keys" section of the Mintty options</span></div><div class="line">    <span class="comment"># (see https://github.com/sbt/sbt/issues/562).</span></div><div class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</div><div class="line">    <span class="built_in">export</span> SPARK_SUBMIT_OPTS=<span class="string">"<span class="variable">$SPARK_SUBMIT_OPTS</span> -Djline.terminal=unix"</span></div><div class="line">    <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-submit --class org.apache.spark.repl.Main --name <span class="string">"Spark shell"</span> <span class="string">"<span class="variable">$@</span>"</span></div><div class="line">    stty icanon <span class="built_in">echo</span> &gt; /dev/null 2&gt;&amp;1</div><div class="line">  <span class="keyword">else</span></div><div class="line">    <span class="built_in">export</span> SPARK_SUBMIT_OPTS</div><div class="line">    <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-submit --class org.apache.spark.repl.Main --name <span class="string">"Spark shell"</span> <span class="string">"<span class="variable">$@</span>"</span></div><div class="line">  <span class="keyword">fi</span></div><div class="line">&#125;</div><div class="line"><span class="comment">##...</span></div><div class="line">main <span class="string">"<span class="variable">$@</span>"</span></div><div class="line"><span class="comment">##...</span></div></pre></td></tr></table></figure>
<p>可以看出最后执行的是main方法并传入我们使用spark-shell命令时候的所有参数，比如–master，而main方法中无论是什么操作系统(当然生产环境是linux系统)都会最终执行spark-submit，并且class为org.apache.spark.repl.Main、name为“Spark shell”并且将spark-shell所有接收到的用户输入的参数一起传进去，下面我们来看spark-submit：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">export</span> SPARK_HOME=<span class="string">"<span class="variable">$(cd "`dirname "$0"`"/..; pwd)</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment"># disable randomized hash for string in Python 3.3+</span></div><div class="line"><span class="built_in">export</span> PYTHONHASHSEED=0</div><div class="line"></div><div class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure>
<p>spark-submit的代码比较简洁，最后使用exec通过spark-class来启动SparkSubmit并将spark-submit接收到的所有参数传入，下面我们来看一下spark-class：(<em>这里要说明一下，从这里开始起始就是我们通过spark-submit提交application的过程，只不过spark-submit提交的application运行完成后就会结束，而REPL一直等待用户的输入</em>)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">export</span> SPARK_HOME=<span class="string">"<span class="variable">$(cd "`dirname "$0"`"/..; pwd)</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment">## 载入环境变量</span></div><div class="line">. <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/load-spark-env.sh</div><div class="line"></div><div class="line"><span class="comment">## 获得java的二进制文件，后面会用来启动一个JVM进行</span></div><div class="line"><span class="comment"># Find the java binary</span></div><div class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$&#123;JAVA_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  RUNNER=<span class="string">"<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/java"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">  <span class="keyword">if</span> [ `<span class="built_in">command</span> -v java` ]; <span class="keyword">then</span></div><div class="line">    RUNNER=<span class="string">"java"</span></div><div class="line">  <span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"JAVA_HOME is not set"</span> &gt;&amp;2</div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line">  <span class="keyword">fi</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment">## jar包的相关依赖</span></div><div class="line"><span class="comment"># Find assembly jar</span></div><div class="line">SPARK_ASSEMBLY_JAR=</div><div class="line"><span class="keyword">if</span> [ <span class="_">-f</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/RELEASE"</span> ]; <span class="keyword">then</span></div><div class="line">  ASSEMBLY_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/lib"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">  ASSEMBLY_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/assembly/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line">GREP_OPTIONS=</div><div class="line">num_jars=<span class="string">"<span class="variable">$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" | wc -l)</span>"</span></div><div class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$num_jars</span>"</span> <span class="_">-eq</span> <span class="string">"0"</span> <span class="_">-a</span> -z <span class="string">"<span class="variable">$SPARK_ASSEMBLY_JAR</span>"</span> <span class="_">-a</span> <span class="string">"<span class="variable">$SPARK_PREPEND_CLASSES</span>"</span> != <span class="string">"1"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">echo</span> <span class="string">"Failed to find Spark assembly in <span class="variable">$ASSEMBLY_DIR</span>."</span> 1&gt;&amp;2</div><div class="line">  <span class="built_in">echo</span> <span class="string">"You need to build Spark before running this program."</span> 1&gt;&amp;2</div><div class="line">  <span class="built_in">exit</span> 1</div><div class="line"><span class="keyword">fi</span></div><div class="line"><span class="keyword">if</span> [ <span class="_">-d</span> <span class="string">"<span class="variable">$ASSEMBLY_DIR</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  ASSEMBLY_JARS=<span class="string">"<span class="variable">$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" || true)</span>"</span></div><div class="line">  <span class="keyword">if</span> [ <span class="string">"<span class="variable">$num_jars</span>"</span> <span class="_">-gt</span> <span class="string">"1"</span> ]; <span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"Found multiple Spark assembly jars in <span class="variable">$ASSEMBLY_DIR</span>:"</span> 1&gt;&amp;2</div><div class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$ASSEMBLY_JARS</span>"</span> 1&gt;&amp;2</div><div class="line">    <span class="built_in">echo</span> <span class="string">"Please remove all but one jar."</span> 1&gt;&amp;2</div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line">  <span class="keyword">fi</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line">SPARK_ASSEMBLY_JAR=<span class="string">"<span class="variable">$&#123;ASSEMBLY_DIR&#125;</span>/<span class="variable">$&#123;ASSEMBLY_JARS&#125;</span>"</span></div><div class="line"></div><div class="line">LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$SPARK_ASSEMBLY_JAR</span>"</span></div><div class="line"></div><div class="line"><span class="comment"># Add the launcher build dir to the classpath if requested.</span></div><div class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$SPARK_PREPEND_CLASSES</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/launcher/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/classes:<span class="variable">$LAUNCH_CLASSPATH</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> _SPARK_ASSEMBLY=<span class="string">"<span class="variable">$SPARK_ASSEMBLY_JAR</span>"</span></div><div class="line"></div><div class="line"><span class="comment"># For tests</span></div><div class="line"><span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$SPARK_TESTING</span>"</span> ]]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">unset</span> YARN_CONF_DIR</div><div class="line">  <span class="built_in">unset</span> HADOOP_CONF_DIR</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment"># The launcher library will print arguments separated by a NULL character, to allow arguments with</span></div><div class="line"><span class="comment"># characters that would be otherwise interpreted by the shell. Read that in a while loop, populating</span></div><div class="line"><span class="comment"># an array that will be used to exec the final command.</span></div><div class="line">CMD=()</div><div class="line"><span class="keyword">while</span> IFS= <span class="built_in">read</span> <span class="_">-d</span> <span class="string">''</span> -r ARG; <span class="keyword">do</span></div><div class="line">  CMD+=(<span class="string">"<span class="variable">$ARG</span>"</span>)</div><div class="line">  <span class="comment">## 使用java -cp命令启动一个JVM进程并执行org.apache.spark.launcher.Main类的main方法，后面我们会看到这个进程就是SparkSubmit进程</span></div><div class="line"><span class="keyword">done</span> &lt; &lt;(<span class="string">"<span class="variable">$RUNNER</span>"</span> -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span>)</div><div class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$&#123;CMD[@]&#125;</span>"</span></div></pre></td></tr></table></figure>
<p>spark-class是Spark应用程序的命令行启动器，负责设置JVM环境并执行Spark的应用程序，这里我们执行的就是SparkSubmit，下面我们就进入到Spark源码的部分。</p>
<h2 id="Spark源码部分"><a href="#Spark源码部分" class="headerlink" title="Spark源码部分"></a>Spark源码部分</h2><p>承接上文，我们直接进入Spark的源码：</p>
<p>关于org.apache.spark.launcher.Main的源码我们这里不做说明，大家可以把它看成Spark应用程序命令行的启动器，我们主要关注Spark本身，所以直接进入SparkSubmit的源码部分：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">/** 使用SparkSubmitArguments封装spark-submit传入的参数，还记得都有什么吗？</span></div><div class="line">  如果是spark-shell，就包括spark-shell及后面的一串参数，如果是直接使用spark-submit进行提交</div><div class="line">  后面就是提交时传入的参数，由于SparkSubmitArguments中的参数比较多，本文中不再一一列出</div><div class="line">  会在使用到某个参数的时候进行说明，详细的参数可以参看SparkSubmitArguments的源码。</div><div class="line">  */</div><div class="line">  <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args)</div><div class="line">  <span class="comment">// 如果开启了debug模式就打印出参数</span></div><div class="line">  <span class="keyword">if</span> (appArgs.verbose) &#123;</div><div class="line">    <span class="comment">// scalastyle:off println</span></div><div class="line">    printStream.println(appArgs)</div><div class="line">    <span class="comment">// scalastyle:on println</span></div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">/** 这里的action就是spark-submit执行的动作，包括：SUBMIT, KILL, REQUEST_STATUS(使</span></div><div class="line">  用了SparkSubmitAction进行了封装)，如果没有指定，默认就是SparkSubmitAction.SUBMIT，</div><div class="line">  所以下面的这个模式匹配将执行submit(appArgs)</div><div class="line">  */</div><div class="line">  appArgs.action <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs)</div><div class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</div><div class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下面我们来看submit(appArgs)方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">submit方法的主要功能就是使用传进来的参数来提交应用程序。</div><div class="line">主要分为两步骤：</div><div class="line">1. 准备启动所需的环境，包括设置classpath、系统参数和应用程序的参数(根据部署模式和cluster</div><div class="line">manager运行child main类)</div><div class="line">2. 使用上一步准备好的环境调用child main class中的main函数，如果是spark-shell，child</div><div class="line">main class就是org.apache.spark.repl.Main，如果是spark-submit直接进行提交，child</div><div class="line">main class就是用户编写的应用程序(含有main方法的类)</div><div class="line">*/</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(args: <span class="type">SparkSubmitArguments</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">// 准备环境，主要就是获得childMainClass，即我们上面所说的child main class</span></div><div class="line">  <span class="keyword">val</span> (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)</div><div class="line">   <span class="comment">// 注意：源码中这里是doRunMain()方法，我们在后面单独拿出来进行分析</span></div><div class="line">   <span class="comment">// 判断gateway使用的是Akka还是基于REST的，但是不论那种方式最后都会调用doRunMain()方法</span></div><div class="line">   <span class="comment">// In standalone cluster mode, there are two submission gateways:</span></div><div class="line">   <span class="comment">//   (1) The traditional Akka gateway using o.a.s.deploy.Client as a wrapper</span></div><div class="line">   <span class="comment">//   (2) The new REST-based gateway introduced in Spark 1.3</span></div><div class="line">   <span class="comment">// The latter is the default behavior as of Spark 1.3, but Spark submit will fail over</span></div><div class="line">   <span class="comment">// to use the legacy gateway if the master endpoint turns out to be not a REST server.</span></div><div class="line">  <span class="keyword">if</span> (args.isStandaloneCluster &amp;&amp; args.useRest) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">// scalastyle:off println</span></div><div class="line">      printStream.println(<span class="string">"Running Spark using the REST application submission protocol."</span>)</div><div class="line">      <span class="comment">// scalastyle:on println</span></div><div class="line">      doRunMain()</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// Fail over to use the legacy submission gateway</span></div><div class="line">      <span class="keyword">case</span> e: <span class="type">SubmitRestConnectionException</span> =&gt;</div><div class="line">        printWarning(<span class="string">s"Master endpoint <span class="subst">$&#123;args.master&#125;</span> was not a REST server. "</span> +</div><div class="line">          <span class="string">"Falling back to legacy submission gateway instead."</span>)</div><div class="line">        args.useRest = <span class="literal">false</span></div><div class="line">        submit(args)</div><div class="line">    &#125;</div><div class="line">  <span class="comment">// In all other modes, just run the main class as prepared</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    doRunMain()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>doRunMain()的实现部分：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">doRunMain</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (args.proxyUser != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">// 这里是hadoop相关的用户和组的信息</span></div><div class="line">    <span class="keyword">val</span> proxyUser = <span class="type">UserGroupInformation</span>.createProxyUser(args.proxyUser,</div><div class="line">      <span class="type">UserGroupInformation</span>.getCurrentUser())</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      proxyUser.doAs(<span class="keyword">new</span> <span class="type">PrivilegedExceptionAction</span>[<span class="type">Unit</span>]() &#123;</div><div class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">          runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class="line">        &#125;</div><div class="line">      &#125;)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">        <span class="comment">// Hadoop's AuthorizationException suppresses the exception's stack trace, which</span></div><div class="line">        <span class="comment">// makes the message printed to the output by the JVM not very helpful. Instead,</span></div><div class="line">        <span class="comment">// detect exceptions with empty stack traces here, and treat them differently.</span></div><div class="line">        <span class="keyword">if</span> (e.getStackTrace().length == <span class="number">0</span>) &#123;</div><div class="line">          <span class="comment">// scalastyle:off println</span></div><div class="line">          printStream.println(<span class="string">s"ERROR: <span class="subst">$&#123;e.getClass().getName()&#125;</span>: <span class="subst">$&#123;e.getMessage()&#125;</span>"</span>)</div><div class="line">          <span class="comment">// scalastyle:on println</span></div><div class="line">          exitFn(<span class="number">1</span>)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">throw</span> e</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们看到doRunMain()内部最终都执行了runMain方法，所以我们进入runMain方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** 别看这个方法这么长，主要做的事情就是一件：运行child main class的main方法</span></div><div class="line">再次说明一下，如果是直接使用spark-submit提交的应用程序，就是执行用户指定的类的main方法</div><div class="line">如果是通过spark-shell执行的，就是执行org.apache.spark.repl.Main中的main方法</div><div class="line">*/</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runMain</span></span>(</div><div class="line">    childArgs: <span class="type">Seq</span>[<span class="type">String</span>],</div><div class="line">    childClasspath: <span class="type">Seq</span>[<span class="type">String</span>],</div><div class="line">    sysProps: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</div><div class="line">    childMainClass: <span class="type">String</span>,</div><div class="line">    verbose: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">//是否打印debug信息</span></div><div class="line">  <span class="comment">// scalastyle:off println</span></div><div class="line">  <span class="keyword">if</span> (verbose) &#123;</div><div class="line">    printStream.println(<span class="string">s"Main class:\n<span class="subst">$childMainClass</span>"</span>)</div><div class="line">    printStream.println(<span class="string">s"Arguments:\n<span class="subst">$&#123;childArgs.mkString("\n")&#125;</span>"</span>)</div><div class="line">    printStream.println(<span class="string">s"System properties:\n<span class="subst">$&#123;sysProps.mkString("\n")&#125;</span>"</span>)</div><div class="line">    printStream.println(<span class="string">s"Classpath elements:\n<span class="subst">$&#123;childClasspath.mkString("\n")&#125;</span>"</span>)</div><div class="line">    printStream.println(<span class="string">"\n"</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// scalastyle:on println</span></div><div class="line">  </div><div class="line">  <span class="comment">// 下面这些操作是指定当前运行线程的ClassLoader</span></div><div class="line">  <span class="keyword">val</span> loader =</div><div class="line">    <span class="keyword">if</span> (sysProps.getOrElse(<span class="string">"spark.driver.userClassPathFirst"</span>, <span class="string">"false"</span>).toBoolean) &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</div><div class="line">        <span class="type">Thread</span>.currentThread.getContextClassLoader)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</div><div class="line">        <span class="type">Thread</span>.currentThread.getContextClassLoader)</div><div class="line">    &#125;</div><div class="line">  <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</div><div class="line">  </div><div class="line">  <span class="comment">// 添加jar依赖</span></div><div class="line">  <span class="keyword">for</span> (jar &lt;- childClasspath) &#123;</div><div class="line">    addJarToClasspath(jar, loader)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 系统属性</span></div><div class="line">  <span class="keyword">for</span> ((key, value) &lt;- sysProps) &#123;</div><div class="line">    <span class="type">System</span>.setProperty(key, value)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">var</span> mainClass: <span class="type">Class</span>[_] = <span class="literal">null</span></div><div class="line">  <span class="comment">// 通过反射的方式获得mainClass(child main class)</span></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    mainClass = <span class="type">Utils</span>.classForName(childMainClass)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">ClassNotFoundException</span> =&gt;</div><div class="line">      e.printStackTrace(printStream)</div><div class="line">      <span class="keyword">if</span> (childMainClass.contains(<span class="string">"thriftserver"</span>)) &#123;</div><div class="line">        <span class="comment">// scalastyle:off println</span></div><div class="line">        printStream.println(<span class="string">s"Failed to load main class <span class="subst">$childMainClass</span>."</span>)</div><div class="line">        printStream.println(<span class="string">"You need to build Spark with -Phive and -Phive-thriftserver."</span>)</div><div class="line">        <span class="comment">// scalastyle:on println</span></div><div class="line">      &#125;</div><div class="line">      <span class="type">System</span>.exit(<span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span>)</div><div class="line">    <span class="keyword">case</span> e: <span class="type">NoClassDefFoundError</span> =&gt;</div><div class="line">      e.printStackTrace(printStream)</div><div class="line">      <span class="keyword">if</span> (e.getMessage.contains(<span class="string">"org/apache/hadoop/hive"</span>)) &#123;</div><div class="line">        <span class="comment">// scalastyle:off println</span></div><div class="line">        printStream.println(<span class="string">s"Failed to load hive class."</span>)</div><div class="line">        printStream.println(<span class="string">"You need to build Spark with -Phive and -Phive-thriftserver."</span>)</div><div class="line">        <span class="comment">// scalastyle:on println</span></div><div class="line">      &#125;</div><div class="line">      <span class="type">System</span>.exit(<span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// SPARK-4170</span></div><div class="line">  <span class="keyword">if</span> (classOf[scala.<span class="type">App</span>].isAssignableFrom(mainClass)) &#123;</div><div class="line">    printWarning(<span class="string">"Subclasses of scala.App may not work correctly. Use a main() method instead."</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 获得mainClass(child main class)的main方法</span></div><div class="line">  <span class="keyword">val</span> mainMethod = mainClass.getMethod(<span class="string">"main"</span>, <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">String</span>](<span class="number">0</span>).getClass)</div><div class="line">  <span class="comment">// main方法必须是static级别的</span></div><div class="line">  <span class="keyword">if</span> (!<span class="type">Modifier</span>.isStatic(mainMethod.getModifiers)) &#123;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"The main method in the given main class must be static"</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">findCause</span></span>(t: <span class="type">Throwable</span>): <span class="type">Throwable</span> = t <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">UndeclaredThrowableException</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (e.getCause() != <span class="literal">null</span>) findCause(e.getCause()) <span class="keyword">else</span> e</div><div class="line">    <span class="keyword">case</span> e: <span class="type">InvocationTargetException</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (e.getCause() != <span class="literal">null</span>) findCause(e.getCause()) <span class="keyword">else</span> e</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      e</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 最后调用main方法</span></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    mainMethod.invoke(<span class="literal">null</span>, childArgs.toArray)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">      findCause(t) <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">SparkUserAppException</span>(exitCode) =&gt;</div><div class="line">          <span class="type">System</span>.exit(exitCode)</div><div class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">          <span class="keyword">throw</span> t</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>走到这里，如果是用户通过spark-submit提交自己编写的spark application，那么就直接调用main方法，然后一步一步执行用户编写的代码:SparkContext等等，我们会在以后的文章中进行分析，所以我们现在要跟随的就是org.apache.spark.repl.Main中的main方法，这里我们贴出SparkSubmit进程中主线程的thread dump：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">java.io.FileInputStream.read0(Native Method)</div><div class="line">java.io.FileInputStream.read(FileInputStream.java:207)</div><div class="line">scala.tools.jline.TerminalSupport.readCharacter(TerminalSupport.java:152)</div><div class="line">scala.tools.jline.UnixTerminal.readVirtualKey(UnixTerminal.java:125)</div><div class="line">scala.tools.jline.console.ConsoleReader.readVirtualKey(ConsoleReader.java:933)</div><div class="line">scala.tools.jline.console.ConsoleReader.readBinding(ConsoleReader.java:1136)</div><div class="line">scala.tools.jline.console.ConsoleReader.readLine(ConsoleReader.java:1218)</div><div class="line">scala.tools.jline.console.ConsoleReader.readLine(ConsoleReader.java:1170)</div><div class="line">org.apache.spark.repl.SparkJLineReader.readOneLine(SparkJLineReader.scala:80)</div><div class="line">scala.tools.nsc.interpreter.InteractiveReader<span class="variable">$class</span>.readLine(InteractiveReader.scala:43) </div><div class="line">org.apache.spark.repl.SparkJLineReader.readLine(SparkJLineReader.scala:25) </div><div class="line">org.apache.spark.repl.SparkILoop.readOneLine<span class="variable">$1</span>(SparkILoop.scala:648) </div><div class="line">org.apache.spark.repl.SparkILoop.innerLoop<span class="variable">$1</span>(SparkILoop.scala:665) </div><div class="line">org.apache.spark.repl.SparkILoop.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$loop</span>(SparkILoop.scala:670)</div><div class="line">org.apache.spark.repl.SparkILoop$<span class="variable">$anonfun</span><span class="variable">$org</span><span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$process</span><span class="variable">$1</span>.apply<span class="variable">$mcZ</span><span class="variable">$sp</span>(SparkILoop.scala:997)</div><div class="line">org.apache.spark.repl.SparkILoop$<span class="variable">$anonfun</span><span class="variable">$org</span><span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$process</span><span class="variable">$1</span>.apply(SparkILoop.scala:945)</div><div class="line">org.apache.spark.repl.SparkILoop$<span class="variable">$anonfun</span><span class="variable">$org</span><span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$process</span><span class="variable">$1</span>.apply(SparkILoop.scala:945) </div><div class="line">scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) </div><div class="line">org.apache.spark.repl.SparkILoop.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$process</span>(SparkILoop.scala:945)</div><div class="line">org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059) </div><div class="line">org.apache.spark.repl.Main$.main(Main.scala:31)</div><div class="line">org.apache.spark.repl.Main.main(Main.scala) </div><div class="line">sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) </div><div class="line">sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) </div><div class="line">sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) </div><div class="line">java.lang.reflect.Method.invoke(Method.java:498) </div><div class="line">org.apache.spark.deploy.SparkSubmit$.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$deploy</span><span class="variable">$SparkSubmit</span>$<span class="variable">$runMain</span>(SparkSubmit.scala:731) </div><div class="line">org.apache.spark.deploy.SparkSubmit$.doRunMain<span class="variable">$1</span>(SparkSubmit.scala:181) </div><div class="line">org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) </div><div class="line">org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) </div><div class="line">org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</div></pre></td></tr></table></figure>
<p>在这个时候贴出来就是为了承上启下，我们可以清楚的看见(注意是从最后一行往上看)前面我们分析的过程，从SparkSubmit的main方法到submit、doRunMain、runMain到最后通过反射的方式调用org.apache.spark.repl.Main的main方法，整个流程都看的很清楚，所以下面我们进入org.apache.spark.repl.Main的main方法(包含了初始化的操作)：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 实例化SparkConf</span></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line"><span class="comment">// 设置各种文件路径</span></div><div class="line"><span class="keyword">val</span> tmp = <span class="type">System</span>.getProperty(<span class="string">"java.io.tmpdir"</span>)</div><div class="line"><span class="keyword">val</span> rootDir = conf.get(<span class="string">"spark.repl.classdir"</span>, tmp)</div><div class="line"><span class="keyword">val</span> outputDir = <span class="type">Utils</span>.createTempDir(rootDir)</div><div class="line"><span class="keyword">val</span> s = <span class="keyword">new</span> <span class="type">Settings</span>()</div><div class="line">s.processArguments(<span class="type">List</span>(<span class="string">"-Yrepl-class-based"</span>,</div><div class="line">  <span class="string">"-Yrepl-outdir"</span>, <span class="string">s"<span class="subst">$&#123;outputDir.getAbsolutePath&#125;</span>"</span>,</div><div class="line">  <span class="string">"-classpath"</span>, getAddedJars.mkString(<span class="type">File</span>.pathSeparator)), <span class="literal">true</span>)</div><div class="line"><span class="comment">// the creation of SecurityManager has to be lazy so SPARK_YARN_MODE is set if needed</span></div><div class="line"><span class="keyword">val</span> classServerPort = conf.getInt(<span class="string">"spark.replClassServer.port"</span>, <span class="number">0</span>)</div><div class="line"><span class="comment">// 实例化了HttpServer，注意这里是lazy级别的</span></div><div class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> classServer =</div><div class="line">  <span class="keyword">new</span> <span class="type">HttpServer</span>(conf, outputDir, <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf), classServerPort, <span class="string">"HTTP class server"</span>)</div><div class="line"><span class="keyword">var</span> sparkContext: <span class="type">SparkContext</span> = _</div><div class="line"><span class="keyword">var</span> sqlContext: <span class="type">SQLContext</span> = _</div><div class="line"><span class="comment">// 实例化了SparkILoop，接下来会详细的分析</span></div><div class="line"><span class="keyword">var</span> interp = <span class="keyword">new</span> <span class="type">SparkILoop</span> <span class="comment">// this is a public var because tests reset it.</span></div><div class="line"><span class="comment">// 执行一些初始化的处理后就执行main方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">  <span class="comment">// 判断是否为yarn的模式，我们在以后的文章中会专门的分析yarn的部署模式</span></div><div class="line">  <span class="keyword">if</span> (getMaster == <span class="string">"yarn-client"</span>) <span class="type">System</span>.setProperty(<span class="string">"SPARK_YARN_MODE"</span>, <span class="string">"true"</span>)</div><div class="line">  <span class="comment">// Start the classServer and store its URI in a spark system property</span></div><div class="line">  <span class="comment">// (which will be passed to executors so that they can connect to it)</span></div><div class="line">  <span class="comment">// 启动HTTP server</span></div><div class="line">  classServer.start()</div><div class="line">  <span class="comment">// 最关键的代码，让解释器循环执行，即REPL</span></div><div class="line">  interp.process(s) <span class="comment">// Repl starts and goes in loop of R.E.P.L</span></div><div class="line">  classServer.stop()</div><div class="line">  <span class="type">Option</span>(sparkContext).map(_.stop)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>写到这里我们再来贴出通过spark-shell进入REPL时打印的部分日志：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">17/02/21 13:40:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</div><div class="line">17/02/21 13:40:17 INFO spark.SecurityManager: Changing view acls to: root</div><div class="line">17/02/21 13:40:17 INFO spark.SecurityManager: Changing modify acls to: root</div><div class="line">17/02/21 13:40:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)</div><div class="line">17/02/21 13:40:18 INFO spark.HttpServer: Starting HTTP Server</div><div class="line">17/02/21 13:40:18 INFO server.Server: jetty-8.y.z-SNAPSHOT</div><div class="line">17/02/21 13:40:18 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:43773</div><div class="line">17/02/21 13:40:18 INFO util.Utils: Successfully started service <span class="string">'HTTP class server'</span> on port 43773.</div></pre></td></tr></table></figure>
<p>上面这段日志其实对应的就是classServer.start()的部分，以后我们再看到这些日志的时候就知道背后到底发生了什么，是不是很有成就感？</p>
<p>下面就进入SparkILoop和ILoop的部分(SparkILoop是继承自ILoop类，而SparkILoop中没有process方法，所以调用的实际上是ILoop类中的process方法)：</p>
<p>ILoop</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 启动解释器，用来解释用户输入的command</span></div><div class="line"><span class="comment">// start an interpreter with the given settings</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(settings: <span class="type">Settings</span>): <span class="type">Boolean</span> = savingContextLoader &#123;</div><div class="line">  <span class="keyword">this</span>.settings = settings</div><div class="line">  <span class="comment">// 创建解释器，内部其实是实例化了一个ILoopInterpreter</span></div><div class="line">  createInterpreter()</div><div class="line">  <span class="comment">// sets in to some kind of reader depending on environmental cues</span></div><div class="line">  in = in0.fold(chooseReader(settings))(r =&gt; <span class="type">SimpleReader</span>(r, out, interactive = <span class="literal">true</span>))</div><div class="line">  globalFuture = future &#123;</div><div class="line">    intp.initializeSynchronous()</div><div class="line">    loopPostInit()</div><div class="line">    !intp.reporter.hasErrors</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 这里应该调用的是其子类SparkILoop的loadFiles方法，而SparkILoop的loadFiles方法内部最后又会调用这里的loadFiles方法</span></div><div class="line">  loadFiles(settings)</div><div class="line">  printWelcome()</div><div class="line">  <span class="comment">// 一直循环接收用户输入的command</span></div><div class="line">  <span class="keyword">try</span> loop() <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">LineResults</span>.<span class="type">EOF</span> =&gt; out print <span class="type">Properties</span>.shellInterruptedString</div><div class="line">    <span class="keyword">case</span> _               =&gt;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">catch</span> <span class="type">AbstractOrMissingHandler</span>()</div><div class="line">  <span class="keyword">finally</span> closeInterpreter()</div><div class="line">  <span class="literal">true</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们先来看一下SparkILoop的loadFiles方法都做了什么：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">loadFiles</span></span>(settings: <span class="type">Settings</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  initializeSpark()</div><div class="line">  <span class="keyword">super</span>.loadFiles(settings)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到首先调用initializeSpark()方法，然后调用父类的loadFiles方法，目的就是先准备好SparkContext、SQLContext然后再执行后面的操作，方便我们在进入到REPL后直接可以访问sc、sqlContext等，所以我们现在明白了为什么我们可以直接在spark-shell中直接访问sc、sqlContext了(成就感爆棚有木有？)。说了这么多，我们看一下initializeSpark()的庐山真面目：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">def initializeSpark() &#123;</div><div class="line">  intp.beQuietDuring &#123;</div><div class="line">    processLine("""</div><div class="line">       @transient val sc = &#123;</div><div class="line">         val _sc = org.apache.spark.repl.Main.createSparkContext()</div><div class="line">         println("Spark context available as sc.")</div><div class="line">         _sc</div><div class="line">       &#125;</div><div class="line">      """)</div><div class="line">    processLine("""</div><div class="line">       @transient val sqlContext = &#123;</div><div class="line">         val _sqlContext = org.apache.spark.repl.Main.createSQLContext()</div><div class="line">         println("SQL context available as sqlContext.")</div><div class="line">         _sqlContext</div><div class="line">       &#125;</div><div class="line">      """)</div><div class="line">    processLine("import org.apache.spark.SparkContext._")</div><div class="line">    processLine("import sqlContext.implicits._")</div><div class="line">    processLine("import sqlContext.sql")</div><div class="line">    processLine("import org.apache.spark.sql.functions._")</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里写的就非常清楚了通过processLine来创建SparkContext、SQLContext并导入一些经常使用的包，都准备完成后再调用父类的loadFiles，然后调用printWelcome()，注意这里调用的是SparkILoop的printWelcome()方法：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">/** Print a welcome message */</div><div class="line">override def printWelcome() &#123;</div><div class="line">  import org.apache.spark.SPARK_VERSION</div><div class="line">  echo("""Welcome to</div><div class="line">    ____              __</div><div class="line">   / __/__  ___ _____/ /__</div><div class="line">  _\ \/ _ \/ _ `/ __/  '_/</div><div class="line"> /___/ .__/\_,_/_/ /_/\_\   version %s</div><div class="line">    /_/</div><div class="line">       """.format(SPARK_VERSION))</div><div class="line">  val welcomeMsg = "Using Scala %s (%s, Java %s)".format(</div><div class="line">    versionString, javaVmName, javaVersion)</div><div class="line">  echo(welcomeMsg)</div><div class="line">  echo("Type in expressions to have them evaluated.")</div><div class="line">  echo("Type :help for more information.")</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>咦？这货看着是不是很眼熟，对，这就是spark-shell中打印的日志：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div></pre></td><td class="code"><pre><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">'_/</span></div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.6.3</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111)</div><div class="line">Type in expressions to have them evaluated.</div><div class="line">Type :help for more information.</div><div class="line">17/02/21 13:40:24 INFO spark.SparkContext: Running Spark version 1.6.3</div><div class="line">17/02/21 13:40:24 INFO spark.SecurityManager: Changing view acls to: root</div><div class="line">17/02/21 13:40:24 INFO spark.SecurityManager: Changing modify acls to: root</div><div class="line">17/02/21 13:40:24 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)</div><div class="line">17/02/21 13:40:25 INFO util.Utils: Successfully started service 'sparkDriver<span class="string">' on port 38463.</span></div><div class="line">17/02/21 13:40:26 INFO slf4j.Slf4jLogger: Slf4jLogger started</div><div class="line">17/02/21 13:40:26 INFO Remoting: Starting remoting</div><div class="line">17/02/21 13:40:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:37221]</div><div class="line">17/02/21 13:40:26 INFO util.Utils: Successfully started service 'sparkDriverActorSystem<span class="string">' on port 37221.</span></div><div class="line">17/02/21 13:40:26 INFO spark.SparkEnv: Registering MapOutputTracker</div><div class="line">17/02/21 13:40:26 INFO spark.SparkEnv: Registering BlockManagerMaster</div><div class="line">17/02/21 13:40:26 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-a06685a8-6f1c-4e8f-805c-e232333f8d85</div><div class="line">17/02/21 13:40:26 INFO storage.MemoryStore: MemoryStore started with capacity 511.1 MB</div><div class="line">17/02/21 13:40:27 INFO spark.SparkEnv: Registering OutputCommitCoordinator</div><div class="line">17/02/21 13:40:27 INFO server.Server: jetty-8.y.z-SNAPSHOT</div><div class="line">17/02/21 13:40:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040</div><div class="line">17/02/21 13:40:27 INFO util.Utils: Successfully started service 'SparkUI<span class="string">' on port 4040.</span></div><div class="line">17/02/21 13:40:27 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:4040</div><div class="line">17/02/21 13:40:27 INFO client.AppClient$ClientEndpoint: Connecting to master spark://master:7077...</div><div class="line">17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20170221134027-0000</div><div class="line">17/02/21 13:40:28 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService<span class="string">' on port 44615.</span></div><div class="line">17/02/21 13:40:28 INFO netty.NettyBlockTransferService: Server created on 44615</div><div class="line">17/02/21 13:40:28 INFO storage.BlockManagerMaster: Trying to register BlockManager</div><div class="line">17/02/21 13:40:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:44615 with 511.1 MB RAM, BlockManagerId(driver, 172.17.0.2, 44615)</div><div class="line">17/02/21 13:40:28 INFO storage.BlockManagerMaster: Registered BlockManager</div><div class="line">17/02/21 13:40:28 INFO client.AppClient$ClientEndpoint: Executor added: app-20170221134027-0000/0 on worker-20170221133811-172.17.0.3-41829 (172.17.0.3:41829) with 2 cores</div><div class="line">17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20170221134027-0000/0 on hostPort 172.17.0.3:41829 with 2 cores, 1024.0 MB RAM</div><div class="line">17/02/21 13:40:28 INFO client.AppClient$ClientEndpoint: Executor added: app-20170221134027-0000/1 on worker-20170221133810-172.17.0.4-39901 (172.17.0.4:39901) with 2 cores</div><div class="line">17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20170221134027-0000/1 on hostPort 172.17.0.4:39901 with 2 cores, 1024.0 MB RAM</div><div class="line">17/02/21 13:40:29 INFO client.AppClient$ClientEndpoint: Executor updated: app-20170221134027-0000/1 is now RUNNING</div><div class="line">17/02/21 13:40:29 INFO client.AppClient$ClientEndpoint: Executor updated: app-20170221134027-0000/0 is now RUNNING</div><div class="line">17/02/21 13:40:45 INFO scheduler.EventLoggingListener: Logging events to hdfs://master:9000/historyserverforspark/app-20170221134027-0000</div><div class="line">17/02/21 13:40:45 INFO cluster.SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0</div><div class="line">17/02/21 13:40:45 INFO repl.SparkILoop: Created spark context..</div><div class="line">Spark context available as sc.</div><div class="line">17/02/21 13:40:46 INFO cluster.SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (worker1:60096) with ID 0</div><div class="line">17/02/21 13:40:46 INFO cluster.SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (worker2:46846) with ID 1</div><div class="line">17/02/21 13:40:47 INFO storage.BlockManagerMasterEndpoint: Registering block manager worker1:39275 with 511.1 MB RAM, BlockManagerId(0, worker1, 39275)</div><div class="line">17/02/21 13:40:47 INFO storage.BlockManagerMasterEndpoint: Registering block manager worker2:37449 with 511.1 MB RAM, BlockManagerId(1, worker2, 37449)</div><div class="line">17/02/21 13:40:50 INFO hive.HiveContext: Initializing execution hive, version 1.2.1</div><div class="line">17/02/21 13:40:51 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0</div><div class="line">17/02/21 13:40:51 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0</div><div class="line">17/02/21 13:40:52 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</div><div class="line">17/02/21 13:40:53 INFO metastore.ObjectStore: ObjectStore, initialize called</div><div class="line">17/02/21 13:40:53 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</div><div class="line">17/02/21 13:40:53 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored</div><div class="line">17/02/21 13:40:54 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)</div><div class="line">17/02/21 13:40:55 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)</div><div class="line">17/02/21 13:41:01 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"</div><div class="line">17/02/21 13:41:08 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:08 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:15 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:15 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:17 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</div><div class="line">17/02/21 13:41:17 INFO metastore.ObjectStore: Initialized ObjectStore</div><div class="line">17/02/21 13:41:18 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0</div><div class="line">17/02/21 13:41:18 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException</div><div class="line">17/02/21 13:41:19 INFO metastore.HiveMetaStore: Added admin role in metastore</div><div class="line">17/02/21 13:41:19 INFO metastore.HiveMetaStore: Added public role in metastore</div><div class="line">17/02/21 13:41:19 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty</div><div class="line">17/02/21 13:41:20 INFO metastore.HiveMetaStore: 0: get_all_databases</div><div class="line">17/02/21 13:41:20 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	</div><div class="line">17/02/21 13:41:20 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*</div><div class="line">17/02/21 13:41:20 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	</div><div class="line">17/02/21 13:41:20 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:21 INFO session.SessionState: Created local directory: /tmp/939dedb5-f724-461b-a41a-a5fd1fe7324b_resources</div><div class="line">17/02/21 13:41:21 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/939dedb5-f724-461b-a41a-a5fd1fe7324b</div><div class="line">17/02/21 13:41:21 INFO session.SessionState: Created local directory: /tmp/root/939dedb5-f724-461b-a41a-a5fd1fe7324b</div><div class="line">17/02/21 13:41:21 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/939dedb5-f724-461b-a41a-a5fd1fe7324b/_tmp_space.db</div><div class="line">17/02/21 13:41:22 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse</div><div class="line">17/02/21 13:41:22 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.</div><div class="line">17/02/21 13:41:22 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0</div><div class="line">17/02/21 13:41:22 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0</div><div class="line">17/02/21 13:41:23 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</div><div class="line">17/02/21 13:41:23 INFO metastore.ObjectStore: ObjectStore, initialize called</div><div class="line">17/02/21 13:41:23 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</div><div class="line">17/02/21 13:41:23 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored</div><div class="line">17/02/21 13:41:24 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)</div><div class="line">17/02/21 13:41:24 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)</div><div class="line">17/02/21 13:41:25 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"</div><div class="line">17/02/21 13:41:29 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:29 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:29 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:29 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:30 INFO DataNucleus.Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing</div><div class="line">17/02/21 13:41:30 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</div><div class="line">17/02/21 13:41:30 INFO metastore.ObjectStore: Initialized ObjectStore</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: Added admin role in metastore</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: Added public role in metastore</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: 0: get_all_databases</div><div class="line">17/02/21 13:41:30 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*</div><div class="line">17/02/21 13:41:30 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	</div><div class="line">17/02/21 13:41:30 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:30 INFO session.SessionState: Created local directory: /tmp/c9c26571-1229-4786-8a8e-d7b090b07d85_resources</div><div class="line">17/02/21 13:41:30 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/c9c26571-1229-4786-8a8e-d7b090b07d85</div><div class="line">17/02/21 13:41:30 INFO session.SessionState: Created local directory: /tmp/root/c9c26571-1229-4786-8a8e-d7b090b07d85</div><div class="line">17/02/21 13:41:30 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/c9c26571-1229-4786-8a8e-d7b090b07d85/_tmp_space.db</div><div class="line">17/02/21 13:41:30 INFO repl.SparkILoop: Created sql context (with Hive support)..</div><div class="line">SQL context available as sqlContext.</div></pre></td></tr></table></figure>
<p>Welcome后面的一大串就是上面initializeSpark()执行打的日志信息，现在所有的日志信息都“名花有主”了，我们会单独拿出文章来分析SparkContext、SQLContext的创建流程，下面我们看process方法中最后就一直进行loop操作，这里我们不再深入的分析下去了，我们要适可而止，否则会迷失在源码中，大家可以简单的理解其实这里的循环过程就是REPL所代表的意思，即Read：读取用户输入的command；Evaluation：通过Spark Framework执行command；P：print打计算结果；L：loop循环前面的流程，同时在读取command后需要进行语法解析，然后用解释器执行，有兴趣的朋友可以继续跟随源码走下去。</p>
<p>至此我们走完了整个spark-shell(包括spark-submit)的整个流程，下面用一张图简单的总结一下：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/006y2nc1ly1fd8fholzmbj30xe16577s.jpg" alt=""></p>
<p>本文参考和拓展阅读：</p>
<p><a href="https://github.com/apache/spark/tree/branch-1.6" target="_blank" rel="external">Spark-1.6.3源码</a></p>
<p><a href="https://github.com/apache/spark/tree/branch-2.1" target="_blank" rel="external">Spark-2.1.0源码</a></p>
<font color="#808080"><em>站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！</em></font>
        
        </div>
        <footer class="article-footer">
            <div class="share-container">


    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>


</div>

            
    
        <a href="http://www.sun4lower.cn/2017/03/02/sc-sparkshell/#comments" class="article-comment-link">Comments</a>
    

        </footer>
    </div>
    
</article>



    <article id="post-sc-schedule" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2017/02/28/sc-schedule/">Spark-Core源码精读(2)、Master中的schedule详解</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2017/02/28/sc-schedule/">
            <time datetime="2017-02-28T08:17:02.000Z" itemprop="datePublished">2017-02-28</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/sparkc/">spark-core</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/driver/">driver</a>, <a class="tag-link" href="/tags/executor/">executor</a>, <a class="tag-link" href="/tags/master/">master</a>, <a class="tag-link" href="/tags/schedule/">schedule</a>, <a class="tag-link" href="/tags/spark/">spark</a>, <a class="tag-link" href="/tags/sparkc/">spark-core</a>, <a class="tag-link" href="/tags/bigdata/">大数据</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>上一篇博客详细分析了Spark在Standalone模式下的部署过程，文中提到在Worker注册完成后需要执行一个schedule操作来分配资源，本文就将具体分析此方法具体是怎样分配资源的。</p>
<p><strong>注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。</strong></p>
<h2 id="什么时候会调用schedule？"><a href="#什么时候会调用schedule？" class="headerlink" title="什么时候会调用schedule？"></a>什么时候会调用schedule？</h2><p>其实每当一个新的application加入活着资源发生变化的时候都会调用schudule方法对资源进行重新分配，那么它是如何分配资源的呢？我们下面进行源码级别的分析。</p>
<h2 id="schedule"><a href="#schedule" class="headerlink" title="schedule"></a>schedule</h2><p>我们先贴出schedule的源码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 既然要分配资源就必须保证Master的当前状态为ALIVE</span></div><div class="line"><span class="keyword">if</span> (state != <span class="type">RecoveryState</span>.<span class="type">ALIVE</span>) &#123;</div><div class="line">  <span class="keyword">return</span></div><div class="line">&#125;</div><div class="line"><span class="comment">// Drivers take strict precedence over executors</span></div><div class="line"><span class="comment">// 注释说的很明确，先注册Drivers然后再注册executors</span></div><div class="line"><span class="comment">// 1. 首先将ALIVE状态的Workers使用shuffle的方式打乱，以免每次都将Driver分配到同一个Worker上</span></div><div class="line"><span class="keyword">val</span> shuffledAliveWorkers = <span class="type">Random</span>.shuffle(workers.toSeq.filter(_.state == <span class="type">WorkerState</span>.<span class="type">ALIVE</span>))</div><div class="line"><span class="keyword">val</span> numWorkersAlive = shuffledAliveWorkers.size</div><div class="line"><span class="keyword">var</span> curPos = <span class="number">0</span></div><div class="line"><span class="comment">// 2. 循环遍历启动Drivers</span></div><div class="line"><span class="keyword">for</span> (driver &lt;- waitingDrivers.toList) &#123; <span class="comment">// iterate over a copy of waitingDrivers</span></div><div class="line">  <span class="comment">// We assign workers to each waiting driver in a round-robin fashion. For each driver, we</span></div><div class="line">  <span class="comment">// start from the last worker that was assigned a driver, and continue onwards until we have</span></div><div class="line">  <span class="comment">// explored all alive workers.</span></div><div class="line">  <span class="keyword">var</span> launched = <span class="literal">false</span></div><div class="line">  <span class="keyword">var</span> numWorkersVisited = <span class="number">0</span></div><div class="line">  <span class="comment">// 2.1 判断是否有剩余的没有分配的Workers，并且尚未启动</span></div><div class="line">  <span class="keyword">while</span> (numWorkersVisited &lt; numWorkersAlive &amp;&amp; !launched) &#123;</div><div class="line">  	<span class="comment">// 2.2 获取一个Worker，第一个的索引为0，后面的索引根据curPos = (curPos + 1) % numWorkersAlive进行计算</span></div><div class="line">    <span class="keyword">val</span> worker = shuffledAliveWorkers(curPos)</div><div class="line">    <span class="comment">// 2.3 标记分配过的Worker加1</span></div><div class="line">    numWorkersVisited += <span class="number">1</span></div><div class="line">    <span class="comment">// 2.4 判断当前的Worker的内存和cpu是否满足Driver的需求</span></div><div class="line">    <span class="keyword">if</span> (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123;</div><div class="line">      <span class="comment">// 2.5 如果满足资源的需求就在当前Worker上启动Driver</span></div><div class="line">      launchDriver(worker, driver)</div><div class="line">      <span class="comment">// 2.6 启动完成后从等待的队列中删除，并将launched标记为true</span></div><div class="line">      waitingDrivers -= driver</div><div class="line">      launched = <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">    curPos = (curPos + <span class="number">1</span>) % numWorkersAlive</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// 3 启动Executors</span></div><div class="line">startExecutorsOnWorkers()</div></pre></td></tr></table></figure>
<h3 id="启动Driver"><a href="#启动Driver" class="headerlink" title="启动Driver"></a>启动Driver</h3><p>我已经在上面的源码中对分配的流程进行了详细的注释，现在我们看一下launchDriver方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchDriver</span></span>(worker: <span class="type">WorkerInfo</span>, driver: <span class="type">DriverInfo</span>) &#123;</div><div class="line">  <span class="comment">// 1. 打日志</span></div><div class="line">  logInfo(<span class="string">"Launching driver "</span> + driver.id + <span class="string">" on worker "</span> + worker.id)</div><div class="line">  <span class="comment">// 2. 向worker中添加driver的信息，包括增加已经使用的内存和cpu信息</span></div><div class="line">  worker.addDriver(driver)</div><div class="line">  <span class="comment">// 3. 向driver中添加该worker的引用</span></div><div class="line">  driver.worker = <span class="type">Some</span>(worker)</div><div class="line">  <span class="comment">// 4. 向Worker发送LaunchDriver的消息，通知Worker启动Driver</span></div><div class="line">  worker.endpoint.send(<span class="type">LaunchDriver</span>(driver.id, driver.desc))</div><div class="line">  <span class="comment">// 5. 将driver的状态变成RUNNING</span></div><div class="line">  driver.state = <span class="type">DriverState</span>.<span class="type">RUNNING</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接下来我们看一下对应的Worker在接收到LaunchDriver消息后是怎么处理的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> <span class="type">LaunchDriver</span>(driverId, driverDesc) =&gt; &#123;</div><div class="line">  <span class="comment">// 1. 打日志</span></div><div class="line">  logInfo(<span class="string">s"Asked to launch driver <span class="subst">$driverId</span>"</span>)</div><div class="line">  <span class="comment">// 2. 实例化DriverRunner</span></div><div class="line">  <span class="keyword">val</span> driver = <span class="keyword">new</span> <span class="type">DriverRunner</span>(</div><div class="line">    conf,</div><div class="line">    driverId,</div><div class="line">    workDir,</div><div class="line">    sparkHome,</div><div class="line">    driverDesc.copy(command = <span class="type">Worker</span>.maybeUpdateSSLSettings(driverDesc.command, conf)),</div><div class="line">    self,</div><div class="line">    workerUri,</div><div class="line">    securityMgr)</div><div class="line">  <span class="comment">// 3. 实例化完成后向drivers中添加该driver的记录</span></div><div class="line">  drivers(driverId) = driver</div><div class="line">  <span class="comment">// 4. 启动driver</span></div><div class="line">  driver.start()</div><div class="line"></div><div class="line">  <span class="comment">// 5. 启动完成后记录资源的变化</span></div><div class="line">  coresUsed += driverDesc.cores</div><div class="line">  memoryUsed += driverDesc.mem</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>继续跟踪driver.start()：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 英文注释说的很清楚：启动一个线程来运行和管理driver</span></div><div class="line"><span class="comment">/** Starts a thread to run and manage the driver. */</span></div><div class="line"><span class="keyword">private</span>[worker] <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() = &#123;</div><div class="line">  <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"DriverRunner for "</span> + driverId) &#123;</div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">// 创建driver的工作目录</span></div><div class="line">        <span class="keyword">val</span> driverDir = createWorkingDirectory()</div><div class="line">        <span class="comment">// 下载用户的Jar文件到driver的工作目录并返回路径名称</span></div><div class="line">        <span class="keyword">val</span> localJarFilename = downloadUserJar(driverDir)</div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">substituteVariables</span></span>(argument: <span class="type">String</span>): <span class="type">String</span> = argument <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="string">"&#123;&#123;WORKER_URL&#125;&#125;"</span> =&gt; workerUrl</div><div class="line">          <span class="keyword">case</span> <span class="string">"&#123;&#123;USER_JAR&#125;&#125;"</span> =&gt; localJarFilename</div><div class="line">          <span class="keyword">case</span> other =&gt; other</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// <span class="doctag">TODO:</span> If we add ability to submit multiple jars they should also be added here</span></div><div class="line">        <span class="keyword">val</span> builder = <span class="type">CommandUtils</span>.buildProcessBuilder(driverDesc.command, securityManager,</div><div class="line">          driverDesc.mem, sparkHome.getAbsolutePath, substituteVariables)</div><div class="line">        <span class="comment">// 具体的启动Driver的操作，这里不再详细分析</span></div><div class="line">        launchDriver(builder, driverDir, driverDesc.supervise)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; finalException = <span class="type">Some</span>(e)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">val</span> state =</div><div class="line">        <span class="keyword">if</span> (killed) &#123;</div><div class="line">          <span class="type">DriverState</span>.<span class="type">KILLED</span></div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (finalException.isDefined) &#123;</div><div class="line">          <span class="type">DriverState</span>.<span class="type">ERROR</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          finalExitCode <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> <span class="type">Some</span>(<span class="number">0</span>) =&gt; <span class="type">DriverState</span>.<span class="type">FINISHED</span></div><div class="line">            <span class="keyword">case</span> _ =&gt; <span class="type">DriverState</span>.<span class="type">FAILED</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      finalState = <span class="type">Some</span>(state)</div><div class="line">      worker.send(<span class="type">DriverStateChanged</span>(driverId, state, finalException))</div><div class="line">    &#125;</div><div class="line">  &#125;.start()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果启动成功最后要向worker发送一条DriverStateChanged的消息，而Worker在接收到该消息后会调用handleDriverStateChanged方法进行一系列处理，具体的处理细节就不再说明，主要的就是向Master发送一条driverStateChanged的消息，Master在接收到该消息后移除Driver的信息：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">ase <span class="type">DriverStateChanged</span>(driverId, state, exception) =&gt; &#123;</div><div class="line">  state <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">DriverState</span>.<span class="type">ERROR</span> | <span class="type">DriverState</span>.<span class="type">FINISHED</span> | <span class="type">DriverState</span>.<span class="type">KILLED</span> | <span class="type">DriverState</span>.<span class="type">FAILED</span> =&gt;</div><div class="line">      removeDriver(driverId, state, exception)</div><div class="line">    <span class="keyword">case</span> _ =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">s"Received unexpected state update for driver <span class="subst">$driverId</span>: <span class="subst">$state</span>"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>至此向Driver分配资源并启动Driver的过程结束，下面我们看一下启动Executors即执行startExecutorsOnWorkers()的流程。</p>
<h3 id="启动Executors"><a href="#启动Executors" class="headerlink" title="启动Executors"></a>启动Executors</h3><p>startExecutorsOnWorkers():</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * Schedule and launch executors on workers</div><div class="line">   */</div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startExecutorsOnWorkers</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  	 <span class="comment">// 采用的是先进先出的原则</span></div><div class="line">    <span class="comment">// Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app</span></div><div class="line">    <span class="comment">// in the queue, then the second app, etc.</span></div><div class="line">    <span class="keyword">for</span> (app &lt;- waitingApps <span class="keyword">if</span> app.coresLeft &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="keyword">val</span> coresPerExecutor: <span class="type">Option</span>[<span class="type">Int</span>] = app.desc.coresPerExecutor</div><div class="line">      <span class="comment">// Filter out workers that don't have enough resources to launch an executor</span></div><div class="line">      <span class="comment">// 过滤出ALIVE状态并且资源满足要求的workers，同时按照空闲cpu cores的个数倒序排列</span></div><div class="line">      <span class="keyword">val</span> usableWorkers = workers.toArray.filter(_.state == <span class="type">WorkerState</span>.<span class="type">ALIVE</span>)</div><div class="line">        .filter(worker =&gt; worker.memoryFree &gt;= app.desc.memoryPerExecutorMB &amp;&amp;</div><div class="line">          worker.coresFree &gt;= coresPerExecutor.getOrElse(<span class="number">1</span>))</div><div class="line">        .sortBy(_.coresFree).reverse</div><div class="line">        </div><div class="line">      <span class="comment">// 决定在每个worker上面分配多少个cpu cores</span></div><div class="line">      <span class="keyword">val</span> assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)</div><div class="line"></div><div class="line">      <span class="comment">// 然后开始进行分配</span></div><div class="line">      <span class="comment">// Now that we've decided how many cores to allocate on each worker, let's allocate them</span></div><div class="line">      <span class="keyword">for</span> (pos &lt;- <span class="number">0</span> until usableWorkers.length <span class="keyword">if</span> assignedCores(pos) &gt; <span class="number">0</span>) &#123;</div><div class="line">        allocateWorkerResourceToExecutors(</div><div class="line">          app, assignedCores(pos), coresPerExecutor, usableWorkers(pos))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>我们首先看一下是如何决定在每个worker上分配多少个cores的，这里我们只列出scheduleExecutorsOnWorkers方法的英文注释，并进行说明，具体的操作大家可以去看源码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Schedule executors to be launched on the workers.</div><div class="line"> * Returns an array containing number of cores assigned to each worker.</div><div class="line"> *</div><div class="line"> * There are two modes of launching executors. The first attempts to spread out an application's</div><div class="line"> * executors on as many workers as possible, while the second does the opposite (i.e. launch them</div><div class="line"> * on as few workers as possible). The former is usually better for data locality purposes and is</div><div class="line"> * the default.</div><div class="line"> *</div><div class="line"> * The number of cores assigned to each executor is configurable. When this is explicitly set,</div><div class="line"> * multiple executors from the same application may be launched on the same worker if the worker</div><div class="line"> * has enough cores and memory. Otherwise, each executor grabs all the cores available on the</div><div class="line"> * worker by default, in which case only one executor may be launched on each worker.</div><div class="line"> *</div><div class="line"> * It is important to allocate coresPerExecutor on each worker at a time (instead of 1 core</div><div class="line"> * at a time). Consider the following example: cluster has 4 workers with 16 cores each.</div><div class="line"> * User requests 3 executors (spark.cores.max = 48, spark.executor.cores = 16). If 1 core is</div><div class="line"> * allocated at a time, 12 cores from each worker would be assigned to each executor.</div><div class="line"> * Since 12 &lt; 16, no executors would launch [SPARK-8881].</div><div class="line"> */</div></pre></td></tr></table></figure>
<p>大致意思是说有两种分配模型，第一种是将executors分配到尽可能多的workers上；第二种与第一种相反。默认使用的是第一种模型，这种模型更加符合数据的本地性原则，为每个Executor分配的cores的个数是可以进行配置的（spark-submit 或者 spark-env.sh），如果设置了，多个executors可能会被分配在一个worker上（前提是该worker拥有足够的cores和memory），否则每个executor会充分利用worker上的cores，这种情况下一个executor会被分配在一个worker上。具体在集群上分配cores的时候会尽可能的满足我们的要求，如果需要的cores的个数大于workers中空闲的cores的个数，那么就先分配空闲的cores，尽可能的去满足要求。</p>
<p>接下来就是具体为executors分配计算资源并启动executors的过程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">allocateWorkerResourceToExecutors</span></span>(</div><div class="line">      app: <span class="type">ApplicationInfo</span>,</div><div class="line">      assignedCores: <span class="type">Int</span>,</div><div class="line">      coresPerExecutor: <span class="type">Option</span>[<span class="type">Int</span>],</div><div class="line">      worker: <span class="type">WorkerInfo</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// If the number of cores per executor is specified, we divide the cores assigned</span></div><div class="line">    <span class="comment">// to this worker evenly among the executors with no remainder.</span></div><div class="line">    <span class="comment">// Otherwise, we launch a single executor that grabs all the assignedCores on this worker.</span></div><div class="line">    <span class="keyword">val</span> numExecutors = coresPerExecutor.map &#123; assignedCores / _ &#125;.getOrElse(<span class="number">1</span>)</div><div class="line">    <span class="keyword">val</span> coresToAssign = coresPerExecutor.getOrElse(assignedCores)</div><div class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to numExecutors) &#123;</div><div class="line">      <span class="comment">// 向application中添加executor的信息</span></div><div class="line">      <span class="keyword">val</span> exec = app.addExecutor(worker, coresToAssign)</div><div class="line">      <span class="comment">// 启动executors</span></div><div class="line">      launchExecutor(worker, exec)</div><div class="line">      app.state = <span class="type">ApplicationState</span>.<span class="type">RUNNING</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>启动executors：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchExecutor</span></span>(worker: <span class="type">WorkerInfo</span>, exec: <span class="type">ExecutorDesc</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    logInfo(<span class="string">"Launching executor "</span> + exec.fullId + <span class="string">" on worker "</span> + worker.id)</div><div class="line">    worker.addExecutor(exec)</div><div class="line">    <span class="comment">// 向worker发消息启动executor</span></div><div class="line">    worker.endpoint.send(<span class="type">LaunchExecutor</span>(masterUrl,</div><div class="line">      exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory))</div><div class="line">    <span class="comment">// 然后向driver发送executors的信息</span></div><div class="line">    exec.application.driver.send(</div><div class="line">      <span class="type">ExecutorAdded</span>(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>worker在接收到启动executor的消息后执行具体的启动操作，并向Master汇报。</p>
<p>然后也要向driver发送executors的资源信息，driver收到信息后执行application，至此分配并启动executors的大致流程也就执行完毕。</p>
<p>最后用一张图总结一下启动Driver和Worker的简易流程：</p>
<p><img src="http://wx3.sinaimg.cn/large/006y2nc1ly1fd6iau7to2j30rs0azt9i.jpg" alt=""></p>
<p>本文只是大致的分析了Master在执行schedule的时候具体为Driver、Executors分配资源并启动它们的流程，以后我们还会分析整个application的运行流程，那时我们会具体进行分析。</p>
<p>本文参考和拓展阅读：</p>
<p><a href="https://github.com/apache/spark/tree/branch-1.6" target="_blank" rel="external">Spark-1.6.3源码</a></p>
<p><a href="https://github.com/apache/spark/tree/branch-2.1" target="_blank" rel="external">Spark-2.1.0源码</a></p>
<font color="#808080"><em>站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！</em></font>
        
        </div>
        <footer class="article-footer">
            <div class="share-container">


    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>


</div>

            
    
        <a href="http://www.sun4lower.cn/2017/02/28/sc-schedule/#comments" class="article-comment-link">Comments</a>
    

        </footer>
    </div>
    
</article>



    <article id="post-sc-deploy" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2017/02/25/sc-deploy/">Spark-Core源码精读(1)、Spark Deployment &amp; start-all.sh on Standalone mode</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2017/02/25/sc-deploy/">
            <time datetime="2017-02-25T05:58:04.000Z" itemprop="datePublished">2017-02-25</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/sparkc/">spark-core</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/prc/">RPC</a>, <a class="tag-link" href="/tags/deployment/">deployment</a>, <a class="tag-link" href="/tags/master/">master</a>, <a class="tag-link" href="/tags/mesos/">mesos</a>, <a class="tag-link" href="/tags/spark/">spark</a>, <a class="tag-link" href="/tags/sparkc/">spark-core</a>, <a class="tag-link" href="/tags/standalone/">standalone</a>, <a class="tag-link" href="/tags/worker/">worker</a>, <a class="tag-link" href="/tags/yarn/">yarn</a>, <a class="tag-link" href="/tags/bigdata/">大数据</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>本文为精度Spark-core的源码的第一节，主要内容包括Spark Deployment的简介和Standalone模式下启动集群的详细流程精读。</p>
<p><strong>注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。</strong></p>
<h2 id="Spark-Deployment"><a href="#Spark-Deployment" class="headerlink" title="Spark Deployment"></a>Spark Deployment</h2><p>Spark 的部署主要有三种方式：local、standalone、yarn、mesos</p>
<p><img src="http://wx1.sinaimg.cn/mw690/006y2nc1ly1fd2oty5qcnj30fs07l3z2.jpg" alt="图片来源：Spark-Essentials-SSW2016-TE1.pdf"></p>
<p>其中local和standalone模式主要用于测试学习，实际生产环境下国内一般都是使用yarn，这是历史原因造成的（考虑到集群中同时有Hadoop）；而国外一般都是使用mesos，而且个人认为mesos也是一种趋势，关于yarn和mesos的部分，以后会单独进行分析，下面我们详细解读standalone模式下的集群启动的具体流程。</p>
<h2 id="Standalone-mode下集群启动源码精读"><a href="#Standalone-mode下集群启动源码精读" class="headerlink" title="Standalone mode下集群启动源码精读"></a>Standalone mode下集群启动源码精读</h2><p>我们就从start-all.sh开始，主要代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load the Spark configuration</span></div><div class="line">. <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh"</span></div><div class="line"></div><div class="line"><span class="comment"># Start Master</span></div><div class="line"><span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/start-master.sh <span class="variable">$TACHYON_STR</span></div><div class="line"></div><div class="line"><span class="comment"># Start Workers</span></div><div class="line"><span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/start-slaves.sh <span class="variable">$TACHYON_STR</span></div></pre></td></tr></table></figure>
<p>注释说的很明确了，我们继续追踪start-master.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">CLASS=<span class="string">"org.apache.spark.deploy.master.Master"</span></div><div class="line">...</div><div class="line"><span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/spark-daemon.sh start <span class="variable">$CLASS</span> 1 \</div><div class="line">  --ip <span class="variable">$SPARK_MASTER_IP</span> --port <span class="variable">$SPARK_MASTER_PORT</span> --webui-port <span class="variable">$SPARK_MASTER_WEBUI_PORT</span> \</div><div class="line">  <span class="variable">$ORIGINAL_ARGS</span></div><div class="line">...</div></pre></td></tr></table></figure>
<p>可以看出，是执行了spark-daemon.sh的start方法，即通过动态加载的方式将org.apache.spark.deploy.master.Master作为一个daemon（守护线程）来运行，所以我们直接分析Master的源码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[deploy] <span class="class"><span class="keyword">object</span> <span class="title">Master</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="keyword">val</span> <span class="type">SYSTEM_NAME</span> = <span class="string">"sparkMaster"</span></div><div class="line">  <span class="keyword">val</span> <span class="type">ENDPOINT_NAME</span> = <span class="string">"Master"</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">    <span class="comment">//注册log</span></div><div class="line">    <span class="type">SignalLogger</span>.register(log)</div><div class="line">    <span class="comment">//实例化SparkConf，会加载`spark.*`格式的配置信息</span></div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span></div><div class="line">    <span class="comment">//使用MasterArguments对传入的参数argStrings和默认加载的conf进行封装，并执行一些初始化操作</span></div><div class="line">    <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">MasterArguments</span>(argStrings, conf)</div><div class="line">    <span class="keyword">val</span> (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf)</div><div class="line">    rpcEnv.awaitTermination()</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Start the Master and return a three tuple of:</div><div class="line">   *   (1) The Master RpcEnv</div><div class="line">   *   (2) The web UI bound port</div><div class="line">   *   (3) The REST server bound port, if any</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startRpcEnvAndEndpoint</span></span>(</div><div class="line">      host: <span class="type">String</span>,</div><div class="line">      port: <span class="type">Int</span>,</div><div class="line">      webUiPort: <span class="type">Int</span>,</div><div class="line">      conf: <span class="type">SparkConf</span>): (<span class="type">RpcEnv</span>, <span class="type">Int</span>, <span class="type">Option</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="keyword">val</span> securityMgr = <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf)</div><div class="line">    <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(<span class="type">SYSTEM_NAME</span>, host, port, conf, securityMgr)</div><div class="line">    <span class="keyword">val</span> masterEndpoint = rpcEnv.setupEndpoint(<span class="type">ENDPOINT_NAME</span>,</div><div class="line">      <span class="keyword">new</span> <span class="type">Master</span>(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf))</div><div class="line">    <span class="keyword">val</span> portsResponse = masterEndpoint.askWithRetry[<span class="type">BoundPortsResponse</span>](<span class="type">BoundPortsRequest</span>)</div><div class="line">    (rpcEnv, portsResponse.webUIPort, portsResponse.restPort)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>首先注册log，实例化SparkConf并加载<code>spark.*</code>格式的配置信息，然后使用MasterArguments对传入的参数argStrings和默认加载的conf进行封装，并执行一些初始化操作，主要是加载配置信息，这里不做详细说明，我们接着往下看。</p>
<p>下面才是真正意义上的Start Master，startRpcEnvAndEndpoint函数中首先实例化了SecurityManager（Spark中负责安全的类），然后创建了RpcEnv（Spark的Rpc通信有三个抽象：RpcEnv、RpcEndpoint、RpcEndpointRef，这样做屏蔽了底层的实现，方便用户进行扩展，Spark-1.6.3底层的默认实现方式是Netty，而Spark-2.x已经将Akka的依赖移除），接着实例化Master，实际上就是实例化了一个RpcEndpoint（因为Master实现了ThreadSafeRpcEndpoint接口，而ThreadSafeRpcEndpoint又继承了RpcEndpoint），实例化完成后通过RpcEnv的setupEndpoint向RpcEnv进行注册，注册的时候执行了Master的onStart方法，最后返回了一个RpcEndpointRef（实际上是NettyRpcEndpointRef），通过获得的RpcEndpointRef向Master（Endpoint）发送了一条BoundPortsRequest消息，Master通过receiveAndReply方法接受到该消息（实际上是通过NettyRpcEnv中的Dispatcher进行消息的分配），模式匹配到是BoundPortsRequest类型的消息，然后执行reply方法进行回复，源码如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> <span class="type">BoundPortsRequest</span> =&gt; &#123;</div><div class="line">      context.reply(<span class="type">BoundPortsResponse</span>(address.port, webUi.boundPort, restServerBoundPort))</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<p>至此Master启动完成，Rpc部分可以参考另一篇文章：<a href="http://sun4lower.cn/2017/02/22/SparkRpcBasic/" target="_blank" rel="external">Spark RPC 到底是个什么鬼？</a>，下面贴出Master实例化部分和onStart方法的源码及中文注释：</p>
<p>Master实例化部分：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//默认的情况下，取消的task不会从工作的队列中移除直到延迟时间完成，所以创建一个守护线程来“手动”移除它</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> forwardMessageThread =</div><div class="line">  <span class="type">ThreadUtils</span>.newDaemonSingleThreadScheduledExecutor(<span class="string">"master-forward-message-thread"</span>)</div><div class="line"></div><div class="line"><span class="comment">//用于执行重建UI代码的守护线程</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> rebuildUIThread =</div><div class="line">  <span class="type">ThreadUtils</span>.newDaemonSingleThreadExecutor(<span class="string">"master-rebuild-ui-thread"</span>)</div><div class="line">  </div><div class="line"><span class="comment">//通过rebuildUIThread获得重建UI的执行上下文</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> rebuildUIContext = <span class="type">ExecutionContext</span>.fromExecutor(rebuildUIThread)</div><div class="line"></div><div class="line"><span class="comment">//获取hadoop的配置文件</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> hadoopConf = <span class="type">SparkHadoopUtil</span>.get.newConfiguration(conf)</div><div class="line"></div><div class="line"><span class="comment">//时间格式，用于构建application ID</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createDateFormat</span> </span>= <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMMddHHmmss"</span>) <span class="comment">// For application IDs</span></div><div class="line"></div><div class="line"><span class="comment">//如果Master在60s内没有收到Worker发送的heartbeat信息就认为这个Worker timeout</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">WORKER_TIMEOUT_MS</span> = conf.getLong(<span class="string">"spark.worker.timeout"</span>, <span class="number">60</span>) * <span class="number">1000</span></div><div class="line"><span class="comment">//webUI中显示的完成的application的最大个数，超过200个就移除掉(200/10,1)=20个完成的applications</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">RETAINED_APPLICATIONS</span> = conf.getInt(<span class="string">"spark.deploy.retainedApplications"</span>, <span class="number">200</span>)</div><div class="line"><span class="comment">//webUI中显示的完成的drivers的最大个数，超过200个就移除掉(200/10,1)=20个完成的drivers</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">RETAINED_DRIVERS</span> = conf.getInt(<span class="string">"spark.deploy.retainedDrivers"</span>, <span class="number">200</span>)</div><div class="line"><span class="comment">//如果Master在(REAPER_ITERATIONS + 1) * WORKER_TIMEOUT_MS)秒内仍然没有收到Worker发送的heartbeat信息，就删除这个Worker</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">REAPER_ITERATIONS</span> = conf.getInt(<span class="string">"spark.dead.worker.persistence"</span>, <span class="number">15</span>)</div><div class="line"><span class="comment">//recoveryMode：NONE、ZOOKEEPER、FILESYSTEM、CUSTOM，默认是NONE</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">RECOVERY_MODE</span> = conf.get(<span class="string">"spark.deploy.recoveryMode"</span>, <span class="string">"NONE"</span>)</div><div class="line"><span class="comment">//Executor失败的最大重试次数</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">MAX_EXECUTOR_RETRIES</span> = conf.getInt(<span class="string">"spark.deploy.maxExecutorRetries"</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="comment">//下面是各种“数据结构”，不再一一说明</span></div><div class="line"><span class="keyword">val</span> workers = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">WorkerInfo</span>]</div><div class="line"><span class="keyword">val</span> idToApp = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">ApplicationInfo</span>]</div><div class="line"><span class="keyword">val</span> waitingApps = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">ApplicationInfo</span>]</div><div class="line"><span class="keyword">val</span> apps = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ApplicationInfo</span>]</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> idToWorker = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">WorkerInfo</span>]</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> addressToWorker = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">RpcAddress</span>, <span class="type">WorkerInfo</span>]</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> endpointToApp = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">RpcEndpointRef</span>, <span class="type">ApplicationInfo</span>]</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> addressToApp = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">RpcAddress</span>, <span class="type">ApplicationInfo</span>]</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> completedApps = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">ApplicationInfo</span>]</div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> nextAppNumber = <span class="number">0</span></div><div class="line"><span class="comment">// Using ConcurrentHashMap so that master-rebuild-ui-thread can add a UI after asyncRebuildUI</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> appIdToUI = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">String</span>, <span class="type">SparkUI</span>]</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> drivers = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">DriverInfo</span>]</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> completedDrivers = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">DriverInfo</span>]</div><div class="line"><span class="comment">// Drivers currently spooled for scheduling</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> waitingDrivers = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">DriverInfo</span>]</div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> nextDriverNumber = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="type">Utils</span>.checkHost(address.host, <span class="string">"Expected hostname"</span>)</div><div class="line"></div><div class="line"><span class="comment">//下面是Metrics系统相关的代码</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> masterMetricsSystem = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">"master"</span>, conf, securityMgr)</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> applicationMetricsSystem = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">"applications"</span>, conf,</div><div class="line">  securityMgr)</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> masterSource = <span class="keyword">new</span> <span class="type">MasterSource</span>(<span class="keyword">this</span>)</div><div class="line"></div><div class="line"><span class="comment">// After onStart, webUi will be set</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> webUi: <span class="type">MasterWebUI</span> = <span class="literal">null</span></div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> masterPublicAddress = &#123;</div><div class="line">  <span class="keyword">val</span> envVar = conf.getenv(<span class="string">"SPARK_PUBLIC_DNS"</span>)</div><div class="line">  <span class="keyword">if</span> (envVar != <span class="literal">null</span>) envVar <span class="keyword">else</span> address.host</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> masterUrl = address.toSparkURL</div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> masterWebUiUrl: <span class="type">String</span> = _</div><div class="line"></div><div class="line"><span class="comment">//当前Master的状态：STANDBY, ALIVE, RECOVERING, COMPLETING_RECOVERY</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> state = <span class="type">RecoveryState</span>.<span class="type">STANDBY</span></div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> persistenceEngine: <span class="type">PersistenceEngine</span> = _</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> leaderElectionAgent: <span class="type">LeaderElectionAgent</span> = _</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> recoveryCompletionTask: <span class="type">ScheduledFuture</span>[_] = _</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> checkForWorkerTimeOutTask: <span class="type">ScheduledFuture</span>[_] = _</div><div class="line"></div><div class="line"><span class="comment">// As a temporary workaround before better ways of configuring memory, we allow users to set</span></div><div class="line"><span class="comment">// a flag that will perform round-robin scheduling across the nodes (spreading out each app</span></div><div class="line"><span class="comment">// among all the nodes) instead of trying to consolidate each app onto a small # of nodes.</span></div><div class="line"><span class="comment">// 避免将application的运行限制在固定的几个节点上</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> spreadOutApps = conf.getBoolean(<span class="string">"spark.deploy.spreadOut"</span>, <span class="literal">true</span>)</div><div class="line"></div><div class="line"><span class="comment">// Default maxCores for applications that don't specify it (i.e. pass Int.MaxValue)</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> defaultCores = conf.getInt(<span class="string">"spark.deploy.defaultCores"</span>, <span class="type">Int</span>.<span class="type">MaxValue</span>)</div><div class="line"><span class="keyword">if</span> (defaultCores &lt; <span class="number">1</span>) &#123;</div><div class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"spark.deploy.defaultCores must be positive"</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Alternative application submission gateway that is stable across Spark versions</span></div><div class="line"><span class="comment">// 用来接受application提交的restServer</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> restServerEnabled = conf.getBoolean(<span class="string">"spark.master.rest.enabled"</span>, <span class="literal">true</span>)</div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> restServer: <span class="type">Option</span>[<span class="type">StandaloneRestServer</span>] = <span class="type">None</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> restServerBoundPort: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">None</span></div></pre></td></tr></table></figure>
<p>onStart方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">//打日志</span></div><div class="line">    logInfo(<span class="string">"Starting Spark master at "</span> + masterUrl)</div><div class="line">    logInfo(<span class="string">s"Running Spark version <span class="subst">$&#123;org.apache.spark.SPARK_VERSION&#125;</span>"</span>)</div><div class="line">    <span class="comment">//实例化standalone模式下的MasterWebUI并绑定到HTTP Server</span></div><div class="line">    webUi = <span class="keyword">new</span> <span class="type">MasterWebUI</span>(<span class="keyword">this</span>, webUiPort)</div><div class="line">    webUi.bind()</div><div class="line">    <span class="comment">//可以通过这个Url地址看到Master的信息</span></div><div class="line">    masterWebUiUrl = <span class="string">"http://"</span> + masterPublicAddress + <span class="string">":"</span> + webUi.boundPort</div><div class="line">    </div><div class="line">    <span class="comment">//以固定的时间间隔检查并移除time-out的worker</span></div><div class="line">    checkForWorkerTimeOutTask = forwardMessageThread.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</div><div class="line">        self.send(<span class="type">CheckForWorkerTimeOut</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;, <span class="number">0</span>, <span class="type">WORKER_TIMEOUT_MS</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line"></div><div class="line">    <span class="comment">//实例化并启动restServer用于接受application的提交</span></div><div class="line">    <span class="keyword">if</span> (restServerEnabled) &#123;</div><div class="line">      <span class="keyword">val</span> port = conf.getInt(<span class="string">"spark.master.rest.port"</span>, <span class="number">6066</span>)</div><div class="line">      restServer = <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">StandaloneRestServer</span>(address.host, port, conf, self, masterUrl))</div><div class="line">    &#125;</div><div class="line">    restServerBoundPort = restServer.map(_.start())</div><div class="line"></div><div class="line">    <span class="comment">//启动MetricsSystem</span></div><div class="line">    masterMetricsSystem.registerSource(masterSource)</div><div class="line">    masterMetricsSystem.start()</div><div class="line">    applicationMetricsSystem.start()</div><div class="line">    <span class="comment">// Attach the master and app metrics servlet handler to the web ui after the metrics systems are</span></div><div class="line">    <span class="comment">// started.</span></div><div class="line">    masterMetricsSystem.getServletHandlers.foreach(webUi.attachHandler)</div><div class="line">    applicationMetricsSystem.getServletHandlers.foreach(webUi.attachHandler)</div><div class="line"></div><div class="line">    <span class="comment">//序列化器</span></div><div class="line">    <span class="keyword">val</span> serializer = <span class="keyword">new</span> <span class="type">JavaSerializer</span>(conf)</div><div class="line">    </div><div class="line">    <span class="comment">//恢复机制，包括持久化引擎和选举机制</span></div><div class="line">    <span class="keyword">val</span> (persistenceEngine_, leaderElectionAgent_) = <span class="type">RECOVERY_MODE</span> <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="string">"ZOOKEEPER"</span> =&gt;</div><div class="line">        logInfo(<span class="string">"Persisting recovery state to ZooKeeper"</span>)</div><div class="line">        <span class="keyword">val</span> zkFactory =</div><div class="line">          <span class="keyword">new</span> <span class="type">ZooKeeperRecoveryModeFactory</span>(conf, serializer)</div><div class="line">        (zkFactory.createPersistenceEngine(), zkFactory.createLeaderElectionAgent(<span class="keyword">this</span>))</div><div class="line">      <span class="keyword">case</span> <span class="string">"FILESYSTEM"</span> =&gt;</div><div class="line">        <span class="keyword">val</span> fsFactory =</div><div class="line">          <span class="keyword">new</span> <span class="type">FileSystemRecoveryModeFactory</span>(conf, serializer)</div><div class="line">        (fsFactory.createPersistenceEngine(), fsFactory.createLeaderElectionAgent(<span class="keyword">this</span>))</div><div class="line">      <span class="keyword">case</span> <span class="string">"CUSTOM"</span> =&gt;</div><div class="line">        <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(conf.get(<span class="string">"spark.deploy.recoveryMode.factory"</span>))</div><div class="line">        <span class="keyword">val</span> factory = clazz.getConstructor(classOf[<span class="type">SparkConf</span>], classOf[<span class="type">Serializer</span>])</div><div class="line">          .newInstance(conf, serializer)</div><div class="line">          .asInstanceOf[<span class="type">StandaloneRecoveryModeFactory</span>]</div><div class="line">        (factory.createPersistenceEngine(), factory.createLeaderElectionAgent(<span class="keyword">this</span>))</div><div class="line">      <span class="keyword">case</span> _ =&gt;</div><div class="line">        (<span class="keyword">new</span> <span class="type">BlackHolePersistenceEngine</span>(), <span class="keyword">new</span> <span class="type">MonarchyLeaderAgent</span>(<span class="keyword">this</span>))</div><div class="line">    &#125;</div><div class="line">    persistenceEngine = persistenceEngine_</div><div class="line">    leaderElectionAgent = leaderElectionAgent_</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>下面介绍Worker的启动</p>
<p>start-slaves.sh:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Launch the slaves</span></div><div class="line"><span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/slaves.sh"</span> <span class="built_in">cd</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> \; <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/start-slave.sh"</span> <span class="string">"spark://<span class="variable">$SPARK_MASTER_IP</span>:<span class="variable">$SPARK_MASTER_PORT</span>"</span></div></pre></td></tr></table></figure>
<p>start-slave.sh:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">CLASS=<span class="string">"org.apache.spark.deploy.worker.Worker"</span></div><div class="line">...</div><div class="line">  <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/spark-daemon.sh start <span class="variable">$CLASS</span> <span class="variable">$WORKER_NUM</span> \</div><div class="line">     --webui-port <span class="string">"<span class="variable">$WEBUI_PORT</span>"</span> <span class="variable">$PORT_FLAG</span> <span class="variable">$PORT_NUM</span> <span class="variable">$MASTER</span> <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure>
<p>和Master的启动类似，我们直接看Worker文件，仍然从main方法开始：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">    <span class="type">SignalLogger</span>.register(log)</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span></div><div class="line">    <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">WorkerArguments</span>(argStrings, conf)</div><div class="line">    <span class="keyword">val</span> rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores,</div><div class="line">      args.memory, args.masters, args.workDir, conf = conf)</div><div class="line">    rpcEnv.awaitTermination()</div><div class="line">  &#125;</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startRpcEnvAndEndpoint</span></span>(</div><div class="line">      host: <span class="type">String</span>,</div><div class="line">      port: <span class="type">Int</span>,</div><div class="line">      webUiPort: <span class="type">Int</span>,</div><div class="line">      cores: <span class="type">Int</span>,</div><div class="line">      memory: <span class="type">Int</span>,</div><div class="line">      masterUrls: <span class="type">Array</span>[<span class="type">String</span>],</div><div class="line">      workDir: <span class="type">String</span>,</div><div class="line">      workerNumber: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">None</span>,</div><div class="line">      conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>): <span class="type">RpcEnv</span> = &#123;</div><div class="line"></div><div class="line">    <span class="comment">// The LocalSparkCluster runs multiple local sparkWorkerX RPC Environments</span></div><div class="line">    <span class="keyword">val</span> systemName = <span class="type">SYSTEM_NAME</span> + workerNumber.map(_.toString).getOrElse(<span class="string">""</span>)</div><div class="line">    <span class="keyword">val</span> securityMgr = <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf)</div><div class="line">    <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(systemName, host, port, conf, securityMgr)</div><div class="line">    <span class="keyword">val</span> masterAddresses = masterUrls.map(<span class="type">RpcAddress</span>.fromSparkURL(_))</div><div class="line">    rpcEnv.setupEndpoint(<span class="type">ENDPOINT_NAME</span>, <span class="keyword">new</span> <span class="type">Worker</span>(rpcEnv, webUiPort, cores, memory,</div><div class="line">      masterAddresses, systemName, <span class="type">ENDPOINT_NAME</span>, workDir, conf, securityMgr))</div><div class="line">    rpcEnv</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>可以看到前面和Master类似，只不过Worker有可能是多个，所以需要根据workerNumber构造一个systemName，用来创建不同的RpcEnv，然后实例化Worker（即实例化Endpoint），实例化的时候需要传入masterAddresses（注意此处可能有多个Master），以便以后向Master注册，同时由于要向对应的RpcEnv注册，注册的时候同样要执行Worker的onStart方法，我会将Worker实例化和onStart的源码放到后面，这里我们先来看一下Worker向Master注册的代码（onStart方法中调用registerWithMaster）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerWithMaster</span></span>() &#123;</div><div class="line">    <span class="comment">// onDisconnected may be triggered multiple times, so don't attempt registration</span></div><div class="line">    <span class="comment">// if there are outstanding registration attempts scheduled.</span></div><div class="line">    registrationRetryTimer <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        registered = <span class="literal">false</span></div><div class="line">        registerMasterFutures = tryRegisterAllMasters()</div><div class="line">        connectionAttemptCount = <span class="number">0</span></div><div class="line">        registrationRetryTimer = <span class="type">Some</span>(forwordMessageScheduler.scheduleAtFixedRate(</div><div class="line">          <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</div><div class="line">              <span class="type">Option</span>(self).foreach(_.send(<span class="type">ReregisterWithMaster</span>))</div><div class="line">            &#125;</div><div class="line">          &#125;,</div><div class="line">          <span class="type">INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</div><div class="line">          <span class="type">INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</div><div class="line">          <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>))</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt;</div><div class="line">        logInfo(<span class="string">"Not spawning another attempt to register with the master, since there is an"</span> +</div><div class="line">          <span class="string">" attempt scheduled already."</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>可以看到内部调用了tryRegisterAllMasters方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">tryRegisterAllMasters</span></span>(): <span class="type">Array</span>[<span class="type">JFuture</span>[_]] = &#123;</div><div class="line">    masterRpcAddresses.map &#123; masterAddress =&gt;</div><div class="line">      registerMasterThreadPool.submit(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">          <span class="keyword">try</span> &#123;</div><div class="line">            logInfo(<span class="string">"Connecting to master "</span> + masterAddress + <span class="string">"..."</span>)</div><div class="line">            <span class="keyword">val</span> masterEndpoint =</div><div class="line">              rpcEnv.setupEndpointRef(<span class="type">Master</span>.<span class="type">SYSTEM_NAME</span>, masterAddress, <span class="type">Master</span>.<span class="type">ENDPOINT_NAME</span>)</div><div class="line">            registerWithMaster(masterEndpoint)</div><div class="line">          &#125; <span class="keyword">catch</span> &#123;</div><div class="line">            <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// Cancelled</span></div><div class="line">            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logWarning(<span class="string">s"Failed to connect to master <span class="subst">$masterAddress</span>"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;)</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>通过一个名为registerMasterThreadPool的线程池（最大线程数为Worker的个数）来运行run方法中的内容：首先通过setupEndpointRef方法获得其中一个Master的一个引用（RpcEndpointRef），然后执行registerWithMaster(masterEndpoint)方法，刚才得到的Master的引用作为参数传入，下面进入registerWithMaster方法：（注意此处的registerWithMaster方法是有一个RpcEndpointRef作为参数的，和刚开始的那个不一样）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerWithMaster</span></span>(masterEndpoint: <span class="type">RpcEndpointRef</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    masterEndpoint.ask[<span class="type">RegisterWorkerResponse</span>](<span class="type">RegisterWorker</span>(</div><div class="line">      workerId, host, port, self, cores, memory, webUi.boundPort, publicAddress))</div><div class="line">      .onComplete &#123;</div><div class="line">        <span class="comment">// This is a very fast action so we can use "ThreadUtils.sameThread"</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Success</span>(msg) =&gt;</div><div class="line">          <span class="type">Utils</span>.tryLogNonFatalError &#123;</div><div class="line">            handleRegisterResponse(msg)</div><div class="line">          &#125;</div><div class="line">        <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt;</div><div class="line">          logError(<span class="string">s"Cannot register with master: <span class="subst">$&#123;masterEndpoint.address&#125;</span>"</span>, e)</div><div class="line">          <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">      &#125;(<span class="type">ThreadUtils</span>.sameThread)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>内部使用masterEndpoint（Master的RpcEndpointRef）的ask方法向Master发送一条RegisterWorker的消息，并使用onComplete方法接受Master的处理结果，下面我们先来看一下消息到达Master端进行怎样的处理：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">RegisterWorker</span>(</div><div class="line">        id, workerHost, workerPort, workerRef, cores, memory, workerUiPort, publicAddress) =&gt; &#123;</div><div class="line">      logInfo(<span class="string">"Registering worker %s:%d with %d cores, %s RAM"</span>.format(</div><div class="line">        workerHost, workerPort, cores, <span class="type">Utils</span>.megabytesToString(memory)))</div><div class="line">      <span class="keyword">if</span> (state == <span class="type">RecoveryState</span>.<span class="type">STANDBY</span>) &#123;</div><div class="line">        context.reply(<span class="type">MasterInStandby</span>)</div><div class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (idToWorker.contains(id)) &#123;</div><div class="line">        context.reply(<span class="type">RegisterWorkerFailed</span>(<span class="string">"Duplicate worker ID"</span>))</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> worker = <span class="keyword">new</span> <span class="type">WorkerInfo</span>(id, workerHost, workerPort, cores, memory,</div><div class="line">          workerRef, workerUiPort, publicAddress)</div><div class="line">        <span class="keyword">if</span> (registerWorker(worker)) &#123;</div><div class="line">          persistenceEngine.addWorker(worker)</div><div class="line">          context.reply(<span class="type">RegisteredWorker</span>(self, masterWebUiUrl))</div><div class="line">          schedule()</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">val</span> workerAddress = worker.endpoint.address</div><div class="line">          logWarning(<span class="string">"Worker registration failed. Attempted to re-register worker at same "</span> +</div><div class="line">            <span class="string">"address: "</span> + workerAddress)</div><div class="line">          context.reply(<span class="type">RegisterWorkerFailed</span>(<span class="string">"Attempted to re-register worker at same address: "</span></div><div class="line">            + workerAddress))</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>首先receiveAndReply方法匹配到Worker发过来的RegisterWorker消息，然后执行具体的操作：打了一个日志，判断Master现在的状态，如果是STANDBY就reply一个MasterInStandby的消息，如果idToWorker中已经存在该Worker的ID就回复重复的worker ID的失败信息，如果都不是，将获得的Worker信息用WorkerInfo进行封装，然后执行registerWorker(worker)操作注册该Worker，如果成功就向persistenceEngine中添加该Worker并reply给Worker RegisteredWorker(self, masterWebUiUrl)消息并执行schedule方法，如果注册失败就reply RegisterWorkerFailed消息，下面我们具体看一下Master端是如何注册Worker的，即registerWorker(worker)方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerWorker</span></span>(worker: <span class="type">WorkerInfo</span>): <span class="type">Boolean</span> = &#123;</div><div class="line">    <span class="comment">// There may be one or more refs to dead workers on this same node (w/ different ID's),</span></div><div class="line">    <span class="comment">// remove them.</span></div><div class="line">    workers.filter &#123; w =&gt;</div><div class="line">      (w.host == worker.host &amp;&amp; w.port == worker.port) &amp;&amp; (w.state == <span class="type">WorkerState</span>.<span class="type">DEAD</span>)</div><div class="line">    &#125;.foreach &#123; w =&gt;</div><div class="line">      workers -= w</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> workerAddress = worker.endpoint.address</div><div class="line">    <span class="keyword">if</span> (addressToWorker.contains(workerAddress)) &#123;</div><div class="line">      <span class="keyword">val</span> oldWorker = addressToWorker(workerAddress)</div><div class="line">      <span class="keyword">if</span> (oldWorker.state == <span class="type">WorkerState</span>.<span class="type">UNKNOWN</span>) &#123;</div><div class="line">        <span class="comment">// A worker registering from UNKNOWN implies that the worker was restarted during recovery.</span></div><div class="line">        <span class="comment">// The old worker must thus be dead, so we will remove it and accept the new worker.</span></div><div class="line">        removeWorker(oldWorker)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        logInfo(<span class="string">"Attempted to re-register worker at same address: "</span> + workerAddress)</div><div class="line">        <span class="keyword">return</span> <span class="literal">false</span></div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    workers += worker</div><div class="line">    idToWorker(worker.id) = worker</div><div class="line">    addressToWorker(workerAddress) = worker</div><div class="line">    <span class="literal">true</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>首先判断是否有和该Worker的host和port相同且状态为DEAD的Worker，如果有就remove掉，然后获得该Worker的RpcAddress，然后根据RpcAddress判断addressToWorker中是否有相同地址的记录，如果有记录且老的Worker的状态为UNKNOWN就remove掉老的Worker，如果没有记录就打日志并返回false（导致上一步reply：RegisterWorkerFailed）然后分别在workers、idToWorker、addressToWorker中添加该Worker，最后返回true，导致上一步向Worker reply注册成功的消息：context.reply(RegisteredWorker(self, masterWebUiUrl))，并执行schedule()，即向等待的applications分配当前可用的资源（每当新的application加入或者有资源变化时都会调用该方法），这个方法我会用单独的一片文章详细分析，现在我们先来看Worker端是如何进行回复的，回到上面的registerWithMaster方法（有参数的），我们直接看成功后执行的handleRegisterResponse(msg)这个方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">handleRegisterResponse</span></span>(msg: <span class="type">RegisterWorkerResponse</span>): <span class="type">Unit</span> = synchronized &#123;</div><div class="line">    msg <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">RegisteredWorker</span>(masterRef, masterWebUiUrl) =&gt;</div><div class="line">        logInfo(<span class="string">"Successfully registered with master "</span> + masterRef.address.toSparkURL)</div><div class="line">        registered = <span class="literal">true</span></div><div class="line">        changeMaster(masterRef, masterWebUiUrl)</div><div class="line">        forwordMessageScheduler.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</div><div class="line">            self.send(<span class="type">SendHeartbeat</span>)</div><div class="line">          &#125;</div><div class="line">        &#125;, <span class="number">0</span>, <span class="type">HEARTBEAT_MILLIS</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">        <span class="keyword">if</span> (<span class="type">CLEANUP_ENABLED</span>) &#123;</div><div class="line">          logInfo(</div><div class="line">            <span class="string">s"Worker cleanup enabled; old application directories will be deleted in: <span class="subst">$workDir</span>"</span>)</div><div class="line">          forwordMessageScheduler.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</div><div class="line">              self.send(<span class="type">WorkDirCleanup</span>)</div><div class="line">            &#125;</div><div class="line">          &#125;, <span class="type">CLEANUP_INTERVAL_MILLIS</span>, <span class="type">CLEANUP_INTERVAL_MILLIS</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">RegisterWorkerFailed</span>(message) =&gt;</div><div class="line">        <span class="keyword">if</span> (!registered) &#123;</div><div class="line">          logError(<span class="string">"Worker registration failed: "</span> + message)</div><div class="line">          <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">MasterInStandby</span> =&gt;</div><div class="line">        <span class="comment">// Ignore. Master not yet ready.</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>依然是模式匹配的方式：</p>
<ul>
<li>如果接受到的是RegisteredWorker，会执行changeMaster方法，取消最后一次的重试，然后向自己的RpcEnv发送SendHeartBeat消息，使用receive方法接受到该消息后会通过sendToMaster方法向Master发送心跳，最后判断CLEANUP_ENABLED如果开启就向自己的RpcEnv发送WorkDirCleanup消息，接受到消息后将老的application的目录清除</li>
<li>如果接受到的是RegisterWorkerFailed就表明注册失败</li>
</ul>
<p>changeMaster发送：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">changeMaster</span></span>(masterRef: <span class="type">RpcEndpointRef</span>, uiUrl: <span class="type">String</span>) &#123;</div><div class="line">    <span class="comment">// activeMasterUrl it's a valid Spark url since we receive it from master.</span></div><div class="line">    activeMasterUrl = masterRef.address.toSparkURL</div><div class="line">    activeMasterWebUiUrl = uiUrl</div><div class="line">    master = <span class="type">Some</span>(masterRef)</div><div class="line">    connected = <span class="literal">true</span></div><div class="line">    <span class="comment">// Cancel any outstanding re-registration attempts because we found a new master</span></div><div class="line">    cancelLastRegistrationRetry()</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>cancelLastRegistrationRetry:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">cancelLastRegistrationRetry</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">if</span> (registerMasterFutures != <span class="literal">null</span>) &#123;</div><div class="line">      registerMasterFutures.foreach(_.cancel(<span class="literal">true</span>))</div><div class="line">      registerMasterFutures = <span class="literal">null</span></div><div class="line">    &#125;</div><div class="line">    registrationRetryTimer.foreach(_.cancel(<span class="literal">true</span>))</div><div class="line">    registrationRetryTimer = <span class="type">None</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>如果Worker注册失败同样会通过registrationRetryTimer进行重试：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">registrationRetryTimer = <span class="type">Some</span>(forwordMessageScheduler.scheduleAtFixedRate(</div><div class="line">          <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</div><div class="line">              <span class="type">Option</span>(self).foreach(_.send(<span class="type">ReregisterWithMaster</span>))</div><div class="line">            &#125;</div><div class="line">          &#125;,</div><div class="line">          <span class="type">INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</div><div class="line">          <span class="type">INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</div><div class="line">          <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>))</div></pre></td></tr></table></figure>
<p>可以看到向自己发送重新注册的消息：ReregisterWithMaster，receive接收到后会执行reregisterWithMaster()方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">reregisterWithMaster</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="type">Utils</span>.tryOrExit &#123;</div><div class="line">      <span class="comment">//重试次数加1</span></div><div class="line">      connectionAttemptCount += <span class="number">1</span></div><div class="line">      <span class="keyword">if</span> (registered) &#123;</div><div class="line">        <span class="comment">//如果已经注册了，就取消重试</span></div><div class="line">        cancelLastRegistrationRetry()</div><div class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (connectionAttemptCount &lt;= <span class="type">TOTAL_REGISTRATION_RETRIES</span>) &#123;  <span class="comment">//判断是否超过最大重试次数</span></div><div class="line">        logInfo(<span class="string">s"Retrying connection to master (attempt # <span class="subst">$connectionAttemptCount</span>)"</span>)</div><div class="line">        <span class="comment">/**</span></div><div class="line">         * Re-register with the active master this worker has been communicating with. If there</div><div class="line">         * is none, then it means this worker is still bootstrapping and hasn't established a</div><div class="line">         * connection with a master yet, in which case we should re-register with all masters.</div><div class="line">         *</div><div class="line">         * It is important to re-register only with the active master during failures. Otherwise,</div><div class="line">         * if the worker unconditionally attempts to re-register with all masters, the following</div><div class="line">         * race condition may arise and cause a "duplicate worker" error detailed in SPARK-4592:</div><div class="line">         *</div><div class="line">         *   (1) Master A fails and Worker attempts to reconnect to all masters</div><div class="line">         *   (2) Master B takes over and notifies Worker</div><div class="line">         *   (3) Worker responds by registering with Master B</div><div class="line">         *   (4) Meanwhile, Worker's previous reconnection attempt reaches Master B,</div><div class="line">         *       causing the same Worker to register with Master B twice</div><div class="line">         *</div><div class="line">         * Instead, if we only register with the known active master, we can assume that the</div><div class="line">         * old master must have died because another master has taken over. Note that this is</div><div class="line">         * still not safe if the old master recovers within this interval, but this is a much</div><div class="line">         * less likely scenario.</div><div class="line">         */</div><div class="line">        master <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(masterRef) =&gt;</div><div class="line">            <span class="comment">// registered == false &amp;&amp; master != None means we lost the connection to master, so</span></div><div class="line">            <span class="comment">// masterRef cannot be used and we need to recreate it again. Note: we must not set</span></div><div class="line">            <span class="comment">// master to None due to the above comments.</span></div><div class="line">            <span class="comment">// 这里说的很清楚，如果注册失败了，但是master != None说明我们失去了和master的连接，所以需要重新创建一个masterRef</span></div><div class="line">            <span class="comment">// 先取消原来阻塞的用来等待消息回复的线程</span></div><div class="line">            <span class="keyword">if</span> (registerMasterFutures != <span class="literal">null</span>) &#123;</div><div class="line">              registerMasterFutures.foreach(_.cancel(<span class="literal">true</span>))</div><div class="line">            &#125;</div><div class="line">            </div><div class="line">            <span class="comment">// 然后创建新的masterRef，然后重新注册</span></div><div class="line">            <span class="keyword">val</span> masterAddress = masterRef.address</div><div class="line">            registerMasterFutures = <span class="type">Array</span>(registerMasterThreadPool.submit(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  logInfo(<span class="string">"Connecting to master "</span> + masterAddress + <span class="string">"..."</span>)</div><div class="line">                  <span class="keyword">val</span> masterEndpoint =</div><div class="line">                    rpcEnv.setupEndpointRef(<span class="type">Master</span>.<span class="type">SYSTEM_NAME</span>, masterAddress, <span class="type">Master</span>.<span class="type">ENDPOINT_NAME</span>)</div><div class="line">                  registerWithMaster(masterEndpoint)</div><div class="line">                &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                  <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// Cancelled</span></div><div class="line">                  <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logWarning(<span class="string">s"Failed to connect to master <span class="subst">$masterAddress</span>"</span>, e)</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">            &#125;))</div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">            <span class="comment">// 如果没有masterRef，先取消原来阻塞的用来等待消息回复的线程</span></div><div class="line">            <span class="keyword">if</span> (registerMasterFutures != <span class="literal">null</span>) &#123;</div><div class="line">              registerMasterFutures.foreach(_.cancel(<span class="literal">true</span>))</div><div class="line">            &#125;</div><div class="line">            </div><div class="line">            <span class="comment">// 然后执行最初的注册，即tryRegisterAllMasters</span></div><div class="line">            <span class="comment">// We are retrying the initial registration</span></div><div class="line">            registerMasterFutures = tryRegisterAllMasters()</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// We have exceeded the initial registration retry threshold</span></div><div class="line">        <span class="comment">// All retries from now on should use a higher interval</span></div><div class="line">        <span class="comment">// 如果超过刚开始设置的重试注册次数，取消之前的重试，开启新的注册，并改变重试次数和时间间隔</span></div><div class="line">        <span class="comment">// 刚开始的重试默认为6次，时间间隔在5到15秒之间，接下来的10次重试时间间隔在30到90秒之间</span></div><div class="line">        <span class="keyword">if</span> (connectionAttemptCount == <span class="type">INITIAL_REGISTRATION_RETRIES</span>) &#123;</div><div class="line">          registrationRetryTimer.foreach(_.cancel(<span class="literal">true</span>))</div><div class="line">          registrationRetryTimer = <span class="type">Some</span>(</div><div class="line">            forwordMessageScheduler.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</div><div class="line">                self.send(<span class="type">ReregisterWithMaster</span>)</div><div class="line">              &#125;</div><div class="line">            &#125;, <span class="type">PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</div><div class="line">              <span class="type">PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</div><div class="line">              <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>))</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        logError(<span class="string">"All masters are unresponsive! Giving up."</span>)</div><div class="line">        <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>至此Worker的启动和注册完成，即start-all.sh执行完成。</p>
<p>下面是Worker的初始化部分和onStart方法的源码及注释（重要部分）：</p>
<p>初始化部分：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> host = rpcEnv.address.host</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> port = rpcEnv.address.port</div><div class="line"></div><div class="line"><span class="type">Utils</span>.checkHost(host, <span class="string">"Expected hostname"</span>)</div><div class="line">assert (port &gt; <span class="number">0</span>)</div><div class="line"></div><div class="line"><span class="comment">// A scheduled executor used to send messages at the specified time.</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> forwordMessageScheduler =</div><div class="line">  <span class="type">ThreadUtils</span>.newDaemonSingleThreadScheduledExecutor(<span class="string">"worker-forward-message-scheduler"</span>)</div><div class="line"></div><div class="line"><span class="comment">// A separated thread to clean up the workDir. Used to provide the implicit parameter of `Future`</span></div><div class="line"><span class="comment">// methods.</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> cleanupThreadExecutor = <span class="type">ExecutionContext</span>.fromExecutorService(</div><div class="line">  <span class="type">ThreadUtils</span>.newDaemonSingleThreadExecutor(<span class="string">"worker-cleanup-thread"</span>))</div><div class="line"></div><div class="line"><span class="comment">// For worker and executor IDs</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createDateFormat</span> </span>= <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMMddHHmmss"</span>)</div><div class="line"><span class="comment">// 发送心跳的时间间隔：timeout的时间 / 4</span></div><div class="line"><span class="comment">// Send a heartbeat every (heartbeat timeout) / 4 milliseconds</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">HEARTBEAT_MILLIS</span> = conf.getLong(<span class="string">"spark.worker.timeout"</span>, <span class="number">60</span>) * <span class="number">1000</span> / <span class="number">4</span></div><div class="line"></div><div class="line"><span class="comment">// 重试的模型及其次数设置</span></div><div class="line"><span class="comment">// Model retries to connect to the master, after Hadoop's model.</span></div><div class="line"><span class="comment">// The first six attempts to reconnect are in shorter intervals (between 5 and 15 seconds)</span></div><div class="line"><span class="comment">// Afterwards, the next 10 attempts are between 30 and 90 seconds.</span></div><div class="line"><span class="comment">// A bit of randomness is introduced so that not all of the workers attempt to reconnect at</span></div><div class="line"><span class="comment">// the same time.</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">INITIAL_REGISTRATION_RETRIES</span> = <span class="number">6</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">TOTAL_REGISTRATION_RETRIES</span> = <span class="type">INITIAL_REGISTRATION_RETRIES</span> + <span class="number">10</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">FUZZ_MULTIPLIER_INTERVAL_LOWER_BOUND</span> = <span class="number">0.500</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">REGISTRATION_RETRY_FUZZ_MULTIPLIER</span> = &#123;</div><div class="line">  <span class="keyword">val</span> randomNumberGenerator = <span class="keyword">new</span> <span class="type">Random</span>(<span class="type">UUID</span>.randomUUID.getMostSignificantBits)</div><div class="line">  randomNumberGenerator.nextDouble + <span class="type">FUZZ_MULTIPLIER_INTERVAL_LOWER_BOUND</span></div><div class="line">&#125;</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS</span> = (math.round(<span class="number">10</span> *</div><div class="line">  <span class="type">REGISTRATION_RETRY_FUZZ_MULTIPLIER</span>))</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS</span> = (math.round(<span class="number">60</span></div><div class="line">  * <span class="type">REGISTRATION_RETRY_FUZZ_MULTIPLIER</span>))</div><div class="line"></div><div class="line"><span class="comment">//CLEANUP相关的设置</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">CLEANUP_ENABLED</span> = conf.getBoolean(<span class="string">"spark.worker.cleanup.enabled"</span>, <span class="literal">false</span>)</div><div class="line"><span class="comment">// How often worker will clean up old app folders</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">CLEANUP_INTERVAL_MILLIS</span> =</div><div class="line">  conf.getLong(<span class="string">"spark.worker.cleanup.interval"</span>, <span class="number">60</span> * <span class="number">30</span>) * <span class="number">1000</span></div><div class="line"><span class="comment">// TTL for app folders/data;  after TTL expires it will be cleaned up</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">APP_DATA_RETENTION_SECONDS</span> =</div><div class="line">  conf.getLong(<span class="string">"spark.worker.cleanup.appDataTtl"</span>, <span class="number">7</span> * <span class="number">24</span> * <span class="number">3600</span>)</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> testing: <span class="type">Boolean</span> = sys.props.contains(<span class="string">"spark.testing"</span>)</div><div class="line"><span class="comment">//对master的引用</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> master: <span class="type">Option</span>[<span class="type">RpcEndpointRef</span>] = <span class="type">None</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> activeMasterUrl: <span class="type">String</span> = <span class="string">""</span></div><div class="line"><span class="keyword">private</span>[worker] <span class="keyword">var</span> activeMasterWebUiUrl : <span class="type">String</span> = <span class="string">""</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> workerUri = rpcEnv.uriOf(systemName, rpcEnv.address, endpointName)</div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> registered = <span class="literal">false</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> connected = <span class="literal">false</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> workerId = generateWorkerId()</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> sparkHome =</div><div class="line">  <span class="keyword">if</span> (testing) &#123;</div><div class="line">    assert(sys.props.contains(<span class="string">"spark.test.home"</span>), <span class="string">"spark.test.home is not set!"</span>)</div><div class="line">    <span class="keyword">new</span> <span class="type">File</span>(sys.props(<span class="string">"spark.test.home"</span>))</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">new</span> <span class="type">File</span>(sys.env.get(<span class="string">"SPARK_HOME"</span>).getOrElse(<span class="string">"."</span>))</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="keyword">var</span> workDir: <span class="type">File</span> = <span class="literal">null</span></div><div class="line"><span class="keyword">val</span> finishedExecutors = <span class="keyword">new</span> <span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">ExecutorRunner</span>]</div><div class="line"><span class="keyword">val</span> drivers = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">DriverRunner</span>]</div><div class="line"><span class="keyword">val</span> executors = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">ExecutorRunner</span>]</div><div class="line"><span class="keyword">val</span> finishedDrivers = <span class="keyword">new</span> <span class="type">LinkedHashMap</span>[<span class="type">String</span>, <span class="type">DriverRunner</span>]</div><div class="line"><span class="keyword">val</span> appDirectories = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">String</span>]]</div><div class="line"><span class="keyword">val</span> finishedApps = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">String</span>]</div><div class="line"></div><div class="line"><span class="keyword">val</span> retainedExecutors = conf.getInt(<span class="string">"spark.worker.ui.retainedExecutors"</span>,</div><div class="line">  <span class="type">WorkerWebUI</span>.<span class="type">DEFAULT_RETAINED_EXECUTORS</span>)</div><div class="line"><span class="keyword">val</span> retainedDrivers = conf.getInt(<span class="string">"spark.worker.ui.retainedDrivers"</span>,</div><div class="line">  <span class="type">WorkerWebUI</span>.<span class="type">DEFAULT_RETAINED_DRIVERS</span>)</div><div class="line"></div><div class="line"><span class="comment">// The shuffle service is not actually started unless configured.</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> shuffleService = <span class="keyword">new</span> <span class="type">ExternalShuffleService</span>(conf, securityMgr)</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> publicAddress = &#123;</div><div class="line">  <span class="keyword">val</span> envVar = conf.getenv(<span class="string">"SPARK_PUBLIC_DNS"</span>)</div><div class="line">  <span class="keyword">if</span> (envVar != <span class="literal">null</span>) envVar <span class="keyword">else</span> host</div><div class="line">&#125;</div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> webUi: <span class="type">WorkerWebUI</span> = <span class="literal">null</span></div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> connectionAttemptCount = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> metricsSystem = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">"worker"</span>, conf, securityMgr)</div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> workerSource = <span class="keyword">new</span> <span class="type">WorkerSource</span>(<span class="keyword">this</span>)</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> registerMasterFutures: <span class="type">Array</span>[<span class="type">JFuture</span>[_]] = <span class="literal">null</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">var</span> registrationRetryTimer: <span class="type">Option</span>[<span class="type">JScheduledFuture</span>[_]] = <span class="type">None</span></div><div class="line"></div><div class="line"><span class="comment">// 用来和Master注册使用的线程池，默认线程的最大个数为Worker的个数</span></div><div class="line"><span class="comment">// A thread pool for registering with masters. Because registering with a master is a blocking</span></div><div class="line"><span class="comment">// action, this thread pool must be able to create "masterRpcAddresses.size" threads at the same</span></div><div class="line"><span class="comment">// time so that we can register with all masters.</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> registerMasterThreadPool = <span class="type">ThreadUtils</span>.newDaemonCachedThreadPool(</div><div class="line">  <span class="string">"worker-register-master-threadpool"</span>,</div><div class="line">  masterRpcAddresses.size <span class="comment">// Make sure we can register with all masters at the same time</span></div><div class="line">)</div><div class="line"></div><div class="line"><span class="keyword">var</span> coresUsed = <span class="number">0</span></div><div class="line"><span class="keyword">var</span> memoryUsed = <span class="number">0</span></div></pre></td></tr></table></figure>
<p>onStart()方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>() &#123;</div><div class="line">    assert(!registered)</div><div class="line">    logInfo(<span class="string">"Starting Spark worker %s:%d with %d cores, %s RAM"</span>.format(</div><div class="line">      host, port, cores, <span class="type">Utils</span>.megabytesToString(memory)))</div><div class="line">    logInfo(<span class="string">s"Running Spark version <span class="subst">$&#123;org.apache.spark.SPARK_VERSION&#125;</span>"</span>)</div><div class="line">    logInfo(<span class="string">"Spark home: "</span> + sparkHome)</div><div class="line">    <span class="comment">// 创建Work的目录</span></div><div class="line">    createWorkDir()</div><div class="line">    <span class="comment">// 开启 external shuffle service</span></div><div class="line">    shuffleService.startIfEnabled()</div><div class="line">    webUi = <span class="keyword">new</span> <span class="type">WorkerWebUI</span>(<span class="keyword">this</span>, workDir, webUiPort)</div><div class="line">    webUi.bind()</div><div class="line">    <span class="comment">// 向Master注册自己</span></div><div class="line">    registerWithMaster()</div><div class="line"></div><div class="line">    <span class="comment">// metrics系统</span></div><div class="line">    metricsSystem.registerSource(workerSource)</div><div class="line">    metricsSystem.start()</div><div class="line">    <span class="comment">// Attach the worker metrics servlet handler to the web ui after the metrics system is started.</span></div><div class="line">    metricsSystem.getServletHandlers.foreach(webUi.attachHandler)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>本文简单介绍了Spark的几种部署模式，并详细的分析了start-all.sh所执行源码（Master的启动和注册、Worker的启动和向Master的注册）的具体流程，当然Master的schedule方法并没有详细说明，我们会单独用一篇文章进行详细的分析。</p>
<p>本文参考和拓展阅读：</p>
<p><a href="https://github.com/apache/spark/tree/branch-1.6" target="_blank" rel="external">Spark-1.6.3源码</a></p>
<p><a href="https://github.com/apache/spark/tree/branch-2.1" target="_blank" rel="external">Spark-2.1.0源码</a></p>
<font color="#808080"><em>站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！</em></font>
        
        </div>
        <footer class="article-footer">
            <div class="share-container">


    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>


</div>

            
    
        <a href="http://www.sun4lower.cn/2017/02/25/sc-deploy/#comments" class="article-comment-link">Comments</a>
    

        </footer>
    </div>
    
</article>



    <article id="post-SparkRpcBasic" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2017/02/22/SparkRpcBasic/">Spark RPC 到底是个什么鬼？</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2017/02/22/SparkRpcBasic/">
            <time datetime="2017-02-22T09:49:24.000Z" itemprop="datePublished">2017-02-22</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/sparkc/">spark-core</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/netty/">Netty</a>, <a class="tag-link" href="/tags/prc/">RPC</a>, <a class="tag-link" href="/tags/spark/">spark</a>, <a class="tag-link" href="/tags/sparkc/">spark-core</a>, <a class="tag-link" href="/tags/bigdata/">大数据</a>, <a class="tag-link" href="/tags/event/">通信机制</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>本文会为大家介绍Spark中的RPC通信机制，详细阐述“Spark RPC到底是个什么鬼？”，闲话少叙，让我们来进入Spark RPC的世界！</p>
<h2 id="Spark-RPC三剑客"><a href="#Spark-RPC三剑客" class="headerlink" title="Spark RPC三剑客"></a>Spark RPC三剑客</h2><p>Spark RPC中最为重要的三个抽象（“三剑客”）为：RpcEnv、RpcEndpoint、RpcEndpointRef，这样做的好处有：</p>
<ul>
<li>对上层的API来说，屏蔽了底层的具体实现，使用方便</li>
<li>可以通过不同的实现来完成指定的功能，方便扩展</li>
<li>促进了底层实现层的良性竞争，Spark 1.6.3中默认使用了Netty作为底层的实现，但Akka的依赖依然存在；而Spark 2.1.0中的底层实现只有Netty，这样用户可以方便的使用不同版本的Akka或者将来某种更好的底层实现</li>
</ul>
<p>下面我们就结合Netty和“三剑客”来具体分析他们是如何来协同工作的。</p>
<h2 id="Send-a-message-locally"><a href="#Send-a-message-locally" class="headerlink" title="Send a message locally"></a>Send a message locally</h2><p>我们通过Spark源码中的一个Test（<a href="https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/rpc/RpcEnvSuite.scala" target="_blank" rel="external">RpcEnvSuite.scala</a>）来分析一下发送本地消息的具体流程，源码如下（对源码做了一些修改）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">test(<span class="string">"send a message locally"</span>) &#123;</div><div class="line">  <span class="meta">@volatile</span> <span class="keyword">var</span> message: <span class="type">String</span> = <span class="literal">null</span></div><div class="line">  <span class="keyword">val</span> rpcEndpointRef = env.setupEndpoint(<span class="string">"send-locally"</span>, <span class="keyword">new</span> <span class="type">RpcEndpoint</span> &#123;</div><div class="line">    <span class="keyword">override</span> <span class="keyword">val</span> rpcEnv = env</div><div class="line"></div><div class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span> </span>= &#123;</div><div class="line">      <span class="comment">//case msg: String =&gt; message = msg</span></div><div class="line">      <span class="keyword">case</span> msg: <span class="type">String</span> =&gt; println(message)  <span class="comment">//我们直接将接收到的消息打印出来</span></div><div class="line">    &#125;</div><div class="line">  &#125;)</div><div class="line">  rpcEndpointRef.send(<span class="string">"hello"</span>)</div><div class="line">  <span class="comment">//下面是原来的代码</span></div><div class="line">  <span class="comment">//eventually(timeout(5 seconds), interval(10 millis)) &#123;</span></div><div class="line">  <span class="comment">//  assert("hello" === message)</span></div><div class="line">  <span class="comment">//&#125;</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>为了方便理解，先把流程图贴出来，然后详细进行阐述：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/006y2nc1ly1fd1k9vickfj31gc1p4wob.jpg" alt=""></p>
<p>下面我们来详细阐述上例的具体过程：</p>
<p>首先是RpcEndpoint创建并注册的流程：（图中的蓝色线条部分）</p>
<ul>
<li>1、创建RpcEndpoint，并初始化rpcEnv的引用（RpcEnv已经创建好，底层实际上是实例化了一个NettyRpcEnv，而NettyRpcEnv是通过工厂方法NettyRpcEnvFactory创建的）</li>
<li>2、实例化RpcEndpoint之后需要向RpcEnv注册该RpcEndpoint，底层实现是向NettyRpcEnv进行注册，而实际上是通过调用Dispatcher的registerRpcEndpoint方法向Dispatcher进行注册</li>
<li>3、具体的注册就是向endpoints、endpointRefs、receivers中插入记录：而receivers中插入的信息会被Dispatcher中的线程池中的线程执行：会将记录take出来然后调用Inbox的process方法通过模式匹配的方法进行处理，注册的时候通过匹配到OnStart类型的message，去执行RpcEndpoint的onStart方法（例如Master、Worker注册时，就要执行各自的onStart方法），本例中未做任何操作</li>
<li>4、注册完成后返回RpcEndpointRef，我们通过RpcEndpointRef就可以向其代表的RpcEndpoint发送消息</li>
</ul>
<p>下面就是通过RpcEndpointRef向其代表的RpcEndpoint发送消息的具体流程：（图中的红色线条部分）</p>
<ul>
<li><p>1、2、调用RpcEndpointRef的send方法，底层实现是调用Netty的NettyRpcEndpointRef的send方法，而实际上又是调用的NettyRpcEnv的send方法，发送的消息使用RequestMessage进行封装：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nettyEnv.send(<span class="type">RequestMessage</span>(nettyEnv.address, <span class="keyword">this</span>, message))</div></pre></td></tr></table></figure>
</li>
<li><p>3、4、NettyRpcEnv的send方法首先会根据RpcAddress判断是本地还是远程调用，此处是同一个RpcEnv，所以是本地调用，即调用Dispatcher的postOneWayMessage方法</p>
</li>
<li>5、postOneWayMessage方法内部调用Dispatcher的postMessage方法</li>
<li>6、postMessage会向具体的RpcEndpoint发送消息，首先通过endpointName从endpoints中获得注册时的EndpointData，如果不为空就执行EndpointData中Inbox的post(message)方法，向Inbox的mesages中插入一条InboxMessage，同时向receivers中插入一条记录，此处将Inbox单独画出来是为了方便大家理解</li>
<li>7、Dispatcher中的线程池会拿出一条线程用来循环receivers中的消息，首先使用take方法获得receivers中的一条记录，然后调用Inbox的process方法来执行这条记录，而process将messages中的一条InboxMessage（第6步中插入的）拿出来进行处理，具体的处理方法就是通过模式匹配的方法，匹配到消息的类型（此处是OneWayMessage），然后来执行RpcEndpoint中对应的receive方法，在此例中我们只打印出这条消息（步骤8）</li>
</ul>
<p>至此，一个简单的发送本地消息的流程执行完成。</p>
<p>什么，上面的图太复杂了？我也觉得，下面给出一张简洁的图：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/006y2nc1ly1fd1iphxqe1j30dw0bqglj.jpg" alt=""></p>
<p>我们通过NettyRpcEndpointRef来发出一个消息，消息经过NettyRpcEnv、Dispatcher、Inbox的共同处理最终将消息发送到NettyRpcEndpoint，NettyRpcEndpoint收到消息后进行处理（一般是通过模式匹配的方式进行不同的处理）</p>
<p>如果进一步的进行抽象就得到了我们刚开始所讲的“三剑客”：RpcEnv、RpcEndpoint、RpcEndpointRef</p>
<p><img src="http://wx1.sinaimg.cn/mw690/006y2nc1ly1fd1jh9wem9j30dw08wglh.jpg" alt=""></p>
<p>RpcEndpointRef发送消息给RpcEnv，RpcEnv查询注册信息将消息路由到指定的RpcEndpoint，RpcEndpoint接收到消息后进行处理（模式匹配的方式）</p>
<p>RpcEndpoint的声明周期：constructor -&gt; onStart -&gt; receive* -&gt; onStop</p>
<p>其中receive*包括receive和receiveAndReply</p>
<p>本文我们只是通过一个简单的测试程序分析了Spark Rpc底层的实现，集群中的其它通信（比如Master和Woker的通信）的原理和这个测试类似，只不过具体的发送方式有所不同（包括ask、askWithRetry等），而且远程发消息的时候使用了OutBox和NIO等相关的内容，感兴趣的朋友可以对源码进行详细的阅读，本文不一一说明，目的就是通过简单的测试理解大致流程，不再为“Spark Rpc到底是什么”而纠结，一句话总结：Spark Rpc就是Spark中对分布式消息通信系统的高度抽象。</p>
<p>本文参考和拓展阅读：</p>
<p><a href="https://github.com/apache/spark" target="_blank" rel="external">spark源码</a></p>
<p><a href="http://netty.io" target="_blank" rel="external">Netty官方网站</a></p>
<p><a href="http://tutorials.jenkov.com/java-nio/index.html" target="_blank" rel="external">Java NIO Tutorial</a></p>
<font color="#808080"><em>站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！</em></font>
        
        </div>
        <footer class="article-footer">
            <div class="share-container">


    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>


</div>

            
    
        <a href="http://www.sun4lower.cn/2017/02/22/SparkRpcBasic/#comments" class="article-comment-link">Comments</a>
    

        </footer>
    </div>
    
</article>



    <article id="post-event-threadmodel" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2017/02/22/event-threadmodel/">浅显易懂之线程模型的演变</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2017/02/22/event-threadmodel/">
            <time datetime="2017-02-22T09:49:24.000Z" itemprop="datePublished">2017-02-22</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/event/">通信机制</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/netty/">Netty</a>, <a class="tag-link" href="/tags/多线程/">多线程</a>, <a class="tag-link" href="/tags/event/">通信机制</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>本文旨在通过一个实际工作中例子让大家理解线程模型的演变，从而对 Netty 的模型有一个粗略的印象，面向的是想初步了解 Netty 原理的读者，而并不关心具体的执行细节和相关术语的描述。</p>
<h2 id="单线程"><a href="#单线程" class="headerlink" title="单线程"></a>单线程</h2><p>单线程映射到我们的实际工作中就是由一个人完成所有的工作，如下图所示：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/006y2nc1ly1fczgkkhhnij30tp0abdfw.jpg" alt=""></p>
<p>A 自己独立的完成全部的Job</p>
<h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>多线程映射到我们的工作中就是多个人共同协作完成工作，如下图所示：</p>
<p><img src="http://wx3.sinaimg.cn/mw690/006y2nc1ly1fczgko8yjkj30t60ad3yl.jpg" alt=""></p>
<p>A、B、C 三人工作完成工作，每个人会分到具体的工作去执行，较之单线程，执行效率提高。</p>
<h2 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h2><p>多个 Worker（A、B、C等）已经准备好去完成即将到来的Job，当Job 1过来的时候，会从线程池中选择一个线程也就是Worker（A）来完成这项任务，完成之后A仍然要回到线程池中，等待下一个工作的到来。</p>
<p><img src="http://wx3.sinaimg.cn/mw690/006y2nc1ly1fczgkrlix3j30t60acdgd.jpg" alt=""></p>
<h2 id="Reactor单线程模型"><a href="#Reactor单线程模型" class="headerlink" title="Reactor单线程模型"></a>Reactor单线程模型</h2><p>Reactor 的单线程模型映射到实际工作中如下图所示：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/006y2nc1ly1fczgkvd866j30v10n1ta6.jpg" alt=""></p>
<p>我们有一个Boss来接收具体的Job，然后Boss将具体接收到的Job分配给已经准备好的Worker，交给具体的Worker来完成最终的Job，但是如果Job非常多的时候，Boss的压力就会越来越大，所以Boss成为了性能的瓶颈，而下面提到的Reactor多线程模型就是为了解决这个问题而产生的。</p>
<h2 id="Reactor多线程模型"><a href="#Reactor多线程模型" class="headerlink" title="Reactor多线程模型"></a>Reactor多线程模型</h2><p>为了完善Reactor单线程模型，Reactor多线程模型进行了优化：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/006y2nc1ly1fczkpd3gvaj30v013dwhl.jpg" alt=""></p>
<p>和Reactor单线程模型相比，Reactor多线程模型增加了一个从Manager pool选择Manager(相当于项目经理)的过程，即通过Manager来帮助Boss分配具体的Job给Worker，例如图中选择了M1，那么M1就会去完成选择Worker的工作，虚线代表选择了M2或者M3。</p>
<p>Netty的具体实现就类似于Reactor的多线程模型，而Spark现在Rpc的底层就是通过Netty来实现的。</p>
<p>至此本文的目的已经达到，如果想要更加详细的了解Netty的具体实现细节和NIO相关的知识可以参考如下文章：</p>
<p><a href="http://www.infoq.com/cn/articles/netty-threading-model?utm_source=infoq&amp;utm_campaign=user_page&amp;utm_medium=link" target="_blank" rel="external">Netty系列之Netty线程模型</a></p>
<p><a href="http://ifeve.com/category/netty/" target="_blank" rel="external">并发编程网有关Netty的部分</a></p>
<p><a href="http://netty.io" target="_blank" rel="external">Netty官方网站</a></p>
<p><a href="http://tutorials.jenkov.com/java-nio/index.html" target="_blank" rel="external">Java NIO Tutorial</a></p>
<font color="#808080"><em>站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！</em></font>
        
        </div>
        <footer class="article-footer">
            <div class="share-container">


    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>


</div>

            
    
        <a href="http://www.sun4lower.cn/2017/02/22/event-threadmodel/#comments" class="article-comment-link">Comments</a>
    

        </footer>
    </div>
    
</article>



    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/2017/02/19/hello-world/">Hello World</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2017/02/19/hello-world/">
            <time datetime="2017-02-19T04:52:30.000Z" itemprop="datePublished">2017-02-19</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/sparkc/">spark-core</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/sparkc/">spark-core</a>, <a class="tag-link" href="/tags/bigdata/">大数据</a>, <a class="tag-link" href="/tags/perf/">性能调优</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</div><div class="line">	System.out.print(<span class="string">"hello world"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>): <span class="type">Int</span> = a + b</div><div class="line">println(<span class="string">"hello world"</span>)</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
<p>hello world</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">


    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>


</div>

            
    
        <a href="http://www.sun4lower.cn/2017/02/19/hello-world/#comments" class="article-comment-link">Comments</a>
    

        </footer>
    </div>
    
</article>


</section>
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">recent</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a></p>
                            <p class="item-title"><a href="/2017/03/02/sc-sparkshell/" class="title">Spark-Core源码精读(2)、spark-shell(spark-submit)流程详解</a></p>
                            <p class="item-date"><time datetime="2017-03-02T05:28:17.000Z" itemprop="datePublished">2017-03-02</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a></p>
                            <p class="item-title"><a href="/2017/02/28/sc-schedule/" class="title">Spark-Core源码精读(2)、Master中的schedule详解</a></p>
                            <p class="item-date"><time datetime="2017-02-28T08:17:02.000Z" itemprop="datePublished">2017-02-28</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a></p>
                            <p class="item-title"><a href="/2017/02/25/sc-deploy/" class="title">Spark-Core源码精读(1)、Spark Deployment &amp; start-all.sh on Standalone mode</a></p>
                            <p class="item-date"><time datetime="2017-02-25T05:58:04.000Z" itemprop="datePublished">2017-02-25</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a></p>
                            <p class="item-title"><a href="/2017/02/22/SparkRpcBasic/" class="title">Spark RPC 到底是个什么鬼？</a></p>
                            <p class="item-date"><time datetime="2017-02-22T09:49:24.000Z" itemprop="datePublished">2017-02-22</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/event/">通信机制</a></p>
                            <p class="item-title"><a href="/2017/02/22/event-threadmodel/" class="title">浅显易懂之线程模型的演变</a></p>
                            <p class="item-date"><time datetime="2017-02-22T09:49:24.000Z" itemprop="datePublished">2017-02-22</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/">大数据</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/spark/">spark</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/spark/sparkc/">spark-core</a><span class="category-list-count">5</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/event/">通信机制</a><span class="category-list-count">1</span></li></ul></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">5</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">Netty</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/REPL/">REPL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/prc/">RPC</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SparkContext/">SparkContext</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SqlContext/">SqlContext</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deployment/">deployment</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/driver/">driver</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/executor/">executor</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/master/">master</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mesos/">mesos</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/schedule/">schedule</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-class/">spark-class</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sparkc/">spark-core</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-shell/">spark-shell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-submit/">spark-submit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/standalone/">standalone</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/worker/">worker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/yarn/">yarn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/多线程/">多线程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bigdata/">大数据</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/perf/">性能调优</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/event/">通信机制</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>

    
        
<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
    <div class="widget-wrap">
        <h3 class="widget-title">tag cloud</h3>
        <div id="myCanvasContainer" class="widget tagcloud">
            <canvas width="250" height="250" id="resCanvas" style="width=100%">
                <a href="/tags/netty/" style="font-size: 13.33px;">Netty</a> <a href="/tags/REPL/" style="font-size: 10px;">REPL</a> <a href="/tags/prc/" style="font-size: 13.33px;">RPC</a> <a href="/tags/SparkContext/" style="font-size: 10px;">SparkContext</a> <a href="/tags/SqlContext/" style="font-size: 10px;">SqlContext</a> <a href="/tags/deployment/" style="font-size: 10px;">deployment</a> <a href="/tags/driver/" style="font-size: 10px;">driver</a> <a href="/tags/executor/" style="font-size: 10px;">executor</a> <a href="/tags/master/" style="font-size: 13.33px;">master</a> <a href="/tags/mesos/" style="font-size: 10px;">mesos</a> <a href="/tags/schedule/" style="font-size: 10px;">schedule</a> <a href="/tags/spark/" style="font-size: 16.67px;">spark</a> <a href="/tags/spark-class/" style="font-size: 10px;">spark-class</a> <a href="/tags/sparkc/" style="font-size: 20px;">spark-core</a> <a href="/tags/spark-shell/" style="font-size: 10px;">spark-shell</a> <a href="/tags/spark-submit/" style="font-size: 10px;">spark-submit</a> <a href="/tags/standalone/" style="font-size: 10px;">standalone</a> <a href="/tags/worker/" style="font-size: 10px;">worker</a> <a href="/tags/yarn/" style="font-size: 10px;">yarn</a> <a href="/tags/多线程/" style="font-size: 10px;">多线程</a> <a href="/tags/bigdata/" style="font-size: 20px;">大数据</a> <a href="/tags/perf/" style="font-size: 10px;">性能调优</a> <a href="/tags/event/" style="font-size: 13.33px;">通信机制</a>
            </canvas>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://spark.apache.org">Spark</a>
                    </li>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
	    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次, 访客数 <span id="busuanzi_value_site_uv"></span> 人次, 本文总阅读量 <span id="busuanzi_value_page_pv"></span> 次<br>
            &copy; 2017 Sunflower<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
        </div>
    </div>
</footer>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
        
    
    <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2125727"></script>



    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>