<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>Spark-Core源码精读(2)、spark-shell(spark-submit)流程详解 | Sunflower</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="本文将解读使用spark-shell的方式进入REPL的具体流程。
注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。
shell部分下面我们来看一下当我们输入 spark-shell –master spark://master:7077时具体的执行流程，首先当然是看一下spark-shell.sh的源码，我们只选取了相对比">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-Core源码精读(2)、spark-shell(spark-submit)流程详解">
<meta property="og:url" content="http://www.sun4lower.cn/2017/03/02/sc-sparkshell/index.html">
<meta property="og:site_name" content="Sunflower">
<meta property="og:description" content="本文将解读使用spark-shell的方式进入REPL的具体流程。
注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。
shell部分下面我们来看一下当我们输入 spark-shell –master spark://master:7077时具体的执行流程，首先当然是看一下spark-shell.sh的源码，我们只选取了相对比">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/006y2nc1ly1fd8fholzmbj30xe16577s.jpg">
<meta property="og:updated_time" content="2017-03-02T05:33:14.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark-Core源码精读(2)、spark-shell(spark-submit)流程详解">
<meta name="twitter:description" content="本文将解读使用spark-shell的方式进入REPL的具体流程。
注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。
shell部分下面我们来看一下当我们输入 spark-shell –master spark://master:7077时具体的执行流程，首先当然是看一下spark-shell.sh的源码，我们只选取了相对比">
<meta name="twitter:image" content="http://wx1.sinaimg.cn/mw690/006y2nc1ly1fd8fholzmbj30xe16577s.jpg">
    

    

    
        <link rel="icon" href="/sunflower.ico" />
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?4d423c10c1a9ce21e23383733c383235";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>

    


</head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Sunflower</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/.">Home</a>
                
                    <a class="main-nav-link" href="/archives">Archives</a>
                
                    <a class="main-nav-link" href="/categories">Categories</a>
                
                    <a class="main-nav-link" href="/tags">Tags</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/css/images/avatar.png" />
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/.">Home</a></td>
                
                    <td><a class="main-nav-link" href="/archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/css/images/avatar.png" />
            <h2 id="name">Sunflower</h2>
            <h3 id="title">Studying Spark &amp; Bigdata</h3>
            <span id="location"><i class="fa fa-map-marker"></i>ShangHai, China</span>
            <a id="follow" target="_blank" href="https://github.com/sun4lower/">FOLLOW</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                6
                <span>posts</span>
            </div>
            <div class="article-info-block">
                23
                <span>tags</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="http://github.com/sun4lower" target="_blank" title="github" class=tooltip>
                            <i class="fa fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="http://wpa.qq.com/msgrd?v=3&uin=1054844760&site=qq&menu=yes" target="_blank" title="qq" class=tooltip>
                            <i class="fa fa-qq"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="http://stackoverflow.com/users/7587826/sunflower" target="_blank" title="stack-overflow" class=tooltip>
                            <i class="fa fa-stack-overflow"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="mailto:sun4lower@163.com" target="_blank" title="send" class=tooltip>
                            <i class="fa fa-send"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main"><article id="post-sc-sparkshell" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
            Spark-Core源码精读(2)、spark-shell(spark-submit)流程详解
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2017/03/02/sc-sparkshell/">
            <time datetime="2017-03-02T05:28:17.000Z" itemprop="datePublished">2017-03-02</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/sparkc/">spark-core</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/REPL/">REPL</a>, <a class="tag-link" href="/tags/SparkContext/">SparkContext</a>, <a class="tag-link" href="/tags/SqlContext/">SqlContext</a>, <a class="tag-link" href="/tags/spark/">spark</a>, <a class="tag-link" href="/tags/spark-class/">spark-class</a>, <a class="tag-link" href="/tags/sparkc/">spark-core</a>, <a class="tag-link" href="/tags/spark-shell/">spark-shell</a>, <a class="tag-link" href="/tags/spark-submit/">spark-submit</a>, <a class="tag-link" href="/tags/bigdata/">大数据</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p>本文将解读使用spark-shell的方式进入REPL的具体流程。</p>
<p><strong>注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。</strong></p>
<h2 id="shell部分"><a href="#shell部分" class="headerlink" title="shell部分"></a>shell部分</h2><p>下面我们来看一下当我们输入 spark-shell –master spark://master:7077时具体的执行流程，首先当然是看一下spark-shell.sh的源码，我们只选取了相对比较重要的部分：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">##检测有没有设置SPARK_HOME环境变量，如果没有进行设置</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">export</span> SPARK_HOME=<span class="string">"<span class="variable">$(cd "`dirname "$0"`"/..; pwd)</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"><span class="comment">##...</span></div><div class="line"><span class="keyword">function</span> <span class="function"><span class="title">main</span></span>() &#123;</div><div class="line">  <span class="keyword">if</span> <span class="variable">$cygwin</span>; <span class="keyword">then</span></div><div class="line">    <span class="comment"># Workaround for issue involving JLine and Cygwin</span></div><div class="line">    <span class="comment"># (see http://sourceforge.net/p/jline/bugs/40/).</span></div><div class="line">    <span class="comment"># If you're using the Mintty terminal emulator in Cygwin, may need to set the</span></div><div class="line">    <span class="comment"># "Backspace sends ^H" setting in "Keys" section of the Mintty options</span></div><div class="line">    <span class="comment"># (see https://github.com/sbt/sbt/issues/562).</span></div><div class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</div><div class="line">    <span class="built_in">export</span> SPARK_SUBMIT_OPTS=<span class="string">"<span class="variable">$SPARK_SUBMIT_OPTS</span> -Djline.terminal=unix"</span></div><div class="line">    <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-submit --class org.apache.spark.repl.Main --name <span class="string">"Spark shell"</span> <span class="string">"<span class="variable">$@</span>"</span></div><div class="line">    stty icanon <span class="built_in">echo</span> &gt; /dev/null 2&gt;&amp;1</div><div class="line">  <span class="keyword">else</span></div><div class="line">    <span class="built_in">export</span> SPARK_SUBMIT_OPTS</div><div class="line">    <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-submit --class org.apache.spark.repl.Main --name <span class="string">"Spark shell"</span> <span class="string">"<span class="variable">$@</span>"</span></div><div class="line">  <span class="keyword">fi</span></div><div class="line">&#125;</div><div class="line"><span class="comment">##...</span></div><div class="line">main <span class="string">"<span class="variable">$@</span>"</span></div><div class="line"><span class="comment">##...</span></div></pre></td></tr></table></figure>
<p>可以看出最后执行的是main方法并传入我们使用spark-shell命令时候的所有参数，比如–master，而main方法中无论是什么操作系统(当然生产环境是linux系统)都会最终执行spark-submit，并且class为org.apache.spark.repl.Main、name为“Spark shell”并且将spark-shell所有接收到的用户输入的参数一起传进去，下面我们来看spark-submit：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">export</span> SPARK_HOME=<span class="string">"<span class="variable">$(cd "`dirname "$0"`"/..; pwd)</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment"># disable randomized hash for string in Python 3.3+</span></div><div class="line"><span class="built_in">export</span> PYTHONHASHSEED=0</div><div class="line"></div><div class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure>
<p>spark-submit的代码比较简洁，最后使用exec通过spark-class来启动SparkSubmit并将spark-submit接收到的所有参数传入，下面我们来看一下spark-class：(<em>这里要说明一下，从这里开始起始就是我们通过spark-submit提交application的过程，只不过spark-submit提交的application运行完成后就会结束，而REPL一直等待用户的输入</em>)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">export</span> SPARK_HOME=<span class="string">"<span class="variable">$(cd "`dirname "$0"`"/..; pwd)</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment">## 载入环境变量</span></div><div class="line">. <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/load-spark-env.sh</div><div class="line"></div><div class="line"><span class="comment">## 获得java的二进制文件，后面会用来启动一个JVM进行</span></div><div class="line"><span class="comment"># Find the java binary</span></div><div class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$&#123;JAVA_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  RUNNER=<span class="string">"<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/java"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">  <span class="keyword">if</span> [ `<span class="built_in">command</span> -v java` ]; <span class="keyword">then</span></div><div class="line">    RUNNER=<span class="string">"java"</span></div><div class="line">  <span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"JAVA_HOME is not set"</span> &gt;&amp;2</div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line">  <span class="keyword">fi</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment">## jar包的相关依赖</span></div><div class="line"><span class="comment"># Find assembly jar</span></div><div class="line">SPARK_ASSEMBLY_JAR=</div><div class="line"><span class="keyword">if</span> [ <span class="_">-f</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/RELEASE"</span> ]; <span class="keyword">then</span></div><div class="line">  ASSEMBLY_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/lib"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">  ASSEMBLY_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/assembly/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line">GREP_OPTIONS=</div><div class="line">num_jars=<span class="string">"<span class="variable">$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" | wc -l)</span>"</span></div><div class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$num_jars</span>"</span> <span class="_">-eq</span> <span class="string">"0"</span> <span class="_">-a</span> -z <span class="string">"<span class="variable">$SPARK_ASSEMBLY_JAR</span>"</span> <span class="_">-a</span> <span class="string">"<span class="variable">$SPARK_PREPEND_CLASSES</span>"</span> != <span class="string">"1"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">echo</span> <span class="string">"Failed to find Spark assembly in <span class="variable">$ASSEMBLY_DIR</span>."</span> 1&gt;&amp;2</div><div class="line">  <span class="built_in">echo</span> <span class="string">"You need to build Spark before running this program."</span> 1&gt;&amp;2</div><div class="line">  <span class="built_in">exit</span> 1</div><div class="line"><span class="keyword">fi</span></div><div class="line"><span class="keyword">if</span> [ <span class="_">-d</span> <span class="string">"<span class="variable">$ASSEMBLY_DIR</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  ASSEMBLY_JARS=<span class="string">"<span class="variable">$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" || true)</span>"</span></div><div class="line">  <span class="keyword">if</span> [ <span class="string">"<span class="variable">$num_jars</span>"</span> <span class="_">-gt</span> <span class="string">"1"</span> ]; <span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"Found multiple Spark assembly jars in <span class="variable">$ASSEMBLY_DIR</span>:"</span> 1&gt;&amp;2</div><div class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$ASSEMBLY_JARS</span>"</span> 1&gt;&amp;2</div><div class="line">    <span class="built_in">echo</span> <span class="string">"Please remove all but one jar."</span> 1&gt;&amp;2</div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line">  <span class="keyword">fi</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line">SPARK_ASSEMBLY_JAR=<span class="string">"<span class="variable">$&#123;ASSEMBLY_DIR&#125;</span>/<span class="variable">$&#123;ASSEMBLY_JARS&#125;</span>"</span></div><div class="line"></div><div class="line">LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$SPARK_ASSEMBLY_JAR</span>"</span></div><div class="line"></div><div class="line"><span class="comment"># Add the launcher build dir to the classpath if requested.</span></div><div class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$SPARK_PREPEND_CLASSES</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/launcher/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/classes:<span class="variable">$LAUNCH_CLASSPATH</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> _SPARK_ASSEMBLY=<span class="string">"<span class="variable">$SPARK_ASSEMBLY_JAR</span>"</span></div><div class="line"></div><div class="line"><span class="comment"># For tests</span></div><div class="line"><span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$SPARK_TESTING</span>"</span> ]]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">unset</span> YARN_CONF_DIR</div><div class="line">  <span class="built_in">unset</span> HADOOP_CONF_DIR</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment"># The launcher library will print arguments separated by a NULL character, to allow arguments with</span></div><div class="line"><span class="comment"># characters that would be otherwise interpreted by the shell. Read that in a while loop, populating</span></div><div class="line"><span class="comment"># an array that will be used to exec the final command.</span></div><div class="line">CMD=()</div><div class="line"><span class="keyword">while</span> IFS= <span class="built_in">read</span> <span class="_">-d</span> <span class="string">''</span> -r ARG; <span class="keyword">do</span></div><div class="line">  CMD+=(<span class="string">"<span class="variable">$ARG</span>"</span>)</div><div class="line">  <span class="comment">## 使用java -cp命令启动一个JVM进程并执行org.apache.spark.launcher.Main类的main方法，后面我们会看到这个进程就是SparkSubmit进程</span></div><div class="line"><span class="keyword">done</span> &lt; &lt;(<span class="string">"<span class="variable">$RUNNER</span>"</span> -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span>)</div><div class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$&#123;CMD[@]&#125;</span>"</span></div></pre></td></tr></table></figure>
<p>spark-class是Spark应用程序的命令行启动器，负责设置JVM环境并执行Spark的应用程序，这里我们执行的就是SparkSubmit，下面我们就进入到Spark源码的部分。</p>
<h2 id="Spark源码部分"><a href="#Spark源码部分" class="headerlink" title="Spark源码部分"></a>Spark源码部分</h2><p>承接上文，我们直接进入Spark的源码：</p>
<p>关于org.apache.spark.launcher.Main的源码我们这里不做说明，大家可以把它看成Spark应用程序命令行的启动器，我们主要关注Spark本身，所以直接进入SparkSubmit的源码部分：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">/** 使用SparkSubmitArguments封装spark-submit传入的参数，还记得都有什么吗？</span></div><div class="line">  如果是spark-shell，就包括spark-shell及后面的一串参数，如果是直接使用spark-submit进行提交</div><div class="line">  后面就是提交时传入的参数，由于SparkSubmitArguments中的参数比较多，本文中不再一一列出</div><div class="line">  会在使用到某个参数的时候进行说明，详细的参数可以参看SparkSubmitArguments的源码。</div><div class="line">  */</div><div class="line">  <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args)</div><div class="line">  <span class="comment">// 如果开启了debug模式就打印出参数</span></div><div class="line">  <span class="keyword">if</span> (appArgs.verbose) &#123;</div><div class="line">    <span class="comment">// scalastyle:off println</span></div><div class="line">    printStream.println(appArgs)</div><div class="line">    <span class="comment">// scalastyle:on println</span></div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">/** 这里的action就是spark-submit执行的动作，包括：SUBMIT, KILL, REQUEST_STATUS(使</span></div><div class="line">  用了SparkSubmitAction进行了封装)，如果没有指定，默认就是SparkSubmitAction.SUBMIT，</div><div class="line">  所以下面的这个模式匹配将执行submit(appArgs)</div><div class="line">  */</div><div class="line">  appArgs.action <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs)</div><div class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</div><div class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下面我们来看submit(appArgs)方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">submit方法的主要功能就是使用传进来的参数来提交应用程序。</div><div class="line">主要分为两步骤：</div><div class="line">1. 准备启动所需的环境，包括设置classpath、系统参数和应用程序的参数(根据部署模式和cluster</div><div class="line">manager运行child main类)</div><div class="line">2. 使用上一步准备好的环境调用child main class中的main函数，如果是spark-shell，child</div><div class="line">main class就是org.apache.spark.repl.Main，如果是spark-submit直接进行提交，child</div><div class="line">main class就是用户编写的应用程序(含有main方法的类)</div><div class="line">*/</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(args: <span class="type">SparkSubmitArguments</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">// 准备环境，主要就是获得childMainClass，即我们上面所说的child main class</span></div><div class="line">  <span class="keyword">val</span> (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)</div><div class="line">   <span class="comment">// 注意：源码中这里是doRunMain()方法，我们在后面单独拿出来进行分析</span></div><div class="line">   <span class="comment">// 判断gateway使用的是Akka还是基于REST的，但是不论那种方式最后都会调用doRunMain()方法</span></div><div class="line">   <span class="comment">// In standalone cluster mode, there are two submission gateways:</span></div><div class="line">   <span class="comment">//   (1) The traditional Akka gateway using o.a.s.deploy.Client as a wrapper</span></div><div class="line">   <span class="comment">//   (2) The new REST-based gateway introduced in Spark 1.3</span></div><div class="line">   <span class="comment">// The latter is the default behavior as of Spark 1.3, but Spark submit will fail over</span></div><div class="line">   <span class="comment">// to use the legacy gateway if the master endpoint turns out to be not a REST server.</span></div><div class="line">  <span class="keyword">if</span> (args.isStandaloneCluster &amp;&amp; args.useRest) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">// scalastyle:off println</span></div><div class="line">      printStream.println(<span class="string">"Running Spark using the REST application submission protocol."</span>)</div><div class="line">      <span class="comment">// scalastyle:on println</span></div><div class="line">      doRunMain()</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// Fail over to use the legacy submission gateway</span></div><div class="line">      <span class="keyword">case</span> e: <span class="type">SubmitRestConnectionException</span> =&gt;</div><div class="line">        printWarning(<span class="string">s"Master endpoint <span class="subst">$&#123;args.master&#125;</span> was not a REST server. "</span> +</div><div class="line">          <span class="string">"Falling back to legacy submission gateway instead."</span>)</div><div class="line">        args.useRest = <span class="literal">false</span></div><div class="line">        submit(args)</div><div class="line">    &#125;</div><div class="line">  <span class="comment">// In all other modes, just run the main class as prepared</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    doRunMain()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>doRunMain()的实现部分：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">doRunMain</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (args.proxyUser != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">// 这里是hadoop相关的用户和组的信息</span></div><div class="line">    <span class="keyword">val</span> proxyUser = <span class="type">UserGroupInformation</span>.createProxyUser(args.proxyUser,</div><div class="line">      <span class="type">UserGroupInformation</span>.getCurrentUser())</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      proxyUser.doAs(<span class="keyword">new</span> <span class="type">PrivilegedExceptionAction</span>[<span class="type">Unit</span>]() &#123;</div><div class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">          runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class="line">        &#125;</div><div class="line">      &#125;)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">        <span class="comment">// Hadoop's AuthorizationException suppresses the exception's stack trace, which</span></div><div class="line">        <span class="comment">// makes the message printed to the output by the JVM not very helpful. Instead,</span></div><div class="line">        <span class="comment">// detect exceptions with empty stack traces here, and treat them differently.</span></div><div class="line">        <span class="keyword">if</span> (e.getStackTrace().length == <span class="number">0</span>) &#123;</div><div class="line">          <span class="comment">// scalastyle:off println</span></div><div class="line">          printStream.println(<span class="string">s"ERROR: <span class="subst">$&#123;e.getClass().getName()&#125;</span>: <span class="subst">$&#123;e.getMessage()&#125;</span>"</span>)</div><div class="line">          <span class="comment">// scalastyle:on println</span></div><div class="line">          exitFn(<span class="number">1</span>)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">throw</span> e</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们看到doRunMain()内部最终都执行了runMain方法，所以我们进入runMain方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** 别看这个方法这么长，主要做的事情就是一件：运行child main class的main方法</span></div><div class="line">再次说明一下，如果是直接使用spark-submit提交的应用程序，就是执行用户指定的类的main方法</div><div class="line">如果是通过spark-shell执行的，就是执行org.apache.spark.repl.Main中的main方法</div><div class="line">*/</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runMain</span></span>(</div><div class="line">    childArgs: <span class="type">Seq</span>[<span class="type">String</span>],</div><div class="line">    childClasspath: <span class="type">Seq</span>[<span class="type">String</span>],</div><div class="line">    sysProps: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</div><div class="line">    childMainClass: <span class="type">String</span>,</div><div class="line">    verbose: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">//是否打印debug信息</span></div><div class="line">  <span class="comment">// scalastyle:off println</span></div><div class="line">  <span class="keyword">if</span> (verbose) &#123;</div><div class="line">    printStream.println(<span class="string">s"Main class:\n<span class="subst">$childMainClass</span>"</span>)</div><div class="line">    printStream.println(<span class="string">s"Arguments:\n<span class="subst">$&#123;childArgs.mkString("\n")&#125;</span>"</span>)</div><div class="line">    printStream.println(<span class="string">s"System properties:\n<span class="subst">$&#123;sysProps.mkString("\n")&#125;</span>"</span>)</div><div class="line">    printStream.println(<span class="string">s"Classpath elements:\n<span class="subst">$&#123;childClasspath.mkString("\n")&#125;</span>"</span>)</div><div class="line">    printStream.println(<span class="string">"\n"</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// scalastyle:on println</span></div><div class="line">  </div><div class="line">  <span class="comment">// 下面这些操作是指定当前运行线程的ClassLoader</span></div><div class="line">  <span class="keyword">val</span> loader =</div><div class="line">    <span class="keyword">if</span> (sysProps.getOrElse(<span class="string">"spark.driver.userClassPathFirst"</span>, <span class="string">"false"</span>).toBoolean) &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</div><div class="line">        <span class="type">Thread</span>.currentThread.getContextClassLoader)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</div><div class="line">        <span class="type">Thread</span>.currentThread.getContextClassLoader)</div><div class="line">    &#125;</div><div class="line">  <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</div><div class="line">  </div><div class="line">  <span class="comment">// 添加jar依赖</span></div><div class="line">  <span class="keyword">for</span> (jar &lt;- childClasspath) &#123;</div><div class="line">    addJarToClasspath(jar, loader)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 系统属性</span></div><div class="line">  <span class="keyword">for</span> ((key, value) &lt;- sysProps) &#123;</div><div class="line">    <span class="type">System</span>.setProperty(key, value)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">var</span> mainClass: <span class="type">Class</span>[_] = <span class="literal">null</span></div><div class="line">  <span class="comment">// 通过反射的方式获得mainClass(child main class)</span></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    mainClass = <span class="type">Utils</span>.classForName(childMainClass)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">ClassNotFoundException</span> =&gt;</div><div class="line">      e.printStackTrace(printStream)</div><div class="line">      <span class="keyword">if</span> (childMainClass.contains(<span class="string">"thriftserver"</span>)) &#123;</div><div class="line">        <span class="comment">// scalastyle:off println</span></div><div class="line">        printStream.println(<span class="string">s"Failed to load main class <span class="subst">$childMainClass</span>."</span>)</div><div class="line">        printStream.println(<span class="string">"You need to build Spark with -Phive and -Phive-thriftserver."</span>)</div><div class="line">        <span class="comment">// scalastyle:on println</span></div><div class="line">      &#125;</div><div class="line">      <span class="type">System</span>.exit(<span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span>)</div><div class="line">    <span class="keyword">case</span> e: <span class="type">NoClassDefFoundError</span> =&gt;</div><div class="line">      e.printStackTrace(printStream)</div><div class="line">      <span class="keyword">if</span> (e.getMessage.contains(<span class="string">"org/apache/hadoop/hive"</span>)) &#123;</div><div class="line">        <span class="comment">// scalastyle:off println</span></div><div class="line">        printStream.println(<span class="string">s"Failed to load hive class."</span>)</div><div class="line">        printStream.println(<span class="string">"You need to build Spark with -Phive and -Phive-thriftserver."</span>)</div><div class="line">        <span class="comment">// scalastyle:on println</span></div><div class="line">      &#125;</div><div class="line">      <span class="type">System</span>.exit(<span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// SPARK-4170</span></div><div class="line">  <span class="keyword">if</span> (classOf[scala.<span class="type">App</span>].isAssignableFrom(mainClass)) &#123;</div><div class="line">    printWarning(<span class="string">"Subclasses of scala.App may not work correctly. Use a main() method instead."</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 获得mainClass(child main class)的main方法</span></div><div class="line">  <span class="keyword">val</span> mainMethod = mainClass.getMethod(<span class="string">"main"</span>, <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">String</span>](<span class="number">0</span>).getClass)</div><div class="line">  <span class="comment">// main方法必须是static级别的</span></div><div class="line">  <span class="keyword">if</span> (!<span class="type">Modifier</span>.isStatic(mainMethod.getModifiers)) &#123;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"The main method in the given main class must be static"</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">findCause</span></span>(t: <span class="type">Throwable</span>): <span class="type">Throwable</span> = t <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">UndeclaredThrowableException</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (e.getCause() != <span class="literal">null</span>) findCause(e.getCause()) <span class="keyword">else</span> e</div><div class="line">    <span class="keyword">case</span> e: <span class="type">InvocationTargetException</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (e.getCause() != <span class="literal">null</span>) findCause(e.getCause()) <span class="keyword">else</span> e</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      e</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 最后调用main方法</span></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    mainMethod.invoke(<span class="literal">null</span>, childArgs.toArray)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">      findCause(t) <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">SparkUserAppException</span>(exitCode) =&gt;</div><div class="line">          <span class="type">System</span>.exit(exitCode)</div><div class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">          <span class="keyword">throw</span> t</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>走到这里，如果是用户通过spark-submit提交自己编写的spark application，那么就直接调用main方法，然后一步一步执行用户编写的代码:SparkContext等等，我们会在以后的文章中进行分析，所以我们现在要跟随的就是org.apache.spark.repl.Main中的main方法，这里我们贴出SparkSubmit进程中主线程的thread dump：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">java.io.FileInputStream.read0(Native Method)</div><div class="line">java.io.FileInputStream.read(FileInputStream.java:207)</div><div class="line">scala.tools.jline.TerminalSupport.readCharacter(TerminalSupport.java:152)</div><div class="line">scala.tools.jline.UnixTerminal.readVirtualKey(UnixTerminal.java:125)</div><div class="line">scala.tools.jline.console.ConsoleReader.readVirtualKey(ConsoleReader.java:933)</div><div class="line">scala.tools.jline.console.ConsoleReader.readBinding(ConsoleReader.java:1136)</div><div class="line">scala.tools.jline.console.ConsoleReader.readLine(ConsoleReader.java:1218)</div><div class="line">scala.tools.jline.console.ConsoleReader.readLine(ConsoleReader.java:1170)</div><div class="line">org.apache.spark.repl.SparkJLineReader.readOneLine(SparkJLineReader.scala:80)</div><div class="line">scala.tools.nsc.interpreter.InteractiveReader<span class="variable">$class</span>.readLine(InteractiveReader.scala:43) </div><div class="line">org.apache.spark.repl.SparkJLineReader.readLine(SparkJLineReader.scala:25) </div><div class="line">org.apache.spark.repl.SparkILoop.readOneLine<span class="variable">$1</span>(SparkILoop.scala:648) </div><div class="line">org.apache.spark.repl.SparkILoop.innerLoop<span class="variable">$1</span>(SparkILoop.scala:665) </div><div class="line">org.apache.spark.repl.SparkILoop.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$loop</span>(SparkILoop.scala:670)</div><div class="line">org.apache.spark.repl.SparkILoop$<span class="variable">$anonfun</span><span class="variable">$org</span><span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$process</span><span class="variable">$1</span>.apply<span class="variable">$mcZ</span><span class="variable">$sp</span>(SparkILoop.scala:997)</div><div class="line">org.apache.spark.repl.SparkILoop$<span class="variable">$anonfun</span><span class="variable">$org</span><span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$process</span><span class="variable">$1</span>.apply(SparkILoop.scala:945)</div><div class="line">org.apache.spark.repl.SparkILoop$<span class="variable">$anonfun</span><span class="variable">$org</span><span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$process</span><span class="variable">$1</span>.apply(SparkILoop.scala:945) </div><div class="line">scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) </div><div class="line">org.apache.spark.repl.SparkILoop.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$repl</span><span class="variable">$SparkILoop</span>$<span class="variable">$process</span>(SparkILoop.scala:945)</div><div class="line">org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059) </div><div class="line">org.apache.spark.repl.Main$.main(Main.scala:31)</div><div class="line">org.apache.spark.repl.Main.main(Main.scala) </div><div class="line">sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) </div><div class="line">sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) </div><div class="line">sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) </div><div class="line">java.lang.reflect.Method.invoke(Method.java:498) </div><div class="line">org.apache.spark.deploy.SparkSubmit$.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$deploy</span><span class="variable">$SparkSubmit</span>$<span class="variable">$runMain</span>(SparkSubmit.scala:731) </div><div class="line">org.apache.spark.deploy.SparkSubmit$.doRunMain<span class="variable">$1</span>(SparkSubmit.scala:181) </div><div class="line">org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) </div><div class="line">org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) </div><div class="line">org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</div></pre></td></tr></table></figure>
<p>在这个时候贴出来就是为了承上启下，我们可以清楚的看见(注意是从最后一行往上看)前面我们分析的过程，从SparkSubmit的main方法到submit、doRunMain、runMain到最后通过反射的方式调用org.apache.spark.repl.Main的main方法，整个流程都看的很清楚，所以下面我们进入org.apache.spark.repl.Main的main方法(包含了初始化的操作)：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 实例化SparkConf</span></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line"><span class="comment">// 设置各种文件路径</span></div><div class="line"><span class="keyword">val</span> tmp = <span class="type">System</span>.getProperty(<span class="string">"java.io.tmpdir"</span>)</div><div class="line"><span class="keyword">val</span> rootDir = conf.get(<span class="string">"spark.repl.classdir"</span>, tmp)</div><div class="line"><span class="keyword">val</span> outputDir = <span class="type">Utils</span>.createTempDir(rootDir)</div><div class="line"><span class="keyword">val</span> s = <span class="keyword">new</span> <span class="type">Settings</span>()</div><div class="line">s.processArguments(<span class="type">List</span>(<span class="string">"-Yrepl-class-based"</span>,</div><div class="line">  <span class="string">"-Yrepl-outdir"</span>, <span class="string">s"<span class="subst">$&#123;outputDir.getAbsolutePath&#125;</span>"</span>,</div><div class="line">  <span class="string">"-classpath"</span>, getAddedJars.mkString(<span class="type">File</span>.pathSeparator)), <span class="literal">true</span>)</div><div class="line"><span class="comment">// the creation of SecurityManager has to be lazy so SPARK_YARN_MODE is set if needed</span></div><div class="line"><span class="keyword">val</span> classServerPort = conf.getInt(<span class="string">"spark.replClassServer.port"</span>, <span class="number">0</span>)</div><div class="line"><span class="comment">// 实例化了HttpServer，注意这里是lazy级别的</span></div><div class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> classServer =</div><div class="line">  <span class="keyword">new</span> <span class="type">HttpServer</span>(conf, outputDir, <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf), classServerPort, <span class="string">"HTTP class server"</span>)</div><div class="line"><span class="keyword">var</span> sparkContext: <span class="type">SparkContext</span> = _</div><div class="line"><span class="keyword">var</span> sqlContext: <span class="type">SQLContext</span> = _</div><div class="line"><span class="comment">// 实例化了SparkILoop，接下来会详细的分析</span></div><div class="line"><span class="keyword">var</span> interp = <span class="keyword">new</span> <span class="type">SparkILoop</span> <span class="comment">// this is a public var because tests reset it.</span></div><div class="line"><span class="comment">// 执行一些初始化的处理后就执行main方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">  <span class="comment">// 判断是否为yarn的模式，我们在以后的文章中会专门的分析yarn的部署模式</span></div><div class="line">  <span class="keyword">if</span> (getMaster == <span class="string">"yarn-client"</span>) <span class="type">System</span>.setProperty(<span class="string">"SPARK_YARN_MODE"</span>, <span class="string">"true"</span>)</div><div class="line">  <span class="comment">// Start the classServer and store its URI in a spark system property</span></div><div class="line">  <span class="comment">// (which will be passed to executors so that they can connect to it)</span></div><div class="line">  <span class="comment">// 启动HTTP server</span></div><div class="line">  classServer.start()</div><div class="line">  <span class="comment">// 最关键的代码，让解释器循环执行，即REPL</span></div><div class="line">  interp.process(s) <span class="comment">// Repl starts and goes in loop of R.E.P.L</span></div><div class="line">  classServer.stop()</div><div class="line">  <span class="type">Option</span>(sparkContext).map(_.stop)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>写到这里我们再来贴出通过spark-shell进入REPL时打印的部分日志：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">17/02/21 13:40:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</div><div class="line">17/02/21 13:40:17 INFO spark.SecurityManager: Changing view acls to: root</div><div class="line">17/02/21 13:40:17 INFO spark.SecurityManager: Changing modify acls to: root</div><div class="line">17/02/21 13:40:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)</div><div class="line">17/02/21 13:40:18 INFO spark.HttpServer: Starting HTTP Server</div><div class="line">17/02/21 13:40:18 INFO server.Server: jetty-8.y.z-SNAPSHOT</div><div class="line">17/02/21 13:40:18 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:43773</div><div class="line">17/02/21 13:40:18 INFO util.Utils: Successfully started service <span class="string">'HTTP class server'</span> on port 43773.</div></pre></td></tr></table></figure>
<p>上面这段日志其实对应的就是classServer.start()的部分，以后我们再看到这些日志的时候就知道背后到底发生了什么，是不是很有成就感？</p>
<p>下面就进入SparkILoop和ILoop的部分(SparkILoop是继承自ILoop类，而SparkILoop中没有process方法，所以调用的实际上是ILoop类中的process方法)：</p>
<p>ILoop</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 启动解释器，用来解释用户输入的command</span></div><div class="line"><span class="comment">// start an interpreter with the given settings</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(settings: <span class="type">Settings</span>): <span class="type">Boolean</span> = savingContextLoader &#123;</div><div class="line">  <span class="keyword">this</span>.settings = settings</div><div class="line">  <span class="comment">// 创建解释器，内部其实是实例化了一个ILoopInterpreter</span></div><div class="line">  createInterpreter()</div><div class="line">  <span class="comment">// sets in to some kind of reader depending on environmental cues</span></div><div class="line">  in = in0.fold(chooseReader(settings))(r =&gt; <span class="type">SimpleReader</span>(r, out, interactive = <span class="literal">true</span>))</div><div class="line">  globalFuture = future &#123;</div><div class="line">    intp.initializeSynchronous()</div><div class="line">    loopPostInit()</div><div class="line">    !intp.reporter.hasErrors</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 这里应该调用的是其子类SparkILoop的loadFiles方法，而SparkILoop的loadFiles方法内部最后又会调用这里的loadFiles方法</span></div><div class="line">  loadFiles(settings)</div><div class="line">  printWelcome()</div><div class="line">  <span class="comment">// 一直循环接收用户输入的command</span></div><div class="line">  <span class="keyword">try</span> loop() <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">LineResults</span>.<span class="type">EOF</span> =&gt; out print <span class="type">Properties</span>.shellInterruptedString</div><div class="line">    <span class="keyword">case</span> _               =&gt;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">catch</span> <span class="type">AbstractOrMissingHandler</span>()</div><div class="line">  <span class="keyword">finally</span> closeInterpreter()</div><div class="line">  <span class="literal">true</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们先来看一下SparkILoop的loadFiles方法都做了什么：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">loadFiles</span></span>(settings: <span class="type">Settings</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  initializeSpark()</div><div class="line">  <span class="keyword">super</span>.loadFiles(settings)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到首先调用initializeSpark()方法，然后调用父类的loadFiles方法，目的就是先准备好SparkContext、SQLContext然后再执行后面的操作，方便我们在进入到REPL后直接可以访问sc、sqlContext等，所以我们现在明白了为什么我们可以直接在spark-shell中直接访问sc、sqlContext了(成就感爆棚有木有？)。说了这么多，我们看一下initializeSpark()的庐山真面目：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">def initializeSpark() &#123;</div><div class="line">  intp.beQuietDuring &#123;</div><div class="line">    processLine("""</div><div class="line">       @transient val sc = &#123;</div><div class="line">         val _sc = org.apache.spark.repl.Main.createSparkContext()</div><div class="line">         println("Spark context available as sc.")</div><div class="line">         _sc</div><div class="line">       &#125;</div><div class="line">      """)</div><div class="line">    processLine("""</div><div class="line">       @transient val sqlContext = &#123;</div><div class="line">         val _sqlContext = org.apache.spark.repl.Main.createSQLContext()</div><div class="line">         println("SQL context available as sqlContext.")</div><div class="line">         _sqlContext</div><div class="line">       &#125;</div><div class="line">      """)</div><div class="line">    processLine("import org.apache.spark.SparkContext._")</div><div class="line">    processLine("import sqlContext.implicits._")</div><div class="line">    processLine("import sqlContext.sql")</div><div class="line">    processLine("import org.apache.spark.sql.functions._")</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里写的就非常清楚了通过processLine来创建SparkContext、SQLContext并导入一些经常使用的包，都准备完成后再调用父类的loadFiles，然后调用printWelcome()，注意这里调用的是SparkILoop的printWelcome()方法：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">/** Print a welcome message */</div><div class="line">override def printWelcome() &#123;</div><div class="line">  import org.apache.spark.SPARK_VERSION</div><div class="line">  echo("""Welcome to</div><div class="line">    ____              __</div><div class="line">   / __/__  ___ _____/ /__</div><div class="line">  _\ \/ _ \/ _ `/ __/  '_/</div><div class="line"> /___/ .__/\_,_/_/ /_/\_\   version %s</div><div class="line">    /_/</div><div class="line">       """.format(SPARK_VERSION))</div><div class="line">  val welcomeMsg = "Using Scala %s (%s, Java %s)".format(</div><div class="line">    versionString, javaVmName, javaVersion)</div><div class="line">  echo(welcomeMsg)</div><div class="line">  echo("Type in expressions to have them evaluated.")</div><div class="line">  echo("Type :help for more information.")</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>咦？这货看着是不是很眼熟，对，这就是spark-shell中打印的日志：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div></pre></td><td class="code"><pre><div class="line">Welcome to</div><div class="line">      ____              __</div><div class="line">     / __/__  ___ _____/ /__</div><div class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">'_/</span></div><div class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.6.3</div><div class="line">      /_/</div><div class="line"></div><div class="line">Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111)</div><div class="line">Type in expressions to have them evaluated.</div><div class="line">Type :help for more information.</div><div class="line">17/02/21 13:40:24 INFO spark.SparkContext: Running Spark version 1.6.3</div><div class="line">17/02/21 13:40:24 INFO spark.SecurityManager: Changing view acls to: root</div><div class="line">17/02/21 13:40:24 INFO spark.SecurityManager: Changing modify acls to: root</div><div class="line">17/02/21 13:40:24 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)</div><div class="line">17/02/21 13:40:25 INFO util.Utils: Successfully started service 'sparkDriver<span class="string">' on port 38463.</span></div><div class="line">17/02/21 13:40:26 INFO slf4j.Slf4jLogger: Slf4jLogger started</div><div class="line">17/02/21 13:40:26 INFO Remoting: Starting remoting</div><div class="line">17/02/21 13:40:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:37221]</div><div class="line">17/02/21 13:40:26 INFO util.Utils: Successfully started service 'sparkDriverActorSystem<span class="string">' on port 37221.</span></div><div class="line">17/02/21 13:40:26 INFO spark.SparkEnv: Registering MapOutputTracker</div><div class="line">17/02/21 13:40:26 INFO spark.SparkEnv: Registering BlockManagerMaster</div><div class="line">17/02/21 13:40:26 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-a06685a8-6f1c-4e8f-805c-e232333f8d85</div><div class="line">17/02/21 13:40:26 INFO storage.MemoryStore: MemoryStore started with capacity 511.1 MB</div><div class="line">17/02/21 13:40:27 INFO spark.SparkEnv: Registering OutputCommitCoordinator</div><div class="line">17/02/21 13:40:27 INFO server.Server: jetty-8.y.z-SNAPSHOT</div><div class="line">17/02/21 13:40:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040</div><div class="line">17/02/21 13:40:27 INFO util.Utils: Successfully started service 'SparkUI<span class="string">' on port 4040.</span></div><div class="line">17/02/21 13:40:27 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:4040</div><div class="line">17/02/21 13:40:27 INFO client.AppClient$ClientEndpoint: Connecting to master spark://master:7077...</div><div class="line">17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20170221134027-0000</div><div class="line">17/02/21 13:40:28 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService<span class="string">' on port 44615.</span></div><div class="line">17/02/21 13:40:28 INFO netty.NettyBlockTransferService: Server created on 44615</div><div class="line">17/02/21 13:40:28 INFO storage.BlockManagerMaster: Trying to register BlockManager</div><div class="line">17/02/21 13:40:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:44615 with 511.1 MB RAM, BlockManagerId(driver, 172.17.0.2, 44615)</div><div class="line">17/02/21 13:40:28 INFO storage.BlockManagerMaster: Registered BlockManager</div><div class="line">17/02/21 13:40:28 INFO client.AppClient$ClientEndpoint: Executor added: app-20170221134027-0000/0 on worker-20170221133811-172.17.0.3-41829 (172.17.0.3:41829) with 2 cores</div><div class="line">17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20170221134027-0000/0 on hostPort 172.17.0.3:41829 with 2 cores, 1024.0 MB RAM</div><div class="line">17/02/21 13:40:28 INFO client.AppClient$ClientEndpoint: Executor added: app-20170221134027-0000/1 on worker-20170221133810-172.17.0.4-39901 (172.17.0.4:39901) with 2 cores</div><div class="line">17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20170221134027-0000/1 on hostPort 172.17.0.4:39901 with 2 cores, 1024.0 MB RAM</div><div class="line">17/02/21 13:40:29 INFO client.AppClient$ClientEndpoint: Executor updated: app-20170221134027-0000/1 is now RUNNING</div><div class="line">17/02/21 13:40:29 INFO client.AppClient$ClientEndpoint: Executor updated: app-20170221134027-0000/0 is now RUNNING</div><div class="line">17/02/21 13:40:45 INFO scheduler.EventLoggingListener: Logging events to hdfs://master:9000/historyserverforspark/app-20170221134027-0000</div><div class="line">17/02/21 13:40:45 INFO cluster.SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0</div><div class="line">17/02/21 13:40:45 INFO repl.SparkILoop: Created spark context..</div><div class="line">Spark context available as sc.</div><div class="line">17/02/21 13:40:46 INFO cluster.SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (worker1:60096) with ID 0</div><div class="line">17/02/21 13:40:46 INFO cluster.SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (worker2:46846) with ID 1</div><div class="line">17/02/21 13:40:47 INFO storage.BlockManagerMasterEndpoint: Registering block manager worker1:39275 with 511.1 MB RAM, BlockManagerId(0, worker1, 39275)</div><div class="line">17/02/21 13:40:47 INFO storage.BlockManagerMasterEndpoint: Registering block manager worker2:37449 with 511.1 MB RAM, BlockManagerId(1, worker2, 37449)</div><div class="line">17/02/21 13:40:50 INFO hive.HiveContext: Initializing execution hive, version 1.2.1</div><div class="line">17/02/21 13:40:51 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0</div><div class="line">17/02/21 13:40:51 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0</div><div class="line">17/02/21 13:40:52 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</div><div class="line">17/02/21 13:40:53 INFO metastore.ObjectStore: ObjectStore, initialize called</div><div class="line">17/02/21 13:40:53 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</div><div class="line">17/02/21 13:40:53 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored</div><div class="line">17/02/21 13:40:54 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)</div><div class="line">17/02/21 13:40:55 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)</div><div class="line">17/02/21 13:41:01 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"</div><div class="line">17/02/21 13:41:08 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:08 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:15 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:15 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:17 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</div><div class="line">17/02/21 13:41:17 INFO metastore.ObjectStore: Initialized ObjectStore</div><div class="line">17/02/21 13:41:18 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0</div><div class="line">17/02/21 13:41:18 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException</div><div class="line">17/02/21 13:41:19 INFO metastore.HiveMetaStore: Added admin role in metastore</div><div class="line">17/02/21 13:41:19 INFO metastore.HiveMetaStore: Added public role in metastore</div><div class="line">17/02/21 13:41:19 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty</div><div class="line">17/02/21 13:41:20 INFO metastore.HiveMetaStore: 0: get_all_databases</div><div class="line">17/02/21 13:41:20 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	</div><div class="line">17/02/21 13:41:20 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*</div><div class="line">17/02/21 13:41:20 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	</div><div class="line">17/02/21 13:41:20 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:21 INFO session.SessionState: Created local directory: /tmp/939dedb5-f724-461b-a41a-a5fd1fe7324b_resources</div><div class="line">17/02/21 13:41:21 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/939dedb5-f724-461b-a41a-a5fd1fe7324b</div><div class="line">17/02/21 13:41:21 INFO session.SessionState: Created local directory: /tmp/root/939dedb5-f724-461b-a41a-a5fd1fe7324b</div><div class="line">17/02/21 13:41:21 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/939dedb5-f724-461b-a41a-a5fd1fe7324b/_tmp_space.db</div><div class="line">17/02/21 13:41:22 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse</div><div class="line">17/02/21 13:41:22 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.</div><div class="line">17/02/21 13:41:22 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0</div><div class="line">17/02/21 13:41:22 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0</div><div class="line">17/02/21 13:41:23 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</div><div class="line">17/02/21 13:41:23 INFO metastore.ObjectStore: ObjectStore, initialize called</div><div class="line">17/02/21 13:41:23 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</div><div class="line">17/02/21 13:41:23 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored</div><div class="line">17/02/21 13:41:24 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)</div><div class="line">17/02/21 13:41:24 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)</div><div class="line">17/02/21 13:41:25 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"</div><div class="line">17/02/21 13:41:29 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:29 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:29 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:29 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:30 INFO DataNucleus.Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing</div><div class="line">17/02/21 13:41:30 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</div><div class="line">17/02/21 13:41:30 INFO metastore.ObjectStore: Initialized ObjectStore</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: Added admin role in metastore</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: Added public role in metastore</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: 0: get_all_databases</div><div class="line">17/02/21 13:41:30 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	</div><div class="line">17/02/21 13:41:30 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*</div><div class="line">17/02/21 13:41:30 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	</div><div class="line">17/02/21 13:41:30 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.</div><div class="line">17/02/21 13:41:30 INFO session.SessionState: Created local directory: /tmp/c9c26571-1229-4786-8a8e-d7b090b07d85_resources</div><div class="line">17/02/21 13:41:30 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/c9c26571-1229-4786-8a8e-d7b090b07d85</div><div class="line">17/02/21 13:41:30 INFO session.SessionState: Created local directory: /tmp/root/c9c26571-1229-4786-8a8e-d7b090b07d85</div><div class="line">17/02/21 13:41:30 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/c9c26571-1229-4786-8a8e-d7b090b07d85/_tmp_space.db</div><div class="line">17/02/21 13:41:30 INFO repl.SparkILoop: Created sql context (with Hive support)..</div><div class="line">SQL context available as sqlContext.</div></pre></td></tr></table></figure>
<p>Welcome后面的一大串就是上面initializeSpark()执行打的日志信息，现在所有的日志信息都“名花有主”了，我们会单独拿出文章来分析SparkContext、SQLContext的创建流程，下面我们看process方法中最后就一直进行loop操作，这里我们不再深入的分析下去了，我们要适可而止，否则会迷失在源码中，大家可以简单的理解其实这里的循环过程就是REPL所代表的意思，即Read：读取用户输入的command；Evaluation：通过Spark Framework执行command；P：print打计算结果；L：loop循环前面的流程，同时在读取command后需要进行语法解析，然后用解释器执行，有兴趣的朋友可以继续跟随源码走下去。</p>
<p>至此我们走完了整个spark-shell(包括spark-submit)的整个流程，下面用一张图简单的总结一下：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/006y2nc1ly1fd8fholzmbj30xe16577s.jpg" alt=""></p>
<p>本文参考和拓展阅读：</p>
<p><a href="https://github.com/apache/spark/tree/branch-1.6" target="_blank" rel="external">Spark-1.6.3源码</a></p>
<p><a href="https://github.com/apache/spark/tree/branch-2.1" target="_blank" rel="external">Spark-2.1.0源码</a></p>
<font color="#808080"><em>站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！</em></font>
        
        </div>
        <footer class="article-footer">
            <div class="share-container">


    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>


</div>

            
    
        <a href="http://www.sun4lower.cn/2017/03/02/sc-sparkshell/#comments" class="article-comment-link">Comments</a>
    

        </footer>
    </div>
    
        
<nav id="article-nav">
    
    
        <a href="/2017/02/28/sc-schedule/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">Spark-Core源码精读(2)、Master中的schedule详解</div>
        </a>
    
</nav>


    
</article>


    
    <section id="comments">
    
        
    <div id="uyan_frame"></div>

    
    </section>

</section>
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">recent</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a></p>
                            <p class="item-title"><a href="/2017/03/02/sc-sparkshell/" class="title">Spark-Core源码精读(2)、spark-shell(spark-submit)流程详解</a></p>
                            <p class="item-date"><time datetime="2017-03-02T05:28:17.000Z" itemprop="datePublished">2017-03-02</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a></p>
                            <p class="item-title"><a href="/2017/02/28/sc-schedule/" class="title">Spark-Core源码精读(2)、Master中的schedule详解</a></p>
                            <p class="item-date"><time datetime="2017-02-28T08:17:02.000Z" itemprop="datePublished">2017-02-28</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a></p>
                            <p class="item-title"><a href="/2017/02/25/sc-deploy/" class="title">Spark-Core源码精读(1)、Spark Deployment &amp; start-all.sh on Standalone mode</a></p>
                            <p class="item-date"><time datetime="2017-02-25T05:58:04.000Z" itemprop="datePublished">2017-02-25</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/spark/">spark</a></p>
                            <p class="item-title"><a href="/2017/02/22/SparkRpcBasic/" class="title">Spark RPC 到底是个什么鬼？</a></p>
                            <p class="item-date"><time datetime="2017-02-22T09:49:24.000Z" itemprop="datePublished">2017-02-22</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/bigdata/">大数据</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/bigdata/event/">通信机制</a></p>
                            <p class="item-title"><a href="/2017/02/22/event-threadmodel/" class="title">浅显易懂之线程模型的演变</a></p>
                            <p class="item-date"><time datetime="2017-02-22T09:49:24.000Z" itemprop="datePublished">2017-02-22</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/">大数据</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/spark/">spark</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/spark/sparkc/">spark-core</a><span class="category-list-count">5</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata/event/">通信机制</a><span class="category-list-count">1</span></li></ul></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">5</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">Netty</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/REPL/">REPL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/prc/">RPC</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SparkContext/">SparkContext</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SqlContext/">SqlContext</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deployment/">deployment</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/driver/">driver</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/executor/">executor</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/master/">master</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mesos/">mesos</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/schedule/">schedule</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-class/">spark-class</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sparkc/">spark-core</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-shell/">spark-shell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-submit/">spark-submit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/standalone/">standalone</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/worker/">worker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/yarn/">yarn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/多线程/">多线程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bigdata/">大数据</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/perf/">性能调优</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/event/">通信机制</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>

    
        
<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
    <div class="widget-wrap">
        <h3 class="widget-title">tag cloud</h3>
        <div id="myCanvasContainer" class="widget tagcloud">
            <canvas width="250" height="250" id="resCanvas" style="width=100%">
                <a href="/tags/netty/" style="font-size: 13.33px;">Netty</a> <a href="/tags/REPL/" style="font-size: 10px;">REPL</a> <a href="/tags/prc/" style="font-size: 13.33px;">RPC</a> <a href="/tags/SparkContext/" style="font-size: 10px;">SparkContext</a> <a href="/tags/SqlContext/" style="font-size: 10px;">SqlContext</a> <a href="/tags/deployment/" style="font-size: 10px;">deployment</a> <a href="/tags/driver/" style="font-size: 10px;">driver</a> <a href="/tags/executor/" style="font-size: 10px;">executor</a> <a href="/tags/master/" style="font-size: 13.33px;">master</a> <a href="/tags/mesos/" style="font-size: 10px;">mesos</a> <a href="/tags/schedule/" style="font-size: 10px;">schedule</a> <a href="/tags/spark/" style="font-size: 16.67px;">spark</a> <a href="/tags/spark-class/" style="font-size: 10px;">spark-class</a> <a href="/tags/sparkc/" style="font-size: 20px;">spark-core</a> <a href="/tags/spark-shell/" style="font-size: 10px;">spark-shell</a> <a href="/tags/spark-submit/" style="font-size: 10px;">spark-submit</a> <a href="/tags/standalone/" style="font-size: 10px;">standalone</a> <a href="/tags/worker/" style="font-size: 10px;">worker</a> <a href="/tags/yarn/" style="font-size: 10px;">yarn</a> <a href="/tags/多线程/" style="font-size: 10px;">多线程</a> <a href="/tags/bigdata/" style="font-size: 20px;">大数据</a> <a href="/tags/perf/" style="font-size: 10px;">性能调优</a> <a href="/tags/event/" style="font-size: 13.33px;">通信机制</a>
            </canvas>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://spark.apache.org">Spark</a>
                    </li>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
	    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次, 访客数 <span id="busuanzi_value_site_uv"></span> 人次, 本文总阅读量 <span id="busuanzi_value_page_pv"></span> 次<br>
            &copy; 2017 Sunflower<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
        </div>
    </div>
</footer>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
        
    
    <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2125727"></script>



    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>