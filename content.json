{"pages":[],"posts":[{"title":"Spark-Core源码精读(2)、Master中的schedule详解","slug":"sc-schedule","date":"2017-02-28T08:17:02.000Z","updated":"2017-02-28T12:31:40.000Z","comments":true,"path":"2017/02/28/sc-schedule/","link":"","permalink":"http://www.sun4lower.cn/2017/02/28/sc-schedule/","excerpt":"","text":"上一篇博客详细分析了Spark在Standalone模式下的部署过程，文中提到在Worker注册完成后需要执行一个schedule操作来分配资源，本文就将具体分析此方法具体是怎样分配资源的。 注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。 什么时候会调用schedule？其实每当一个新的application加入活着资源发生变化的时候都会调用schudule方法对资源进行重新分配，那么它是如何分配资源的呢？我们下面进行源码级别的分析。 schedule我们先贴出schedule的源码： 123456789101112131415161718192021222324252627282930313233343536// 既然要分配资源就必须保证Master的当前状态为ALIVEif (state != RecoveryState.ALIVE) &#123; return&#125;// Drivers take strict precedence over executors// 注释说的很明确，先注册Drivers然后再注册executors// 1. 首先将ALIVE状态的Workers使用shuffle的方式打乱，以免每次都将Driver分配到同一个Worker上val shuffledAliveWorkers = Random.shuffle(workers.toSeq.filter(_.state == WorkerState.ALIVE))val numWorkersAlive = shuffledAliveWorkers.sizevar curPos = 0// 2. 循环遍历启动Driversfor (driver &lt;- waitingDrivers.toList) &#123; // iterate over a copy of waitingDrivers // We assign workers to each waiting driver in a round-robin fashion. For each driver, we // start from the last worker that was assigned a driver, and continue onwards until we have // explored all alive workers. var launched = false var numWorkersVisited = 0 // 2.1 判断是否有剩余的没有分配的Workers，并且尚未启动 while (numWorkersVisited &lt; numWorkersAlive &amp;&amp; !launched) &#123; // 2.2 获取一个Worker，第一个的索引为0，后面的索引根据curPos = (curPos + 1) % numWorkersAlive进行计算 val worker = shuffledAliveWorkers(curPos) // 2.3 标记分配过的Worker加1 numWorkersVisited += 1 // 2.4 判断当前的Worker的内存和cpu是否满足Driver的需求 if (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123; // 2.5 如果满足资源的需求就在当前Worker上启动Driver launchDriver(worker, driver) // 2.6 启动完成后从等待的队列中删除，并将launched标记为true waitingDrivers -= driver launched = true &#125; curPos = (curPos + 1) % numWorkersAlive &#125;&#125;// 3 启动ExecutorsstartExecutorsOnWorkers() 启动Driver我已经在上面的源码中对分配的流程进行了详细的注释，现在我们看一下launchDriver方法： 123456789101112private def launchDriver(worker: WorkerInfo, driver: DriverInfo) &#123; // 1. 打日志 logInfo(\"Launching driver \" + driver.id + \" on worker \" + worker.id) // 2. 向worker中添加driver的信息，包括增加已经使用的内存和cpu信息 worker.addDriver(driver) // 3. 向driver中添加该worker的引用 driver.worker = Some(worker) // 4. 向Worker发送LaunchDriver的消息，通知Worker启动Driver worker.endpoint.send(LaunchDriver(driver.id, driver.desc)) // 5. 将driver的状态变成RUNNING driver.state = DriverState.RUNNING&#125; 接下来我们看一下对应的Worker在接收到LaunchDriver消息后是怎么处理的： 12345678910111213141516171819202122 case LaunchDriver(driverId, driverDesc) =&gt; &#123; // 1. 打日志 logInfo(s\"Asked to launch driver $driverId\") // 2. 实例化DriverRunner val driver = new DriverRunner( conf, driverId, workDir, sparkHome, driverDesc.copy(command = Worker.maybeUpdateSSLSettings(driverDesc.command, conf)), self, workerUri, securityMgr) // 3. 实例化完成后向drivers中添加该driver的记录 drivers(driverId) = driver // 4. 启动driver driver.start() // 5. 启动完成后记录资源的变化 coresUsed += driverDesc.cores memoryUsed += driverDesc.mem&#125; 继续跟踪driver.start()： 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 英文注释说的很清楚：启动一个线程来运行和管理driver/** Starts a thread to run and manage the driver. */private[worker] def start() = &#123; new Thread(\"DriverRunner for \" + driverId) &#123; override def run() &#123; try &#123; // 创建driver的工作目录 val driverDir = createWorkingDirectory() // 下载用户的Jar文件到driver的工作目录并返回路径名称 val localJarFilename = downloadUserJar(driverDir) def substituteVariables(argument: String): String = argument match &#123; case \"&#123;&#123;WORKER_URL&#125;&#125;\" =&gt; workerUrl case \"&#123;&#123;USER_JAR&#125;&#125;\" =&gt; localJarFilename case other =&gt; other &#125; // TODO: If we add ability to submit multiple jars they should also be added here val builder = CommandUtils.buildProcessBuilder(driverDesc.command, securityManager, driverDesc.mem, sparkHome.getAbsolutePath, substituteVariables) // 具体的启动Driver的操作，这里不再详细分析 launchDriver(builder, driverDir, driverDesc.supervise) &#125; catch &#123; case e: Exception =&gt; finalException = Some(e) &#125; val state = if (killed) &#123; DriverState.KILLED &#125; else if (finalException.isDefined) &#123; DriverState.ERROR &#125; else &#123; finalExitCode match &#123; case Some(0) =&gt; DriverState.FINISHED case _ =&gt; DriverState.FAILED &#125; &#125; finalState = Some(state) worker.send(DriverStateChanged(driverId, state, finalException)) &#125; &#125;.start()&#125; 如果启动成功最后要向worker发送一条DriverStateChanged的消息，而Worker在接收到该消息后会调用handleDriverStateChanged方法进行一系列处理，具体的处理细节就不再说明，主要的就是向Master发送一条driverStateChanged的消息，Master在接收到该消息后移除Driver的信息： 12345678ase DriverStateChanged(driverId, state, exception) =&gt; &#123; state match &#123; case DriverState.ERROR | DriverState.FINISHED | DriverState.KILLED | DriverState.FAILED =&gt; removeDriver(driverId, state, exception) case _ =&gt; throw new Exception(s\"Received unexpected state update for driver $driverId: $state\") &#125;&#125; 至此向Driver分配资源并启动Driver的过程结束，下面我们看一下启动Executors即执行startExecutorsOnWorkers()的流程。 启动ExecutorsstartExecutorsOnWorkers(): 123456789101112131415161718192021222324252627/** * Schedule and launch executors on workers */ private def startExecutorsOnWorkers(): Unit = &#123; // 采用的是先进先出的原则 // Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app // in the queue, then the second app, etc. for (app &lt;- waitingApps if app.coresLeft &gt; 0) &#123; val coresPerExecutor: Option[Int] = app.desc.coresPerExecutor // Filter out workers that don't have enough resources to launch an executor // 过滤出ALIVE状态并且资源满足要求的workers，同时按照空闲cpu cores的个数倒序排列 val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE) .filter(worker =&gt; worker.memoryFree &gt;= app.desc.memoryPerExecutorMB &amp;&amp; worker.coresFree &gt;= coresPerExecutor.getOrElse(1)) .sortBy(_.coresFree).reverse // 决定在每个worker上面分配多少个cpu cores val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps) // 然后开始进行分配 // Now that we've decided how many cores to allocate on each worker, let's allocate them for (pos &lt;- 0 until usableWorkers.length if assignedCores(pos) &gt; 0) &#123; allocateWorkerResourceToExecutors( app, assignedCores(pos), coresPerExecutor, usableWorkers(pos)) &#125; &#125; &#125; 我们首先看一下是如何决定在每个worker上分配多少个cores的，这里我们只列出scheduleExecutorsOnWorkers方法的英文注释，并进行说明，具体的操作大家可以去看源码： 1234567891011121314151617181920/** * Schedule executors to be launched on the workers. * Returns an array containing number of cores assigned to each worker. * * There are two modes of launching executors. The first attempts to spread out an application's * executors on as many workers as possible, while the second does the opposite (i.e. launch them * on as few workers as possible). The former is usually better for data locality purposes and is * the default. * * The number of cores assigned to each executor is configurable. When this is explicitly set, * multiple executors from the same application may be launched on the same worker if the worker * has enough cores and memory. Otherwise, each executor grabs all the cores available on the * worker by default, in which case only one executor may be launched on each worker. * * It is important to allocate coresPerExecutor on each worker at a time (instead of 1 core * at a time). Consider the following example: cluster has 4 workers with 16 cores each. * User requests 3 executors (spark.cores.max = 48, spark.executor.cores = 16). If 1 core is * allocated at a time, 12 cores from each worker would be assigned to each executor. * Since 12 &lt; 16, no executors would launch [SPARK-8881]. */ 大致意思是说有两种分配模型，第一种是将executors分配到尽可能多的workers上；第二种与第一种相反。默认使用的是第一种模型，这种模型更加符合数据的本地性原则，为每个Executor分配的cores的个数是可以进行配置的（spark-submit 或者 spark-env.sh），如果设置了，多个executors可能会被分配在一个worker上（前提是该worker拥有足够的cores和memory），否则每个executor会充分利用worker上的cores，这种情况下一个executor会被分配在一个worker上。具体在集群上分配cores的时候会尽可能的满足我们的要求，如果需要的cores的个数大于workers中空闲的cores的个数，那么就先分配空闲的cores，尽可能的去满足要求。 接下来就是具体为executors分配计算资源并启动executors的过程： 123456789101112131415161718private def allocateWorkerResourceToExecutors( app: ApplicationInfo, assignedCores: Int, coresPerExecutor: Option[Int], worker: WorkerInfo): Unit = &#123; // If the number of cores per executor is specified, we divide the cores assigned // to this worker evenly among the executors with no remainder. // Otherwise, we launch a single executor that grabs all the assignedCores on this worker. val numExecutors = coresPerExecutor.map &#123; assignedCores / _ &#125;.getOrElse(1) val coresToAssign = coresPerExecutor.getOrElse(assignedCores) for (i &lt;- 1 to numExecutors) &#123; // 向application中添加executor的信息 val exec = app.addExecutor(worker, coresToAssign) // 启动executors launchExecutor(worker, exec) app.state = ApplicationState.RUNNING &#125; &#125; 启动executors： 12345678910private def launchExecutor(worker: WorkerInfo, exec: ExecutorDesc): Unit = &#123; logInfo(\"Launching executor \" + exec.fullId + \" on worker \" + worker.id) worker.addExecutor(exec) // 向worker发消息启动executor worker.endpoint.send(LaunchExecutor(masterUrl, exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory)) // 然后向driver发送executors的信息 exec.application.driver.send( ExecutorAdded(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory)) &#125; worker在接收到启动executor的消息后执行具体的启动操作，并向Master汇报。 然后也要向driver发送executors的资源信息，driver收到信息后执行application，至此分配并启动executors的大致流程也就执行完毕。 本文只是大致的分析了Master在执行schedule的时候具体为Driver、Executors分配资源并启动它们的流程，以后我们还会分析整个application的运行流程，那时我们会具体进行分析。 本文参考和拓展阅读： Spark-1.6.3源码 Spark-2.1.0源码 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"spark","slug":"spark","permalink":"http://www.sun4lower.cn/tags/spark/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"},{"name":"master","slug":"master","permalink":"http://www.sun4lower.cn/tags/master/"},{"name":"driver","slug":"driver","permalink":"http://www.sun4lower.cn/tags/driver/"},{"name":"executor","slug":"executor","permalink":"http://www.sun4lower.cn/tags/executor/"},{"name":"schedule","slug":"schedule","permalink":"http://www.sun4lower.cn/tags/schedule/"}]},{"title":"Spark-Core源码精读(1)、Spark Deployment & start-all.sh on Standalone mode","slug":"sc-deploy","date":"2017-02-25T05:58:04.000Z","updated":"2017-02-25T14:44:34.000Z","comments":true,"path":"2017/02/25/sc-deploy/","link":"","permalink":"http://www.sun4lower.cn/2017/02/25/sc-deploy/","excerpt":"","text":"本文为精度Spark-core的源码的第一节，主要内容包括Spark Deployment的简介和Standalone模式下启动集群的详细流程精读。 注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。 Spark DeploymentSpark 的部署主要有三种方式：local、standalone、yarn、mesos 其中local和standalone模式主要用于测试学习，实际生产环境下国内一般都是使用yarn，这是历史原因造成的（考虑到集群中同时有Hadoop）；而国外一般都是使用mesos，而且个人认为mesos也是一种趋势，关于yarn和mesos的部分，以后会单独进行分析，下面我们详细解读standalone模式下的集群启动的具体流程。 Standalone mode下集群启动源码精读我们就从start-all.sh开始，主要代码如下： 12345678# Load the Spark configuration. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"# Start Master\"$&#123;SPARK_HOME&#125;/sbin\"/start-master.sh $TACHYON_STR# Start Workers\"$&#123;SPARK_HOME&#125;/sbin\"/start-slaves.sh $TACHYON_STR 注释说的很明确了，我们继续追踪start-master.sh 123456CLASS=\"org.apache.spark.deploy.master.Master\"...\"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start $CLASS 1 \\ --ip $SPARK_MASTER_IP --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \\ $ORIGINAL_ARGS... 可以看出，是执行了spark-daemon.sh的start方法，即通过动态加载的方式将org.apache.spark.deploy.master.Master作为一个daemon（守护线程）来运行，所以我们直接分析Master的源码： 12345678910111213141516171819202122232425262728293031323334private[deploy] object Master extends Logging &#123; val SYSTEM_NAME = \"sparkMaster\" val ENDPOINT_NAME = \"Master\" def main(argStrings: Array[String]) &#123; //注册log SignalLogger.register(log) //实例化SparkConf，会加载`spark.*`格式的配置信息 val conf = new SparkConf //使用MasterArguments对传入的参数argStrings和默认加载的conf进行封装，并执行一些初始化操作 val args = new MasterArguments(argStrings, conf) val (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf) rpcEnv.awaitTermination() &#125; /** * Start the Master and return a three tuple of: * (1) The Master RpcEnv * (2) The web UI bound port * (3) The REST server bound port, if any */ def startRpcEnvAndEndpoint( host: String, port: Int, webUiPort: Int, conf: SparkConf): (RpcEnv, Int, Option[Int]) = &#123; val securityMgr = new SecurityManager(conf) val rpcEnv = RpcEnv.create(SYSTEM_NAME, host, port, conf, securityMgr) val masterEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME, new Master(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf)) val portsResponse = masterEndpoint.askWithRetry[BoundPortsResponse](BoundPortsRequest) (rpcEnv, portsResponse.webUIPort, portsResponse.restPort) &#125;&#125; 首先注册log，实例化SparkConf并加载spark.*格式的配置信息，然后使用MasterArguments对传入的参数argStrings和默认加载的conf进行封装，并执行一些初始化操作，主要是加载配置信息，这里不做详细说明，我们接着往下看。 下面才是真正意义上的Start Master，startRpcEnvAndEndpoint函数中首先实例化了SecurityManager（Spark中负责安全的类），然后创建了RpcEnv（Spark的Rpc通信有三个抽象：RpcEnv、RpcEndpoint、RpcEndpointRef，这样做屏蔽了底层的实现，方便用户进行扩展，Spark-1.6.3底层的默认实现方式是Netty，而Spark-2.x已经将Akka的依赖移除），接着实例化Master，实际上就是实例化了一个RpcEndpoint（因为Master实现了ThreadSafeRpcEndpoint接口，而ThreadSafeRpcEndpoint又继承了RpcEndpoint），实例化完成后通过RpcEnv的setupEndpoint向RpcEnv进行注册，注册的时候执行了Master的onStart方法，最后返回了一个RpcEndpointRef（实际上是NettyRpcEndpointRef），通过获得的RpcEndpointRef向Master（Endpoint）发送了一条BoundPortsRequest消息，Master通过receiveAndReply方法接受到该消息（实际上是通过NettyRpcEnv中的Dispatcher进行消息的分配），模式匹配到是BoundPortsRequest类型的消息，然后执行reply方法进行回复，源码如下：123case BoundPortsRequest =&gt; &#123; context.reply(BoundPortsResponse(address.port, webUi.boundPort, restServerBoundPort)) &#125; 至此Master启动完成，Rpc部分可以参考另一篇文章：Spark RPC 到底是个什么鬼？，下面贴出Master实例化部分和onStart方法的源码及中文注释： Master实例化部分： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899//默认的情况下，取消的task不会从工作的队列中移除直到延迟时间完成，所以创建一个守护线程来“手动”移除它private val forwardMessageThread = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"master-forward-message-thread\")//用于执行重建UI代码的守护线程private val rebuildUIThread = ThreadUtils.newDaemonSingleThreadExecutor(\"master-rebuild-ui-thread\") //通过rebuildUIThread获得重建UI的执行上下文private val rebuildUIContext = ExecutionContext.fromExecutor(rebuildUIThread)//获取hadoop的配置文件private val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)//时间格式，用于构建application IDprivate def createDateFormat = new SimpleDateFormat(\"yyyyMMddHHmmss\") // For application IDs//如果Master在60s内没有收到Worker发送的heartbeat信息就认为这个Worker timeoutprivate val WORKER_TIMEOUT_MS = conf.getLong(\"spark.worker.timeout\", 60) * 1000//webUI中显示的完成的application的最大个数，超过200个就移除掉(200/10,1)=20个完成的applicationsprivate val RETAINED_APPLICATIONS = conf.getInt(\"spark.deploy.retainedApplications\", 200)//webUI中显示的完成的drivers的最大个数，超过200个就移除掉(200/10,1)=20个完成的driversprivate val RETAINED_DRIVERS = conf.getInt(\"spark.deploy.retainedDrivers\", 200)//如果Master在(REAPER_ITERATIONS + 1) * WORKER_TIMEOUT_MS)秒内仍然没有收到Worker发送的heartbeat信息，就删除这个Workerprivate val REAPER_ITERATIONS = conf.getInt(\"spark.dead.worker.persistence\", 15)//recoveryMode：NONE、ZOOKEEPER、FILESYSTEM、CUSTOM，默认是NONEprivate val RECOVERY_MODE = conf.get(\"spark.deploy.recoveryMode\", \"NONE\")//Executor失败的最大重试次数private val MAX_EXECUTOR_RETRIES = conf.getInt(\"spark.deploy.maxExecutorRetries\", 10)//下面是各种“数据结构”，不再一一说明val workers = new HashSet[WorkerInfo]val idToApp = new HashMap[String, ApplicationInfo]val waitingApps = new ArrayBuffer[ApplicationInfo]val apps = new HashSet[ApplicationInfo]private val idToWorker = new HashMap[String, WorkerInfo]private val addressToWorker = new HashMap[RpcAddress, WorkerInfo]private val endpointToApp = new HashMap[RpcEndpointRef, ApplicationInfo]private val addressToApp = new HashMap[RpcAddress, ApplicationInfo]private val completedApps = new ArrayBuffer[ApplicationInfo]private var nextAppNumber = 0// Using ConcurrentHashMap so that master-rebuild-ui-thread can add a UI after asyncRebuildUIprivate val appIdToUI = new ConcurrentHashMap[String, SparkUI]private val drivers = new HashSet[DriverInfo]private val completedDrivers = new ArrayBuffer[DriverInfo]// Drivers currently spooled for schedulingprivate val waitingDrivers = new ArrayBuffer[DriverInfo]private var nextDriverNumber = 0Utils.checkHost(address.host, \"Expected hostname\")//下面是Metrics系统相关的代码private val masterMetricsSystem = MetricsSystem.createMetricsSystem(\"master\", conf, securityMgr)private val applicationMetricsSystem = MetricsSystem.createMetricsSystem(\"applications\", conf, securityMgr)private val masterSource = new MasterSource(this)// After onStart, webUi will be setprivate var webUi: MasterWebUI = nullprivate val masterPublicAddress = &#123; val envVar = conf.getenv(\"SPARK_PUBLIC_DNS\") if (envVar != null) envVar else address.host&#125;private val masterUrl = address.toSparkURLprivate var masterWebUiUrl: String = _//当前Master的状态：STANDBY, ALIVE, RECOVERING, COMPLETING_RECOVERYprivate var state = RecoveryState.STANDBYprivate var persistenceEngine: PersistenceEngine = _private var leaderElectionAgent: LeaderElectionAgent = _private var recoveryCompletionTask: ScheduledFuture[_] = _private var checkForWorkerTimeOutTask: ScheduledFuture[_] = _// As a temporary workaround before better ways of configuring memory, we allow users to set// a flag that will perform round-robin scheduling across the nodes (spreading out each app// among all the nodes) instead of trying to consolidate each app onto a small # of nodes.// 避免将application的运行限制在固定的几个节点上private val spreadOutApps = conf.getBoolean(\"spark.deploy.spreadOut\", true)// Default maxCores for applications that don't specify it (i.e. pass Int.MaxValue)private val defaultCores = conf.getInt(\"spark.deploy.defaultCores\", Int.MaxValue)if (defaultCores &lt; 1) &#123; throw new SparkException(\"spark.deploy.defaultCores must be positive\")&#125;// Alternative application submission gateway that is stable across Spark versions// 用来接受application提交的restServerprivate val restServerEnabled = conf.getBoolean(\"spark.master.rest.enabled\", true)private var restServer: Option[StandaloneRestServer] = Noneprivate var restServerBoundPort: Option[Int] = None onStart方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859override def onStart(): Unit = &#123; //打日志 logInfo(\"Starting Spark master at \" + masterUrl) logInfo(s\"Running Spark version $&#123;org.apache.spark.SPARK_VERSION&#125;\") //实例化standalone模式下的MasterWebUI并绑定到HTTP Server webUi = new MasterWebUI(this, webUiPort) webUi.bind() //可以通过这个Url地址看到Master的信息 masterWebUiUrl = \"http://\" + masterPublicAddress + \":\" + webUi.boundPort //以固定的时间间隔检查并移除time-out的worker checkForWorkerTimeOutTask = forwardMessageThread.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; self.send(CheckForWorkerTimeOut) &#125; &#125;, 0, WORKER_TIMEOUT_MS, TimeUnit.MILLISECONDS) //实例化并启动restServer用于接受application的提交 if (restServerEnabled) &#123; val port = conf.getInt(\"spark.master.rest.port\", 6066) restServer = Some(new StandaloneRestServer(address.host, port, conf, self, masterUrl)) &#125; restServerBoundPort = restServer.map(_.start()) //启动MetricsSystem masterMetricsSystem.registerSource(masterSource) masterMetricsSystem.start() applicationMetricsSystem.start() // Attach the master and app metrics servlet handler to the web ui after the metrics systems are // started. masterMetricsSystem.getServletHandlers.foreach(webUi.attachHandler) applicationMetricsSystem.getServletHandlers.foreach(webUi.attachHandler) //序列化器 val serializer = new JavaSerializer(conf) //恢复机制，包括持久化引擎和选举机制 val (persistenceEngine_, leaderElectionAgent_) = RECOVERY_MODE match &#123; case \"ZOOKEEPER\" =&gt; logInfo(\"Persisting recovery state to ZooKeeper\") val zkFactory = new ZooKeeperRecoveryModeFactory(conf, serializer) (zkFactory.createPersistenceEngine(), zkFactory.createLeaderElectionAgent(this)) case \"FILESYSTEM\" =&gt; val fsFactory = new FileSystemRecoveryModeFactory(conf, serializer) (fsFactory.createPersistenceEngine(), fsFactory.createLeaderElectionAgent(this)) case \"CUSTOM\" =&gt; val clazz = Utils.classForName(conf.get(\"spark.deploy.recoveryMode.factory\")) val factory = clazz.getConstructor(classOf[SparkConf], classOf[Serializer]) .newInstance(conf, serializer) .asInstanceOf[StandaloneRecoveryModeFactory] (factory.createPersistenceEngine(), factory.createLeaderElectionAgent(this)) case _ =&gt; (new BlackHolePersistenceEngine(), new MonarchyLeaderAgent(this)) &#125; persistenceEngine = persistenceEngine_ leaderElectionAgent = leaderElectionAgent_ &#125; 下面介绍Worker的启动 start-slaves.sh: 12# Launch the slaves\"$&#123;SPARK_HOME&#125;/sbin/slaves.sh\" cd \"$&#123;SPARK_HOME&#125;\" \\; \"$&#123;SPARK_HOME&#125;/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT\" start-slave.sh: 1234CLASS=\"org.apache.spark.deploy.worker.Worker\"... \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start $CLASS $WORKER_NUM \\ --webui-port \"$WEBUI_PORT\" $PORT_FLAG $PORT_NUM $MASTER \"$@\" 和Master的启动类似，我们直接看Worker文件，仍然从main方法开始： 1234567891011121314151617181920212223242526272829def main(argStrings: Array[String]) &#123; SignalLogger.register(log) val conf = new SparkConf val args = new WorkerArguments(argStrings, conf) val rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores, args.memory, args.masters, args.workDir, conf = conf) rpcEnv.awaitTermination() &#125; def startRpcEnvAndEndpoint( host: String, port: Int, webUiPort: Int, cores: Int, memory: Int, masterUrls: Array[String], workDir: String, workerNumber: Option[Int] = None, conf: SparkConf = new SparkConf): RpcEnv = &#123; // The LocalSparkCluster runs multiple local sparkWorkerX RPC Environments val systemName = SYSTEM_NAME + workerNumber.map(_.toString).getOrElse(\"\") val securityMgr = new SecurityManager(conf) val rpcEnv = RpcEnv.create(systemName, host, port, conf, securityMgr) val masterAddresses = masterUrls.map(RpcAddress.fromSparkURL(_)) rpcEnv.setupEndpoint(ENDPOINT_NAME, new Worker(rpcEnv, webUiPort, cores, memory, masterAddresses, systemName, ENDPOINT_NAME, workDir, conf, securityMgr)) rpcEnv &#125; 可以看到前面和Master类似，只不过Worker有可能是多个，所以需要根据workerNumber构造一个systemName，用来创建不同的RpcEnv，然后实例化Worker（即实例化Endpoint），实例化的时候需要传入masterAddresses（注意此处可能有多个Master），以便以后向Master注册，同时由于要向对应的RpcEnv注册，注册的时候同样要执行Worker的onStart方法，我会将Worker实例化和onStart的源码放到后面，这里我们先来看一下Worker向Master注册的代码（onStart方法中调用registerWithMaster）： 12345678910111213141516171819202122private def registerWithMaster() &#123; // onDisconnected may be triggered multiple times, so don't attempt registration // if there are outstanding registration attempts scheduled. registrationRetryTimer match &#123; case None =&gt; registered = false registerMasterFutures = tryRegisterAllMasters() connectionAttemptCount = 0 registrationRetryTimer = Some(forwordMessageScheduler.scheduleAtFixedRate( new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; Option(self).foreach(_.send(ReregisterWithMaster)) &#125; &#125;, INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS, INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS, TimeUnit.SECONDS)) case Some(_) =&gt; logInfo(\"Not spawning another attempt to register with the master, since there is an\" + \" attempt scheduled already.\") &#125; &#125; 可以看到内部调用了tryRegisterAllMasters方法： 1234567891011121314151617private def tryRegisterAllMasters(): Array[JFuture[_]] = &#123; masterRpcAddresses.map &#123; masterAddress =&gt; registerMasterThreadPool.submit(new Runnable &#123; override def run(): Unit = &#123; try &#123; logInfo(\"Connecting to master \" + masterAddress + \"...\") val masterEndpoint = rpcEnv.setupEndpointRef(Master.SYSTEM_NAME, masterAddress, Master.ENDPOINT_NAME) registerWithMaster(masterEndpoint) &#125; catch &#123; case ie: InterruptedException =&gt; // Cancelled case NonFatal(e) =&gt; logWarning(s\"Failed to connect to master $masterAddress\", e) &#125; &#125; &#125;) &#125; &#125; 通过一个名为registerMasterThreadPool的线程池（最大线程数为Worker的个数）来运行run方法中的内容：首先通过setupEndpointRef方法获得其中一个Master的一个引用（RpcEndpointRef），然后执行registerWithMaster(masterEndpoint)方法，刚才得到的Master的引用作为参数传入，下面进入registerWithMaster方法：（注意此处的registerWithMaster方法是有一个RpcEndpointRef作为参数的，和刚开始的那个不一样） 1234567891011121314private def registerWithMaster(masterEndpoint: RpcEndpointRef): Unit = &#123; masterEndpoint.ask[RegisterWorkerResponse](RegisterWorker( workerId, host, port, self, cores, memory, webUi.boundPort, publicAddress)) .onComplete &#123; // This is a very fast action so we can use \"ThreadUtils.sameThread\" case Success(msg) =&gt; Utils.tryLogNonFatalError &#123; handleRegisterResponse(msg) &#125; case Failure(e) =&gt; logError(s\"Cannot register with master: $&#123;masterEndpoint.address&#125;\", e) System.exit(1) &#125;(ThreadUtils.sameThread) &#125; 内部使用masterEndpoint（Master的RpcEndpointRef）的ask方法向Master发送一条RegisterWorker的消息，并使用onComplete方法接受Master的处理结果，下面我们先来看一下消息到达Master端进行怎样的处理： 12345678910111213141516171819202122232425override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = &#123; case RegisterWorker( id, workerHost, workerPort, workerRef, cores, memory, workerUiPort, publicAddress) =&gt; &#123; logInfo(\"Registering worker %s:%d with %d cores, %s RAM\".format( workerHost, workerPort, cores, Utils.megabytesToString(memory))) if (state == RecoveryState.STANDBY) &#123; context.reply(MasterInStandby) &#125; else if (idToWorker.contains(id)) &#123; context.reply(RegisterWorkerFailed(\"Duplicate worker ID\")) &#125; else &#123; val worker = new WorkerInfo(id, workerHost, workerPort, cores, memory, workerRef, workerUiPort, publicAddress) if (registerWorker(worker)) &#123; persistenceEngine.addWorker(worker) context.reply(RegisteredWorker(self, masterWebUiUrl)) schedule() &#125; else &#123; val workerAddress = worker.endpoint.address logWarning(\"Worker registration failed. Attempted to re-register worker at same \" + \"address: \" + workerAddress) context.reply(RegisterWorkerFailed(\"Attempted to re-register worker at same address: \" + workerAddress)) &#125; &#125; &#125; 首先receiveAndReply方法匹配到Worker发过来的RegisterWorker消息，然后执行具体的操作：打了一个日志，判断Master现在的状态，如果是STANDBY就reply一个MasterInStandby的消息，如果idToWorker中已经存在该Worker的ID就回复重复的worker ID的失败信息，如果都不是，将获得的Worker信息用WorkerInfo进行封装，然后执行registerWorker(worker)操作注册该Worker，如果成功就向persistenceEngine中添加该Worker并reply给Worker RegisteredWorker(self, masterWebUiUrl)消息并执行schedule方法，如果注册失败就reply RegisterWorkerFailed消息，下面我们具体看一下Master端是如何注册Worker的，即registerWorker(worker)方法： 123456789101112131415161718192021222324252627private def registerWorker(worker: WorkerInfo): Boolean = &#123; // There may be one or more refs to dead workers on this same node (w/ different ID's), // remove them. workers.filter &#123; w =&gt; (w.host == worker.host &amp;&amp; w.port == worker.port) &amp;&amp; (w.state == WorkerState.DEAD) &#125;.foreach &#123; w =&gt; workers -= w &#125; val workerAddress = worker.endpoint.address if (addressToWorker.contains(workerAddress)) &#123; val oldWorker = addressToWorker(workerAddress) if (oldWorker.state == WorkerState.UNKNOWN) &#123; // A worker registering from UNKNOWN implies that the worker was restarted during recovery. // The old worker must thus be dead, so we will remove it and accept the new worker. removeWorker(oldWorker) &#125; else &#123; logInfo(\"Attempted to re-register worker at same address: \" + workerAddress) return false &#125; &#125; workers += worker idToWorker(worker.id) = worker addressToWorker(workerAddress) = worker true &#125; 首先判断是否有和该Worker的host和port相同且状态为DEAD的Worker，如果有就remove掉，然后获得该Worker的RpcAddress，然后根据RpcAddress判断addressToWorker中是否有相同地址的记录，如果有记录且老的Worker的状态为UNKNOWN就remove掉老的Worker，如果没有记录就打日志并返回false（导致上一步reply：RegisterWorkerFailed）然后分别在workers、idToWorker、addressToWorker中添加该Worker，最后返回true，导致上一步向Worker reply注册成功的消息：context.reply(RegisteredWorker(self, masterWebUiUrl))，并执行schedule()，即向等待的applications分配当前可用的资源（每当新的application加入或者有资源变化时都会调用该方法），这个方法我会用单独的一片文章详细分析，现在我们先来看Worker端是如何进行回复的，回到上面的registerWithMaster方法（有参数的），我们直接看成功后执行的handleRegisterResponse(msg)这个方法： 12345678910111213141516171819202122232425262728293031private def handleRegisterResponse(msg: RegisterWorkerResponse): Unit = synchronized &#123; msg match &#123; case RegisteredWorker(masterRef, masterWebUiUrl) =&gt; logInfo(\"Successfully registered with master \" + masterRef.address.toSparkURL) registered = true changeMaster(masterRef, masterWebUiUrl) forwordMessageScheduler.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; self.send(SendHeartbeat) &#125; &#125;, 0, HEARTBEAT_MILLIS, TimeUnit.MILLISECONDS) if (CLEANUP_ENABLED) &#123; logInfo( s\"Worker cleanup enabled; old application directories will be deleted in: $workDir\") forwordMessageScheduler.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; self.send(WorkDirCleanup) &#125; &#125;, CLEANUP_INTERVAL_MILLIS, CLEANUP_INTERVAL_MILLIS, TimeUnit.MILLISECONDS) &#125; case RegisterWorkerFailed(message) =&gt; if (!registered) &#123; logError(\"Worker registration failed: \" + message) System.exit(1) &#125; case MasterInStandby =&gt; // Ignore. Master not yet ready. &#125; &#125; 依然是模式匹配的方式： 如果接受到的是RegisteredWorker，会执行changeMaster方法，取消最后一次的重试，然后向自己的RpcEnv发送SendHeartBeat消息，使用receive方法接受到该消息后会通过sendToMaster方法向Master发送心跳，最后判断CLEANUP_ENABLED如果开启就向自己的RpcEnv发送WorkDirCleanup消息，接受到消息后将老的application的目录清除 如果接受到的是RegisterWorkerFailed就表明注册失败 changeMaster发送： 123456789private def changeMaster(masterRef: RpcEndpointRef, uiUrl: String) &#123; // activeMasterUrl it's a valid Spark url since we receive it from master. activeMasterUrl = masterRef.address.toSparkURL activeMasterWebUiUrl = uiUrl master = Some(masterRef) connected = true // Cancel any outstanding re-registration attempts because we found a new master cancelLastRegistrationRetry() &#125; cancelLastRegistrationRetry: 12345678private def cancelLastRegistrationRetry(): Unit = &#123; if (registerMasterFutures != null) &#123; registerMasterFutures.foreach(_.cancel(true)) registerMasterFutures = null &#125; registrationRetryTimer.foreach(_.cancel(true)) registrationRetryTimer = None &#125; 如果Worker注册失败同样会通过registrationRetryTimer进行重试： 123456789registrationRetryTimer = Some(forwordMessageScheduler.scheduleAtFixedRate( new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; Option(self).foreach(_.send(ReregisterWithMaster)) &#125; &#125;, INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS, INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS, TimeUnit.SECONDS)) 可以看到向自己发送重新注册的消息：ReregisterWithMaster，receive接收到后会执行reregisterWithMaster()方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586private def reregisterWithMaster(): Unit = &#123; Utils.tryOrExit &#123; //重试次数加1 connectionAttemptCount += 1 if (registered) &#123; //如果已经注册了，就取消重试 cancelLastRegistrationRetry() &#125; else if (connectionAttemptCount &lt;= TOTAL_REGISTRATION_RETRIES) &#123; //判断是否超过最大重试次数 logInfo(s\"Retrying connection to master (attempt # $connectionAttemptCount)\") /** * Re-register with the active master this worker has been communicating with. If there * is none, then it means this worker is still bootstrapping and hasn't established a * connection with a master yet, in which case we should re-register with all masters. * * It is important to re-register only with the active master during failures. Otherwise, * if the worker unconditionally attempts to re-register with all masters, the following * race condition may arise and cause a \"duplicate worker\" error detailed in SPARK-4592: * * (1) Master A fails and Worker attempts to reconnect to all masters * (2) Master B takes over and notifies Worker * (3) Worker responds by registering with Master B * (4) Meanwhile, Worker's previous reconnection attempt reaches Master B, * causing the same Worker to register with Master B twice * * Instead, if we only register with the known active master, we can assume that the * old master must have died because another master has taken over. Note that this is * still not safe if the old master recovers within this interval, but this is a much * less likely scenario. */ master match &#123; case Some(masterRef) =&gt; // registered == false &amp;&amp; master != None means we lost the connection to master, so // masterRef cannot be used and we need to recreate it again. Note: we must not set // master to None due to the above comments. // 这里说的很清楚，如果注册失败了，但是master != None说明我们失去了和master的连接，所以需要重新创建一个masterRef // 先取消原来阻塞的用来等待消息回复的线程 if (registerMasterFutures != null) &#123; registerMasterFutures.foreach(_.cancel(true)) &#125; // 然后创建新的masterRef，然后重新注册 val masterAddress = masterRef.address registerMasterFutures = Array(registerMasterThreadPool.submit(new Runnable &#123; override def run(): Unit = &#123; try &#123; logInfo(\"Connecting to master \" + masterAddress + \"...\") val masterEndpoint = rpcEnv.setupEndpointRef(Master.SYSTEM_NAME, masterAddress, Master.ENDPOINT_NAME) registerWithMaster(masterEndpoint) &#125; catch &#123; case ie: InterruptedException =&gt; // Cancelled case NonFatal(e) =&gt; logWarning(s\"Failed to connect to master $masterAddress\", e) &#125; &#125; &#125;)) case None =&gt; // 如果没有masterRef，先取消原来阻塞的用来等待消息回复的线程 if (registerMasterFutures != null) &#123; registerMasterFutures.foreach(_.cancel(true)) &#125; // 然后执行最初的注册，即tryRegisterAllMasters // We are retrying the initial registration registerMasterFutures = tryRegisterAllMasters() &#125; // We have exceeded the initial registration retry threshold // All retries from now on should use a higher interval // 如果超过刚开始设置的重试注册次数，取消之前的重试，开启新的注册，并改变重试次数和时间间隔 // 刚开始的重试默认为6次，时间间隔在5到15秒之间，接下来的10次重试时间间隔在30到90秒之间 if (connectionAttemptCount == INITIAL_REGISTRATION_RETRIES) &#123; registrationRetryTimer.foreach(_.cancel(true)) registrationRetryTimer = Some( forwordMessageScheduler.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; self.send(ReregisterWithMaster) &#125; &#125;, PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS, PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS, TimeUnit.SECONDS)) &#125; &#125; else &#123; logError(\"All masters are unresponsive! Giving up.\") System.exit(1) &#125; &#125; &#125; 至此Worker的启动和注册完成，即start-all.sh执行完成。 下面是Worker的初始化部分和onStart方法的源码及注释（重要部分）： 初始化部分： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106private val host = rpcEnv.address.hostprivate val port = rpcEnv.address.portUtils.checkHost(host, \"Expected hostname\")assert (port &gt; 0)// A scheduled executor used to send messages at the specified time.private val forwordMessageScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"worker-forward-message-scheduler\")// A separated thread to clean up the workDir. Used to provide the implicit parameter of `Future`// methods.private val cleanupThreadExecutor = ExecutionContext.fromExecutorService( ThreadUtils.newDaemonSingleThreadExecutor(\"worker-cleanup-thread\"))// For worker and executor IDsprivate def createDateFormat = new SimpleDateFormat(\"yyyyMMddHHmmss\")// 发送心跳的时间间隔：timeout的时间 / 4// Send a heartbeat every (heartbeat timeout) / 4 millisecondsprivate val HEARTBEAT_MILLIS = conf.getLong(\"spark.worker.timeout\", 60) * 1000 / 4// 重试的模型及其次数设置// Model retries to connect to the master, after Hadoop's model.// The first six attempts to reconnect are in shorter intervals (between 5 and 15 seconds)// Afterwards, the next 10 attempts are between 30 and 90 seconds.// A bit of randomness is introduced so that not all of the workers attempt to reconnect at// the same time.private val INITIAL_REGISTRATION_RETRIES = 6private val TOTAL_REGISTRATION_RETRIES = INITIAL_REGISTRATION_RETRIES + 10private val FUZZ_MULTIPLIER_INTERVAL_LOWER_BOUND = 0.500private val REGISTRATION_RETRY_FUZZ_MULTIPLIER = &#123; val randomNumberGenerator = new Random(UUID.randomUUID.getMostSignificantBits) randomNumberGenerator.nextDouble + FUZZ_MULTIPLIER_INTERVAL_LOWER_BOUND&#125;private val INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS = (math.round(10 * REGISTRATION_RETRY_FUZZ_MULTIPLIER))private val PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS = (math.round(60 * REGISTRATION_RETRY_FUZZ_MULTIPLIER))//CLEANUP相关的设置private val CLEANUP_ENABLED = conf.getBoolean(\"spark.worker.cleanup.enabled\", false)// How often worker will clean up old app foldersprivate val CLEANUP_INTERVAL_MILLIS = conf.getLong(\"spark.worker.cleanup.interval\", 60 * 30) * 1000// TTL for app folders/data; after TTL expires it will be cleaned upprivate val APP_DATA_RETENTION_SECONDS = conf.getLong(\"spark.worker.cleanup.appDataTtl\", 7 * 24 * 3600)private val testing: Boolean = sys.props.contains(\"spark.testing\")//对master的引用private var master: Option[RpcEndpointRef] = Noneprivate var activeMasterUrl: String = \"\"private[worker] var activeMasterWebUiUrl : String = \"\"private val workerUri = rpcEnv.uriOf(systemName, rpcEnv.address, endpointName)private var registered = falseprivate var connected = falseprivate val workerId = generateWorkerId()private val sparkHome = if (testing) &#123; assert(sys.props.contains(\"spark.test.home\"), \"spark.test.home is not set!\") new File(sys.props(\"spark.test.home\")) &#125; else &#123; new File(sys.env.get(\"SPARK_HOME\").getOrElse(\".\")) &#125;var workDir: File = nullval finishedExecutors = new LinkedHashMap[String, ExecutorRunner]val drivers = new HashMap[String, DriverRunner]val executors = new HashMap[String, ExecutorRunner]val finishedDrivers = new LinkedHashMap[String, DriverRunner]val appDirectories = new HashMap[String, Seq[String]]val finishedApps = new HashSet[String]val retainedExecutors = conf.getInt(\"spark.worker.ui.retainedExecutors\", WorkerWebUI.DEFAULT_RETAINED_EXECUTORS)val retainedDrivers = conf.getInt(\"spark.worker.ui.retainedDrivers\", WorkerWebUI.DEFAULT_RETAINED_DRIVERS)// The shuffle service is not actually started unless configured.private val shuffleService = new ExternalShuffleService(conf, securityMgr)private val publicAddress = &#123; val envVar = conf.getenv(\"SPARK_PUBLIC_DNS\") if (envVar != null) envVar else host&#125;private var webUi: WorkerWebUI = nullprivate var connectionAttemptCount = 0private val metricsSystem = MetricsSystem.createMetricsSystem(\"worker\", conf, securityMgr)private val workerSource = new WorkerSource(this)private var registerMasterFutures: Array[JFuture[_]] = nullprivate var registrationRetryTimer: Option[JScheduledFuture[_]] = None// 用来和Master注册使用的线程池，默认线程的最大个数为Worker的个数// A thread pool for registering with masters. Because registering with a master is a blocking// action, this thread pool must be able to create \"masterRpcAddresses.size\" threads at the same// time so that we can register with all masters.private val registerMasterThreadPool = ThreadUtils.newDaemonCachedThreadPool( \"worker-register-master-threadpool\", masterRpcAddresses.size // Make sure we can register with all masters at the same time)var coresUsed = 0var memoryUsed = 0 onStart()方法： 123456789101112131415161718192021override def onStart() &#123; assert(!registered) logInfo(\"Starting Spark worker %s:%d with %d cores, %s RAM\".format( host, port, cores, Utils.megabytesToString(memory))) logInfo(s\"Running Spark version $&#123;org.apache.spark.SPARK_VERSION&#125;\") logInfo(\"Spark home: \" + sparkHome) // 创建Work的目录 createWorkDir() // 开启 external shuffle service shuffleService.startIfEnabled() webUi = new WorkerWebUI(this, workDir, webUiPort) webUi.bind() // 向Master注册自己 registerWithMaster() // metrics系统 metricsSystem.registerSource(workerSource) metricsSystem.start() // Attach the worker metrics servlet handler to the web ui after the metrics system is started. metricsSystem.getServletHandlers.foreach(webUi.attachHandler) &#125; 本文简单介绍了Spark的几种部署模式，并详细的分析了start-all.sh所执行源码（Master的启动和注册、Worker的启动和向Master的注册）的具体流程，当然Master的schedule方法并没有详细说明，我们会单独用一篇文章进行详细的分析。 本文参考和拓展阅读： Spark-1.6.3源码 Spark-2.1.0源码 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"RPC","slug":"prc","permalink":"http://www.sun4lower.cn/tags/prc/"},{"name":"spark","slug":"spark","permalink":"http://www.sun4lower.cn/tags/spark/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"},{"name":"master","slug":"master","permalink":"http://www.sun4lower.cn/tags/master/"},{"name":"worker","slug":"worker","permalink":"http://www.sun4lower.cn/tags/worker/"},{"name":"deployment","slug":"deployment","permalink":"http://www.sun4lower.cn/tags/deployment/"},{"name":"standalone","slug":"standalone","permalink":"http://www.sun4lower.cn/tags/standalone/"},{"name":"yarn","slug":"yarn","permalink":"http://www.sun4lower.cn/tags/yarn/"},{"name":"mesos","slug":"mesos","permalink":"http://www.sun4lower.cn/tags/mesos/"}]},{"title":"Spark RPC 到底是个什么鬼？","slug":"SparkRpcBasic","date":"2017-02-22T09:49:24.000Z","updated":"2017-02-24T07:49:21.000Z","comments":true,"path":"2017/02/22/SparkRpcBasic/","link":"","permalink":"http://www.sun4lower.cn/2017/02/22/SparkRpcBasic/","excerpt":"","text":"本文会为大家介绍Spark中的RPC通信机制，详细阐述“Spark RPC到底是个什么鬼？”，闲话少叙，让我们来进入Spark RPC的世界！ Spark RPC三剑客Spark RPC中最为重要的三个抽象（“三剑客”）为：RpcEnv、RpcEndpoint、RpcEndpointRef，这样做的好处有： 对上层的API来说，屏蔽了底层的具体实现，使用方便 可以通过不同的实现来完成指定的功能，方便扩展 促进了底层实现层的良性竞争，Spark 1.6.3中默认使用了Netty作为底层的实现，但Akka的依赖依然存在；而Spark 2.1.0中的底层实现只有Netty，这样用户可以方便的使用不同版本的Akka或者将来某种更好的底层实现 下面我们就结合Netty和“三剑客”来具体分析他们是如何来协同工作的。 Send a message locally我们通过Spark源码中的一个Test（RpcEnvSuite.scala）来分析一下发送本地消息的具体流程，源码如下（对源码做了一些修改）： 12345678910111213141516test(\"send a message locally\") &#123; @volatile var message: String = null val rpcEndpointRef = env.setupEndpoint(\"send-locally\", new RpcEndpoint &#123; override val rpcEnv = env override def receive = &#123; //case msg: String =&gt; message = msg case msg: String =&gt; println(message) //我们直接将接收到的消息打印出来 &#125; &#125;) rpcEndpointRef.send(\"hello\") //下面是原来的代码 //eventually(timeout(5 seconds), interval(10 millis)) &#123; // assert(\"hello\" === message) //&#125;&#125; 为了方便理解，先把流程图贴出来，然后详细进行阐述： 下面我们来详细阐述上例的具体过程： 首先是RpcEndpoint创建并注册的流程：（图中的蓝色线条部分） 1、创建RpcEndpoint，并初始化rpcEnv的引用（RpcEnv已经创建好，底层实际上是实例化了一个NettyRpcEnv，而NettyRpcEnv是通过工厂方法NettyRpcEnvFactory创建的） 2、实例化RpcEndpoint之后需要向RpcEnv注册该RpcEndpoint，底层实现是向NettyRpcEnv进行注册，而实际上是通过调用Dispatcher的registerRpcEndpoint方法向Dispatcher进行注册 3、具体的注册就是向endpoints、endpointRefs、receivers中插入记录：而receivers中插入的信息会被Dispatcher中的线程池中的线程执行：会将记录take出来然后调用Inbox的process方法通过模式匹配的方法进行处理，注册的时候通过匹配到OnStart类型的message，去执行RpcEndpoint的onStart方法（例如Master、Worker注册时，就要执行各自的onStart方法），本例中未做任何操作 4、注册完成后返回RpcEndpointRef，我们通过RpcEndpointRef就可以向其代表的RpcEndpoint发送消息 下面就是通过RpcEndpointRef向其代表的RpcEndpoint发送消息的具体流程：（图中的红色线条部分） 1、2、调用RpcEndpointRef的send方法，底层实现是调用Netty的NettyRpcEndpointRef的send方法，而实际上又是调用的NettyRpcEnv的send方法，发送的消息使用RequestMessage进行封装： 1nettyEnv.send(RequestMessage(nettyEnv.address, this, message)) 3、4、NettyRpcEnv的send方法首先会根据RpcAddress判断是本地还是远程调用，此处是同一个RpcEnv，所以是本地调用，即调用Dispatcher的postOneWayMessage方法 5、postOneWayMessage方法内部调用Dispatcher的postMessage方法 6、postMessage会向具体的RpcEndpoint发送消息，首先通过endpointName从endpoints中获得注册时的EndpointData，如果不为空就执行EndpointData中Inbox的post(message)方法，向Inbox的mesages中插入一条InboxMessage，同时向receivers中插入一条记录，此处将Inbox单独画出来是为了方便大家理解 7、Dispatcher中的线程池会拿出一条线程用来循环receivers中的消息，首先使用take方法获得receivers中的一条记录，然后调用Inbox的process方法来执行这条记录，而process将messages中的一条InboxMessage（第6步中插入的）拿出来进行处理，具体的处理方法就是通过模式匹配的方法，匹配到消息的类型（此处是OneWayMessage），然后来执行RpcEndpoint中对应的receive方法，在此例中我们只打印出这条消息（步骤8） 至此，一个简单的发送本地消息的流程执行完成。 什么，上面的图太复杂了？我也觉得，下面给出一张简洁的图： 我们通过NettyRpcEndpointRef来发出一个消息，消息经过NettyRpcEnv、Dispatcher、Inbox的共同处理最终将消息发送到NettyRpcEndpoint，NettyRpcEndpoint收到消息后进行处理（一般是通过模式匹配的方式进行不同的处理） 如果进一步的进行抽象就得到了我们刚开始所讲的“三剑客”：RpcEnv、RpcEndpoint、RpcEndpointRef RpcEndpointRef发送消息给RpcEnv，RpcEnv查询注册信息将消息路由到指定的RpcEndpoint，RpcEndpoint接收到消息后进行处理（模式匹配的方式） RpcEndpoint的声明周期：constructor -&gt; onStart -&gt; receive* -&gt; onStop 其中receive*包括receive和receiveAndReply 本文我们只是通过一个简单的测试程序分析了Spark Rpc底层的实现，集群中的其它通信（比如Master和Woker的通信）的原理和这个测试类似，只不过具体的发送方式有所不同（包括ask、askWithRetry等），而且远程发消息的时候使用了OutBox和NIO等相关的内容，感兴趣的朋友可以对源码进行详细的阅读，本文不一一说明，目的就是通过简单的测试理解大致流程，不再为“Spark Rpc到底是什么”而纠结，一句话总结：Spark Rpc就是Spark中对分布式消息通信系统的高度抽象。 本文参考和拓展阅读： spark源码 Netty官方网站 Java NIO Tutorial 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"通信机制","slug":"event","permalink":"http://www.sun4lower.cn/tags/event/"},{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"Netty","slug":"netty","permalink":"http://www.sun4lower.cn/tags/netty/"},{"name":"RPC","slug":"prc","permalink":"http://www.sun4lower.cn/tags/prc/"},{"name":"spark","slug":"spark","permalink":"http://www.sun4lower.cn/tags/spark/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"}]},{"title":"浅显易懂之线程模型的演变","slug":"event-threadmodel","date":"2017-02-22T09:49:24.000Z","updated":"2017-02-22T13:36:07.000Z","comments":true,"path":"2017/02/22/event-threadmodel/","link":"","permalink":"http://www.sun4lower.cn/2017/02/22/event-threadmodel/","excerpt":"","text":"本文旨在通过一个实际工作中例子让大家理解线程模型的演变，从而对 Netty 的模型有一个粗略的印象，面向的是想初步了解 Netty 原理的读者，而并不关心具体的执行细节和相关术语的描述。 单线程单线程映射到我们的实际工作中就是由一个人完成所有的工作，如下图所示： A 自己独立的完成全部的Job 多线程多线程映射到我们的工作中就是多个人共同协作完成工作，如下图所示： A、B、C 三人工作完成工作，每个人会分到具体的工作去执行，较之单线程，执行效率提高。 线程池多个 Worker（A、B、C等）已经准备好去完成即将到来的Job，当Job 1过来的时候，会从线程池中选择一个线程也就是Worker（A）来完成这项任务，完成之后A仍然要回到线程池中，等待下一个工作的到来。 Reactor单线程模型Reactor 的单线程模型映射到实际工作中如下图所示： 我们有一个Boss来接收具体的Job，然后Boss将具体接收到的Job分配给已经准备好的Worker，交给具体的Worker来完成最终的Job，但是如果Job非常多的时候，Boss的压力就会越来越大，所以Boss成为了性能的瓶颈，而下面提到的Reactor多线程模型就是为了解决这个问题而产生的。 Reactor多线程模型为了完善Reactor单线程模型，Reactor多线程模型进行了优化： 和Reactor单线程模型相比，Reactor多线程模型增加了一个从Manager pool选择Manager(相当于项目经理)的过程，即通过Manager来帮助Boss分配具体的Job给Worker，例如图中选择了M1，那么M1就会去完成选择Worker的工作，虚线代表选择了M2或者M3。 Netty的具体实现就类似于Reactor的多线程模型，而Spark现在Rpc的底层就是通过Netty来实现的。 至此本文的目的已经达到，如果想要更加详细的了解Netty的具体实现细节和NIO相关的知识可以参考如下文章： Netty系列之Netty线程模型 并发编程网有关Netty的部分 Netty官方网站 Java NIO Tutorial 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"通信机制","slug":"bigdata/event","permalink":"http://www.sun4lower.cn/categories/bigdata/event/"}],"tags":[{"name":"通信机制","slug":"event","permalink":"http://www.sun4lower.cn/tags/event/"},{"name":"Netty","slug":"netty","permalink":"http://www.sun4lower.cn/tags/netty/"},{"name":"多线程","slug":"多线程","permalink":"http://www.sun4lower.cn/tags/多线程/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-02-19T04:52:30.000Z","updated":"2017-02-20T03:19:14.000Z","comments":true,"path":"2017/02/19/hello-world/","link":"","permalink":"http://www.sun4lower.cn/2017/02/19/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server 123public static void main(String[] args)&#123; System.out.print(\"hello world\")&#125; 12def sum(a: Int, b: Int): Int = a + bprintln(\"hello world\") More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment hello world","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"},{"name":"性能调优","slug":"perf","permalink":"http://www.sun4lower.cn/tags/perf/"}]}]}