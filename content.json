{"pages":[],"posts":[{"title":"Spark-Core源码精读(2)、spark-shell(spark-submit)流程详解","slug":"sc-sparkshell","date":"2017-03-02T05:28:17.000Z","updated":"2017-03-02T05:33:14.000Z","comments":true,"path":"2017/03/02/sc-sparkshell/","link":"","permalink":"http://www.sun4lower.cn/2017/03/02/sc-sparkshell/","excerpt":"","text":"本文将解读使用spark-shell的方式进入REPL的具体流程。 注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。 shell部分下面我们来看一下当我们输入 spark-shell –master spark://master:7077时具体的执行流程，首先当然是看一下spark-shell.sh的源码，我们只选取了相对比较重要的部分： 123456789101112131415161718192021222324##检测有没有设置SPARK_HOME环境变量，如果没有进行设置if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi##...function main() &#123; if $cygwin; then # Workaround for issue involving JLine and Cygwin # (see http://sourceforge.net/p/jline/bugs/40/). # If you're using the Mintty terminal emulator in Cygwin, may need to set the # \"Backspace sends ^H\" setting in \"Keys\" section of the Mintty options # (see https://github.com/sbt/sbt/issues/562). stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1 export SPARK_SUBMIT_OPTS=\"$SPARK_SUBMIT_OPTS -Djline.terminal=unix\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-submit --class org.apache.spark.repl.Main --name \"Spark shell\" \"$@\" stty icanon echo &gt; /dev/null 2&gt;&amp;1 else export SPARK_SUBMIT_OPTS \"$&#123;SPARK_HOME&#125;\"/bin/spark-submit --class org.apache.spark.repl.Main --name \"Spark shell\" \"$@\" fi&#125;##...main \"$@\"##... 可以看出最后执行的是main方法并传入我们使用spark-shell命令时候的所有参数，比如–master，而main方法中无论是什么操作系统(当然生产环境是linux系统)都会最终执行spark-submit，并且class为org.apache.spark.repl.Main、name为“Spark shell”并且将spark-shell所有接收到的用户输入的参数一起传进去，下面我们来看spark-submit： 12345678if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi# disable randomized hash for string in Python 3.3+export PYTHONHASHSEED=0exec \"$&#123;SPARK_HOME&#125;\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\" spark-submit的代码比较简洁，最后使用exec通过spark-class来启动SparkSubmit并将spark-submit接收到的所有参数传入，下面我们来看一下spark-class：(这里要说明一下，从这里开始起始就是我们通过spark-submit提交application的过程，只不过spark-submit提交的application运行完成后就会结束，而REPL一直等待用户的输入) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi## 载入环境变量. \"$&#123;SPARK_HOME&#125;\"/bin/load-spark-env.sh## 获得java的二进制文件，后面会用来启动一个JVM进行# Find the java binaryif [ -n \"$&#123;JAVA_HOME&#125;\" ]; then RUNNER=\"$&#123;JAVA_HOME&#125;/bin/java\"else if [ `command -v java` ]; then RUNNER=\"java\" else echo \"JAVA_HOME is not set\" &gt;&amp;2 exit 1 fifi## jar包的相关依赖# Find assembly jarSPARK_ASSEMBLY_JAR=if [ -f \"$&#123;SPARK_HOME&#125;/RELEASE\" ]; then ASSEMBLY_DIR=\"$&#123;SPARK_HOME&#125;/lib\"else ASSEMBLY_DIR=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION\"fiGREP_OPTIONS=num_jars=\"$(ls -1 \"$ASSEMBLY_DIR\" | grep \"^spark-assembly.*hadoop.*\\.jar$\" | wc -l)\"if [ \"$num_jars\" -eq \"0\" -a -z \"$SPARK_ASSEMBLY_JAR\" -a \"$SPARK_PREPEND_CLASSES\" != \"1\" ]; then echo \"Failed to find Spark assembly in $ASSEMBLY_DIR.\" 1&gt;&amp;2 echo \"You need to build Spark before running this program.\" 1&gt;&amp;2 exit 1fiif [ -d \"$ASSEMBLY_DIR\" ]; then ASSEMBLY_JARS=\"$(ls -1 \"$ASSEMBLY_DIR\" | grep \"^spark-assembly.*hadoop.*\\.jar$\" || true)\" if [ \"$num_jars\" -gt \"1\" ]; then echo \"Found multiple Spark assembly jars in $ASSEMBLY_DIR:\" 1&gt;&amp;2 echo \"$ASSEMBLY_JARS\" 1&gt;&amp;2 echo \"Please remove all but one jar.\" 1&gt;&amp;2 exit 1 fifiSPARK_ASSEMBLY_JAR=\"$&#123;ASSEMBLY_DIR&#125;/$&#123;ASSEMBLY_JARS&#125;\"LAUNCH_CLASSPATH=\"$SPARK_ASSEMBLY_JAR\"# Add the launcher build dir to the classpath if requested.if [ -n \"$SPARK_PREPEND_CLASSES\" ]; then LAUNCH_CLASSPATH=\"$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH\"fiexport _SPARK_ASSEMBLY=\"$SPARK_ASSEMBLY_JAR\"# For testsif [[ -n \"$SPARK_TESTING\" ]]; then unset YARN_CONF_DIR unset HADOOP_CONF_DIRfi# The launcher library will print arguments separated by a NULL character, to allow arguments with# characters that would be otherwise interpreted by the shell. Read that in a while loop, populating# an array that will be used to exec the final command.CMD=()while IFS= read -d '' -r ARG; do CMD+=(\"$ARG\") ## 使用java -cp命令启动一个JVM进程并执行org.apache.spark.launcher.Main类的main方法，后面我们会看到这个进程就是SparkSubmit进程done &lt; &lt;(\"$RUNNER\" -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\")exec \"$&#123;CMD[@]&#125;\" spark-class是Spark应用程序的命令行启动器，负责设置JVM环境并执行Spark的应用程序，这里我们执行的就是SparkSubmit，下面我们就进入到Spark源码的部分。 Spark源码部分承接上文，我们直接进入Spark的源码： 关于org.apache.spark.launcher.Main的源码我们这里不做说明，大家可以把它看成Spark应用程序命令行的启动器，我们主要关注Spark本身，所以直接进入SparkSubmit的源码部分： 123456789101112131415161718192021222324def main(args: Array[String]): Unit = &#123; /** 使用SparkSubmitArguments封装spark-submit传入的参数，还记得都有什么吗？ 如果是spark-shell，就包括spark-shell及后面的一串参数，如果是直接使用spark-submit进行提交 后面就是提交时传入的参数，由于SparkSubmitArguments中的参数比较多，本文中不再一一列出 会在使用到某个参数的时候进行说明，详细的参数可以参看SparkSubmitArguments的源码。 */ val appArgs = new SparkSubmitArguments(args) // 如果开启了debug模式就打印出参数 if (appArgs.verbose) &#123; // scalastyle:off println printStream.println(appArgs) // scalastyle:on println &#125; /** 这里的action就是spark-submit执行的动作，包括：SUBMIT, KILL, REQUEST_STATUS(使 用了SparkSubmitAction进行了封装)，如果没有指定，默认就是SparkSubmitAction.SUBMIT， 所以下面的这个模式匹配将执行submit(appArgs) */ appArgs.action match &#123; case SparkSubmitAction.SUBMIT =&gt; submit(appArgs) case SparkSubmitAction.KILL =&gt; kill(appArgs) case SparkSubmitAction.REQUEST_STATUS =&gt; requestStatus(appArgs) &#125;&#125; 下面我们来看submit(appArgs)方法： 1234567891011121314151617181920212223242526272829303132333435363738/**submit方法的主要功能就是使用传进来的参数来提交应用程序。主要分为两步骤：1. 准备启动所需的环境，包括设置classpath、系统参数和应用程序的参数(根据部署模式和clustermanager运行child main类)2. 使用上一步准备好的环境调用child main class中的main函数，如果是spark-shell，childmain class就是org.apache.spark.repl.Main，如果是spark-submit直接进行提交，childmain class就是用户编写的应用程序(含有main方法的类)*/private def submit(args: SparkSubmitArguments): Unit = &#123; // 准备环境，主要就是获得childMainClass，即我们上面所说的child main class val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args) // 注意：源码中这里是doRunMain()方法，我们在后面单独拿出来进行分析 // 判断gateway使用的是Akka还是基于REST的，但是不论那种方式最后都会调用doRunMain()方法 // In standalone cluster mode, there are two submission gateways: // (1) The traditional Akka gateway using o.a.s.deploy.Client as a wrapper // (2) The new REST-based gateway introduced in Spark 1.3 // The latter is the default behavior as of Spark 1.3, but Spark submit will fail over // to use the legacy gateway if the master endpoint turns out to be not a REST server. if (args.isStandaloneCluster &amp;&amp; args.useRest) &#123; try &#123; // scalastyle:off println printStream.println(\"Running Spark using the REST application submission protocol.\") // scalastyle:on println doRunMain() &#125; catch &#123; // Fail over to use the legacy submission gateway case e: SubmitRestConnectionException =&gt; printWarning(s\"Master endpoint $&#123;args.master&#125; was not a REST server. \" + \"Falling back to legacy submission gateway instead.\") args.useRest = false submit(args) &#125; // In all other modes, just run the main class as prepared &#125; else &#123; doRunMain() &#125;&#125; doRunMain()的实现部分： 1234567891011121314151617181920212223242526272829def doRunMain(): Unit = &#123; if (args.proxyUser != null) &#123; // 这里是hadoop相关的用户和组的信息 val proxyUser = UserGroupInformation.createProxyUser(args.proxyUser, UserGroupInformation.getCurrentUser()) try &#123; proxyUser.doAs(new PrivilegedExceptionAction[Unit]() &#123; override def run(): Unit = &#123; runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose) &#125; &#125;) &#125; catch &#123; case e: Exception =&gt; // Hadoop's AuthorizationException suppresses the exception's stack trace, which // makes the message printed to the output by the JVM not very helpful. Instead, // detect exceptions with empty stack traces here, and treat them differently. if (e.getStackTrace().length == 0) &#123; // scalastyle:off println printStream.println(s\"ERROR: $&#123;e.getClass().getName()&#125;: $&#123;e.getMessage()&#125;\") // scalastyle:on println exitFn(1) &#125; else &#123; throw e &#125; &#125; &#125; else &#123; runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose) &#125;&#125; 我们看到doRunMain()内部最终都执行了runMain方法，所以我们进入runMain方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** 别看这个方法这么长，主要做的事情就是一件：运行child main class的main方法再次说明一下，如果是直接使用spark-submit提交的应用程序，就是执行用户指定的类的main方法如果是通过spark-shell执行的，就是执行org.apache.spark.repl.Main中的main方法*/private def runMain( childArgs: Seq[String], childClasspath: Seq[String], sysProps: Map[String, String], childMainClass: String, verbose: Boolean): Unit = &#123; //是否打印debug信息 // scalastyle:off println if (verbose) &#123; printStream.println(s\"Main class:\\n$childMainClass\") printStream.println(s\"Arguments:\\n$&#123;childArgs.mkString(\"\\n\")&#125;\") printStream.println(s\"System properties:\\n$&#123;sysProps.mkString(\"\\n\")&#125;\") printStream.println(s\"Classpath elements:\\n$&#123;childClasspath.mkString(\"\\n\")&#125;\") printStream.println(\"\\n\") &#125; // scalastyle:on println // 下面这些操作是指定当前运行线程的ClassLoader val loader = if (sysProps.getOrElse(\"spark.driver.userClassPathFirst\", \"false\").toBoolean) &#123; new ChildFirstURLClassLoader(new Array[URL](0), Thread.currentThread.getContextClassLoader) &#125; else &#123; new MutableURLClassLoader(new Array[URL](0), Thread.currentThread.getContextClassLoader) &#125; Thread.currentThread.setContextClassLoader(loader) // 添加jar依赖 for (jar &lt;- childClasspath) &#123; addJarToClasspath(jar, loader) &#125; // 系统属性 for ((key, value) &lt;- sysProps) &#123; System.setProperty(key, value) &#125; var mainClass: Class[_] = null // 通过反射的方式获得mainClass(child main class) try &#123; mainClass = Utils.classForName(childMainClass) &#125; catch &#123; case e: ClassNotFoundException =&gt; e.printStackTrace(printStream) if (childMainClass.contains(\"thriftserver\")) &#123; // scalastyle:off println printStream.println(s\"Failed to load main class $childMainClass.\") printStream.println(\"You need to build Spark with -Phive and -Phive-thriftserver.\") // scalastyle:on println &#125; System.exit(CLASS_NOT_FOUND_EXIT_STATUS) case e: NoClassDefFoundError =&gt; e.printStackTrace(printStream) if (e.getMessage.contains(\"org/apache/hadoop/hive\")) &#123; // scalastyle:off println printStream.println(s\"Failed to load hive class.\") printStream.println(\"You need to build Spark with -Phive and -Phive-thriftserver.\") // scalastyle:on println &#125; System.exit(CLASS_NOT_FOUND_EXIT_STATUS) &#125; // SPARK-4170 if (classOf[scala.App].isAssignableFrom(mainClass)) &#123; printWarning(\"Subclasses of scala.App may not work correctly. Use a main() method instead.\") &#125; // 获得mainClass(child main class)的main方法 val mainMethod = mainClass.getMethod(\"main\", new Array[String](0).getClass) // main方法必须是static级别的 if (!Modifier.isStatic(mainMethod.getModifiers)) &#123; throw new IllegalStateException(\"The main method in the given main class must be static\") &#125; def findCause(t: Throwable): Throwable = t match &#123; case e: UndeclaredThrowableException =&gt; if (e.getCause() != null) findCause(e.getCause()) else e case e: InvocationTargetException =&gt; if (e.getCause() != null) findCause(e.getCause()) else e case e: Throwable =&gt; e &#125; // 最后调用main方法 try &#123; mainMethod.invoke(null, childArgs.toArray) &#125; catch &#123; case t: Throwable =&gt; findCause(t) match &#123; case SparkUserAppException(exitCode) =&gt; System.exit(exitCode) case t: Throwable =&gt; throw t &#125; &#125;&#125; 走到这里，如果是用户通过spark-submit提交自己编写的spark application，那么就直接调用main方法，然后一步一步执行用户编写的代码:SparkContext等等，我们会在以后的文章中进行分析，所以我们现在要跟随的就是org.apache.spark.repl.Main中的main方法，这里我们贴出SparkSubmit进程中主线程的thread dump： 12345678910111213141516171819202122232425262728293031java.io.FileInputStream.read0(Native Method)java.io.FileInputStream.read(FileInputStream.java:207)scala.tools.jline.TerminalSupport.readCharacter(TerminalSupport.java:152)scala.tools.jline.UnixTerminal.readVirtualKey(UnixTerminal.java:125)scala.tools.jline.console.ConsoleReader.readVirtualKey(ConsoleReader.java:933)scala.tools.jline.console.ConsoleReader.readBinding(ConsoleReader.java:1136)scala.tools.jline.console.ConsoleReader.readLine(ConsoleReader.java:1218)scala.tools.jline.console.ConsoleReader.readLine(ConsoleReader.java:1170)org.apache.spark.repl.SparkJLineReader.readOneLine(SparkJLineReader.scala:80)scala.tools.nsc.interpreter.InteractiveReader$class.readLine(InteractiveReader.scala:43) org.apache.spark.repl.SparkJLineReader.readLine(SparkJLineReader.scala:25) org.apache.spark.repl.SparkILoop.readOneLine$1(SparkILoop.scala:648) org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665) org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059) org.apache.spark.repl.Main$.main(Main.scala:31)org.apache.spark.repl.Main.main(Main.scala) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 在这个时候贴出来就是为了承上启下，我们可以清楚的看见(注意是从最后一行往上看)前面我们分析的过程，从SparkSubmit的main方法到submit、doRunMain、runMain到最后通过反射的方式调用org.apache.spark.repl.Main的main方法，整个流程都看的很清楚，所以下面我们进入org.apache.spark.repl.Main的main方法(包含了初始化的操作)： 1234567891011121314151617181920212223242526272829303132// 实例化SparkConfval conf = new SparkConf()// 设置各种文件路径val tmp = System.getProperty(\"java.io.tmpdir\")val rootDir = conf.get(\"spark.repl.classdir\", tmp)val outputDir = Utils.createTempDir(rootDir)val s = new Settings()s.processArguments(List(\"-Yrepl-class-based\", \"-Yrepl-outdir\", s\"$&#123;outputDir.getAbsolutePath&#125;\", \"-classpath\", getAddedJars.mkString(File.pathSeparator)), true)// the creation of SecurityManager has to be lazy so SPARK_YARN_MODE is set if neededval classServerPort = conf.getInt(\"spark.replClassServer.port\", 0)// 实例化了HttpServer，注意这里是lazy级别的lazy val classServer = new HttpServer(conf, outputDir, new SecurityManager(conf), classServerPort, \"HTTP class server\")var sparkContext: SparkContext = _var sqlContext: SQLContext = _// 实例化了SparkILoop，接下来会详细的分析var interp = new SparkILoop // this is a public var because tests reset it.// 执行一些初始化的处理后就执行main方法def main(args: Array[String]) &#123; // 判断是否为yarn的模式，我们在以后的文章中会专门的分析yarn的部署模式 if (getMaster == \"yarn-client\") System.setProperty(\"SPARK_YARN_MODE\", \"true\") // Start the classServer and store its URI in a spark system property // (which will be passed to executors so that they can connect to it) // 启动HTTP server classServer.start() // 最关键的代码，让解释器循环执行，即REPL interp.process(s) // Repl starts and goes in loop of R.E.P.L classServer.stop() Option(sparkContext).map(_.stop)&#125; 写到这里我们再来贴出通过spark-shell进入REPL时打印的部分日志： 1234567817/02/21 13:40:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable17/02/21 13:40:17 INFO spark.SecurityManager: Changing view acls to: root17/02/21 13:40:17 INFO spark.SecurityManager: Changing modify acls to: root17/02/21 13:40:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)17/02/21 13:40:18 INFO spark.HttpServer: Starting HTTP Server17/02/21 13:40:18 INFO server.Server: jetty-8.y.z-SNAPSHOT17/02/21 13:40:18 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:4377317/02/21 13:40:18 INFO util.Utils: Successfully started service 'HTTP class server' on port 43773. 上面这段日志其实对应的就是classServer.start()的部分，以后我们再看到这些日志的时候就知道背后到底发生了什么，是不是很有成就感？ 下面就进入SparkILoop和ILoop的部分(SparkILoop是继承自ILoop类，而SparkILoop中没有process方法，所以调用的实际上是ILoop类中的process方法)： ILoop 12345678910111213141516171819202122232425// 启动解释器，用来解释用户输入的command// start an interpreter with the given settingsdef process(settings: Settings): Boolean = savingContextLoader &#123; this.settings = settings // 创建解释器，内部其实是实例化了一个ILoopInterpreter createInterpreter() // sets in to some kind of reader depending on environmental cues in = in0.fold(chooseReader(settings))(r =&gt; SimpleReader(r, out, interactive = true)) globalFuture = future &#123; intp.initializeSynchronous() loopPostInit() !intp.reporter.hasErrors &#125; // 这里应该调用的是其子类SparkILoop的loadFiles方法，而SparkILoop的loadFiles方法内部最后又会调用这里的loadFiles方法 loadFiles(settings) printWelcome() // 一直循环接收用户输入的command try loop() match &#123; case LineResults.EOF =&gt; out print Properties.shellInterruptedString case _ =&gt; &#125; catch AbstractOrMissingHandler() finally closeInterpreter() true&#125; 我们先来看一下SparkILoop的loadFiles方法都做了什么： 1234override def loadFiles(settings: Settings): Unit = &#123; initializeSpark() super.loadFiles(settings)&#125; 可以看到首先调用initializeSpark()方法，然后调用父类的loadFiles方法，目的就是先准备好SparkContext、SQLContext然后再执行后面的操作，方便我们在进入到REPL后直接可以访问sc、sqlContext等，所以我们现在明白了为什么我们可以直接在spark-shell中直接访问sc、sqlContext了(成就感爆棚有木有？)。说了这么多，我们看一下initializeSpark()的庐山真面目： 12345678910111213141516171819202122def initializeSpark() &#123; intp.beQuietDuring &#123; processLine(\"\"\" @transient val sc = &#123; val _sc = org.apache.spark.repl.Main.createSparkContext() println(\"Spark context available as sc.\") _sc &#125; \"\"\") processLine(\"\"\" @transient val sqlContext = &#123; val _sqlContext = org.apache.spark.repl.Main.createSQLContext() println(\"SQL context available as sqlContext.\") _sqlContext &#125; \"\"\") processLine(\"import org.apache.spark.SparkContext._\") processLine(\"import sqlContext.implicits._\") processLine(\"import sqlContext.sql\") processLine(\"import org.apache.spark.sql.functions._\") &#125;&#125; 这里写的就非常清楚了通过processLine来创建SparkContext、SQLContext并导入一些经常使用的包，都准备完成后再调用父类的loadFiles，然后调用printWelcome()，注意这里调用的是SparkILoop的printWelcome()方法： 12345678910111213141516/** Print a welcome message */override def printWelcome() &#123; import org.apache.spark.SPARK_VERSION echo(\"\"\"Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version %s /_/ \"\"\".format(SPARK_VERSION)) val welcomeMsg = \"Using Scala %s (%s, Java %s)\".format( versionString, javaVmName, javaVersion) echo(welcomeMsg) echo(\"Type in expressions to have them evaluated.\") echo(\"Type :help for more information.\")&#125; 咦？这货看着是不是很眼熟，对，这就是spark-shell中打印的日志： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 1.6.3 /_/Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111)Type in expressions to have them evaluated.Type :help for more information.17/02/21 13:40:24 INFO spark.SparkContext: Running Spark version 1.6.317/02/21 13:40:24 INFO spark.SecurityManager: Changing view acls to: root17/02/21 13:40:24 INFO spark.SecurityManager: Changing modify acls to: root17/02/21 13:40:24 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)17/02/21 13:40:25 INFO util.Utils: Successfully started service 'sparkDriver' on port 38463.17/02/21 13:40:26 INFO slf4j.Slf4jLogger: Slf4jLogger started17/02/21 13:40:26 INFO Remoting: Starting remoting17/02/21 13:40:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:37221]17/02/21 13:40:26 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 37221.17/02/21 13:40:26 INFO spark.SparkEnv: Registering MapOutputTracker17/02/21 13:40:26 INFO spark.SparkEnv: Registering BlockManagerMaster17/02/21 13:40:26 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-a06685a8-6f1c-4e8f-805c-e232333f8d8517/02/21 13:40:26 INFO storage.MemoryStore: MemoryStore started with capacity 511.1 MB17/02/21 13:40:27 INFO spark.SparkEnv: Registering OutputCommitCoordinator17/02/21 13:40:27 INFO server.Server: jetty-8.y.z-SNAPSHOT17/02/21 13:40:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:404017/02/21 13:40:27 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.17/02/21 13:40:27 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:404017/02/21 13:40:27 INFO client.AppClient$ClientEndpoint: Connecting to master spark://master:7077...17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20170221134027-000017/02/21 13:40:28 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44615.17/02/21 13:40:28 INFO netty.NettyBlockTransferService: Server created on 4461517/02/21 13:40:28 INFO storage.BlockManagerMaster: Trying to register BlockManager17/02/21 13:40:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:44615 with 511.1 MB RAM, BlockManagerId(driver, 172.17.0.2, 44615)17/02/21 13:40:28 INFO storage.BlockManagerMaster: Registered BlockManager17/02/21 13:40:28 INFO client.AppClient$ClientEndpoint: Executor added: app-20170221134027-0000/0 on worker-20170221133811-172.17.0.3-41829 (172.17.0.3:41829) with 2 cores17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20170221134027-0000/0 on hostPort 172.17.0.3:41829 with 2 cores, 1024.0 MB RAM17/02/21 13:40:28 INFO client.AppClient$ClientEndpoint: Executor added: app-20170221134027-0000/1 on worker-20170221133810-172.17.0.4-39901 (172.17.0.4:39901) with 2 cores17/02/21 13:40:28 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20170221134027-0000/1 on hostPort 172.17.0.4:39901 with 2 cores, 1024.0 MB RAM17/02/21 13:40:29 INFO client.AppClient$ClientEndpoint: Executor updated: app-20170221134027-0000/1 is now RUNNING17/02/21 13:40:29 INFO client.AppClient$ClientEndpoint: Executor updated: app-20170221134027-0000/0 is now RUNNING17/02/21 13:40:45 INFO scheduler.EventLoggingListener: Logging events to hdfs://master:9000/historyserverforspark/app-20170221134027-000017/02/21 13:40:45 INFO cluster.SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.017/02/21 13:40:45 INFO repl.SparkILoop: Created spark context..Spark context available as sc.17/02/21 13:40:46 INFO cluster.SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (worker1:60096) with ID 017/02/21 13:40:46 INFO cluster.SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (worker2:46846) with ID 117/02/21 13:40:47 INFO storage.BlockManagerMasterEndpoint: Registering block manager worker1:39275 with 511.1 MB RAM, BlockManagerId(0, worker1, 39275)17/02/21 13:40:47 INFO storage.BlockManagerMasterEndpoint: Registering block manager worker2:37449 with 511.1 MB RAM, BlockManagerId(1, worker2, 37449)17/02/21 13:40:50 INFO hive.HiveContext: Initializing execution hive, version 1.2.117/02/21 13:40:51 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.017/02/21 13:40:51 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.017/02/21 13:40:52 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore17/02/21 13:40:53 INFO metastore.ObjectStore: ObjectStore, initialize called17/02/21 13:40:53 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored17/02/21 13:40:53 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored17/02/21 13:40:54 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)17/02/21 13:40:55 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)17/02/21 13:41:01 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"17/02/21 13:41:08 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:08 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:15 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:15 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:17 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY17/02/21 13:41:17 INFO metastore.ObjectStore: Initialized ObjectStore17/02/21 13:41:18 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.017/02/21 13:41:18 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException17/02/21 13:41:19 INFO metastore.HiveMetaStore: Added admin role in metastore17/02/21 13:41:19 INFO metastore.HiveMetaStore: Added public role in metastore17/02/21 13:41:19 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty17/02/21 13:41:20 INFO metastore.HiveMetaStore: 0: get_all_databases17/02/21 13:41:20 INFO HiveMetaStore.audit: ugi=root ip=unknown-ip-addr cmd=get_all_databases 17/02/21 13:41:20 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*17/02/21 13:41:20 INFO HiveMetaStore.audit: ugi=root ip=unknown-ip-addr cmd=get_functions: db=default pat=* 17/02/21 13:41:20 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MResourceUri\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:21 INFO session.SessionState: Created local directory: /tmp/939dedb5-f724-461b-a41a-a5fd1fe7324b_resources17/02/21 13:41:21 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/939dedb5-f724-461b-a41a-a5fd1fe7324b17/02/21 13:41:21 INFO session.SessionState: Created local directory: /tmp/root/939dedb5-f724-461b-a41a-a5fd1fe7324b17/02/21 13:41:21 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/939dedb5-f724-461b-a41a-a5fd1fe7324b/_tmp_space.db17/02/21 13:41:22 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse17/02/21 13:41:22 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.17/02/21 13:41:22 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.017/02/21 13:41:22 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.017/02/21 13:41:23 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore17/02/21 13:41:23 INFO metastore.ObjectStore: ObjectStore, initialize called17/02/21 13:41:23 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored17/02/21 13:41:23 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored17/02/21 13:41:24 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)17/02/21 13:41:24 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)17/02/21 13:41:25 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"17/02/21 13:41:29 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:29 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:29 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:29 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:30 INFO DataNucleus.Query: Reading in results for query \"org.datanucleus.store.rdbms.query.SQLQuery@0\" since the connection used is closing17/02/21 13:41:30 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY17/02/21 13:41:30 INFO metastore.ObjectStore: Initialized ObjectStore17/02/21 13:41:30 INFO metastore.HiveMetaStore: Added admin role in metastore17/02/21 13:41:30 INFO metastore.HiveMetaStore: Added public role in metastore17/02/21 13:41:30 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty17/02/21 13:41:30 INFO metastore.HiveMetaStore: 0: get_all_databases17/02/21 13:41:30 INFO HiveMetaStore.audit: ugi=root ip=unknown-ip-addr cmd=get_all_databases 17/02/21 13:41:30 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*17/02/21 13:41:30 INFO HiveMetaStore.audit: ugi=root ip=unknown-ip-addr cmd=get_functions: db=default pat=* 17/02/21 13:41:30 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MResourceUri\" is tagged as \"embedded-only\" so does not have its own datastore table.17/02/21 13:41:30 INFO session.SessionState: Created local directory: /tmp/c9c26571-1229-4786-8a8e-d7b090b07d85_resources17/02/21 13:41:30 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/c9c26571-1229-4786-8a8e-d7b090b07d8517/02/21 13:41:30 INFO session.SessionState: Created local directory: /tmp/root/c9c26571-1229-4786-8a8e-d7b090b07d8517/02/21 13:41:30 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/c9c26571-1229-4786-8a8e-d7b090b07d85/_tmp_space.db17/02/21 13:41:30 INFO repl.SparkILoop: Created sql context (with Hive support)..SQL context available as sqlContext. Welcome后面的一大串就是上面initializeSpark()执行打的日志信息，现在所有的日志信息都“名花有主”了，我们会单独拿出文章来分析SparkContext、SQLContext的创建流程，下面我们看process方法中最后就一直进行loop操作，这里我们不再深入的分析下去了，我们要适可而止，否则会迷失在源码中，大家可以简单的理解其实这里的循环过程就是REPL所代表的意思，即Read：读取用户输入的command；Evaluation：通过Spark Framework执行command；P：print打计算结果；L：loop循环前面的流程，同时在读取command后需要进行语法解析，然后用解释器执行，有兴趣的朋友可以继续跟随源码走下去。 至此我们走完了整个spark-shell(包括spark-submit)的整个流程，下面用一张图简单的总结一下： 本文参考和拓展阅读： Spark-1.6.3源码 Spark-2.1.0源码 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"spark","slug":"spark","permalink":"http://www.sun4lower.cn/tags/spark/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"},{"name":"spark-shell","slug":"spark-shell","permalink":"http://www.sun4lower.cn/tags/spark-shell/"},{"name":"spark-submit","slug":"spark-submit","permalink":"http://www.sun4lower.cn/tags/spark-submit/"},{"name":"SparkContext","slug":"SparkContext","permalink":"http://www.sun4lower.cn/tags/SparkContext/"},{"name":"SqlContext","slug":"SqlContext","permalink":"http://www.sun4lower.cn/tags/SqlContext/"},{"name":"spark-class","slug":"spark-class","permalink":"http://www.sun4lower.cn/tags/spark-class/"},{"name":"REPL","slug":"REPL","permalink":"http://www.sun4lower.cn/tags/REPL/"}]},{"title":"Spark-Core源码精读(2)、Master中的schedule详解","slug":"sc-schedule","date":"2017-02-28T08:17:02.000Z","updated":"2017-02-28T13:34:25.000Z","comments":true,"path":"2017/02/28/sc-schedule/","link":"","permalink":"http://www.sun4lower.cn/2017/02/28/sc-schedule/","excerpt":"","text":"上一篇博客详细分析了Spark在Standalone模式下的部署过程，文中提到在Worker注册完成后需要执行一个schedule操作来分配资源，本文就将具体分析此方法具体是怎样分配资源的。 注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。 什么时候会调用schedule？其实每当一个新的application加入活着资源发生变化的时候都会调用schudule方法对资源进行重新分配，那么它是如何分配资源的呢？我们下面进行源码级别的分析。 schedule我们先贴出schedule的源码： 123456789101112131415161718192021222324252627282930313233343536// 既然要分配资源就必须保证Master的当前状态为ALIVEif (state != RecoveryState.ALIVE) &#123; return&#125;// Drivers take strict precedence over executors// 注释说的很明确，先注册Drivers然后再注册executors// 1. 首先将ALIVE状态的Workers使用shuffle的方式打乱，以免每次都将Driver分配到同一个Worker上val shuffledAliveWorkers = Random.shuffle(workers.toSeq.filter(_.state == WorkerState.ALIVE))val numWorkersAlive = shuffledAliveWorkers.sizevar curPos = 0// 2. 循环遍历启动Driversfor (driver &lt;- waitingDrivers.toList) &#123; // iterate over a copy of waitingDrivers // We assign workers to each waiting driver in a round-robin fashion. For each driver, we // start from the last worker that was assigned a driver, and continue onwards until we have // explored all alive workers. var launched = false var numWorkersVisited = 0 // 2.1 判断是否有剩余的没有分配的Workers，并且尚未启动 while (numWorkersVisited &lt; numWorkersAlive &amp;&amp; !launched) &#123; // 2.2 获取一个Worker，第一个的索引为0，后面的索引根据curPos = (curPos + 1) % numWorkersAlive进行计算 val worker = shuffledAliveWorkers(curPos) // 2.3 标记分配过的Worker加1 numWorkersVisited += 1 // 2.4 判断当前的Worker的内存和cpu是否满足Driver的需求 if (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123; // 2.5 如果满足资源的需求就在当前Worker上启动Driver launchDriver(worker, driver) // 2.6 启动完成后从等待的队列中删除，并将launched标记为true waitingDrivers -= driver launched = true &#125; curPos = (curPos + 1) % numWorkersAlive &#125;&#125;// 3 启动ExecutorsstartExecutorsOnWorkers() 启动Driver我已经在上面的源码中对分配的流程进行了详细的注释，现在我们看一下launchDriver方法： 123456789101112private def launchDriver(worker: WorkerInfo, driver: DriverInfo) &#123; // 1. 打日志 logInfo(\"Launching driver \" + driver.id + \" on worker \" + worker.id) // 2. 向worker中添加driver的信息，包括增加已经使用的内存和cpu信息 worker.addDriver(driver) // 3. 向driver中添加该worker的引用 driver.worker = Some(worker) // 4. 向Worker发送LaunchDriver的消息，通知Worker启动Driver worker.endpoint.send(LaunchDriver(driver.id, driver.desc)) // 5. 将driver的状态变成RUNNING driver.state = DriverState.RUNNING&#125; 接下来我们看一下对应的Worker在接收到LaunchDriver消息后是怎么处理的： 12345678910111213141516171819202122case LaunchDriver(driverId, driverDesc) =&gt; &#123; // 1. 打日志 logInfo(s\"Asked to launch driver $driverId\") // 2. 实例化DriverRunner val driver = new DriverRunner( conf, driverId, workDir, sparkHome, driverDesc.copy(command = Worker.maybeUpdateSSLSettings(driverDesc.command, conf)), self, workerUri, securityMgr) // 3. 实例化完成后向drivers中添加该driver的记录 drivers(driverId) = driver // 4. 启动driver driver.start() // 5. 启动完成后记录资源的变化 coresUsed += driverDesc.cores memoryUsed += driverDesc.mem&#125; 继续跟踪driver.start()： 12345678910111213141516171819202122232425262728293031323334353637383940// 英文注释说的很清楚：启动一个线程来运行和管理driver/** Starts a thread to run and manage the driver. */private[worker] def start() = &#123; new Thread(\"DriverRunner for \" + driverId) &#123; override def run() &#123; try &#123; // 创建driver的工作目录 val driverDir = createWorkingDirectory() // 下载用户的Jar文件到driver的工作目录并返回路径名称 val localJarFilename = downloadUserJar(driverDir) def substituteVariables(argument: String): String = argument match &#123; case \"&#123;&#123;WORKER_URL&#125;&#125;\" =&gt; workerUrl case \"&#123;&#123;USER_JAR&#125;&#125;\" =&gt; localJarFilename case other =&gt; other &#125; // TODO: If we add ability to submit multiple jars they should also be added here val builder = CommandUtils.buildProcessBuilder(driverDesc.command, securityManager, driverDesc.mem, sparkHome.getAbsolutePath, substituteVariables) // 具体的启动Driver的操作，这里不再详细分析 launchDriver(builder, driverDir, driverDesc.supervise) &#125; catch &#123; case e: Exception =&gt; finalException = Some(e) &#125; val state = if (killed) &#123; DriverState.KILLED &#125; else if (finalException.isDefined) &#123; DriverState.ERROR &#125; else &#123; finalExitCode match &#123; case Some(0) =&gt; DriverState.FINISHED case _ =&gt; DriverState.FAILED &#125; &#125; finalState = Some(state) worker.send(DriverStateChanged(driverId, state, finalException)) &#125; &#125;.start()&#125; 如果启动成功最后要向worker发送一条DriverStateChanged的消息，而Worker在接收到该消息后会调用handleDriverStateChanged方法进行一系列处理，具体的处理细节就不再说明，主要的就是向Master发送一条driverStateChanged的消息，Master在接收到该消息后移除Driver的信息： 12345678ase DriverStateChanged(driverId, state, exception) =&gt; &#123; state match &#123; case DriverState.ERROR | DriverState.FINISHED | DriverState.KILLED | DriverState.FAILED =&gt; removeDriver(driverId, state, exception) case _ =&gt; throw new Exception(s\"Received unexpected state update for driver $driverId: $state\") &#125;&#125; 至此向Driver分配资源并启动Driver的过程结束，下面我们看一下启动Executors即执行startExecutorsOnWorkers()的流程。 启动ExecutorsstartExecutorsOnWorkers(): 123456789101112131415161718192021222324252627/** * Schedule and launch executors on workers */ private def startExecutorsOnWorkers(): Unit = &#123; // 采用的是先进先出的原则 // Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app // in the queue, then the second app, etc. for (app &lt;- waitingApps if app.coresLeft &gt; 0) &#123; val coresPerExecutor: Option[Int] = app.desc.coresPerExecutor // Filter out workers that don't have enough resources to launch an executor // 过滤出ALIVE状态并且资源满足要求的workers，同时按照空闲cpu cores的个数倒序排列 val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE) .filter(worker =&gt; worker.memoryFree &gt;= app.desc.memoryPerExecutorMB &amp;&amp; worker.coresFree &gt;= coresPerExecutor.getOrElse(1)) .sortBy(_.coresFree).reverse // 决定在每个worker上面分配多少个cpu cores val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps) // 然后开始进行分配 // Now that we've decided how many cores to allocate on each worker, let's allocate them for (pos &lt;- 0 until usableWorkers.length if assignedCores(pos) &gt; 0) &#123; allocateWorkerResourceToExecutors( app, assignedCores(pos), coresPerExecutor, usableWorkers(pos)) &#125; &#125; &#125; 我们首先看一下是如何决定在每个worker上分配多少个cores的，这里我们只列出scheduleExecutorsOnWorkers方法的英文注释，并进行说明，具体的操作大家可以去看源码： 1234567891011121314151617181920/** * Schedule executors to be launched on the workers. * Returns an array containing number of cores assigned to each worker. * * There are two modes of launching executors. The first attempts to spread out an application's * executors on as many workers as possible, while the second does the opposite (i.e. launch them * on as few workers as possible). The former is usually better for data locality purposes and is * the default. * * The number of cores assigned to each executor is configurable. When this is explicitly set, * multiple executors from the same application may be launched on the same worker if the worker * has enough cores and memory. Otherwise, each executor grabs all the cores available on the * worker by default, in which case only one executor may be launched on each worker. * * It is important to allocate coresPerExecutor on each worker at a time (instead of 1 core * at a time). Consider the following example: cluster has 4 workers with 16 cores each. * User requests 3 executors (spark.cores.max = 48, spark.executor.cores = 16). If 1 core is * allocated at a time, 12 cores from each worker would be assigned to each executor. * Since 12 &lt; 16, no executors would launch [SPARK-8881]. */ 大致意思是说有两种分配模型，第一种是将executors分配到尽可能多的workers上；第二种与第一种相反。默认使用的是第一种模型，这种模型更加符合数据的本地性原则，为每个Executor分配的cores的个数是可以进行配置的（spark-submit 或者 spark-env.sh），如果设置了，多个executors可能会被分配在一个worker上（前提是该worker拥有足够的cores和memory），否则每个executor会充分利用worker上的cores，这种情况下一个executor会被分配在一个worker上。具体在集群上分配cores的时候会尽可能的满足我们的要求，如果需要的cores的个数大于workers中空闲的cores的个数，那么就先分配空闲的cores，尽可能的去满足要求。 接下来就是具体为executors分配计算资源并启动executors的过程： 123456789101112131415161718private def allocateWorkerResourceToExecutors( app: ApplicationInfo, assignedCores: Int, coresPerExecutor: Option[Int], worker: WorkerInfo): Unit = &#123; // If the number of cores per executor is specified, we divide the cores assigned // to this worker evenly among the executors with no remainder. // Otherwise, we launch a single executor that grabs all the assignedCores on this worker. val numExecutors = coresPerExecutor.map &#123; assignedCores / _ &#125;.getOrElse(1) val coresToAssign = coresPerExecutor.getOrElse(assignedCores) for (i &lt;- 1 to numExecutors) &#123; // 向application中添加executor的信息 val exec = app.addExecutor(worker, coresToAssign) // 启动executors launchExecutor(worker, exec) app.state = ApplicationState.RUNNING &#125; &#125; 启动executors： 12345678910private def launchExecutor(worker: WorkerInfo, exec: ExecutorDesc): Unit = &#123; logInfo(\"Launching executor \" + exec.fullId + \" on worker \" + worker.id) worker.addExecutor(exec) // 向worker发消息启动executor worker.endpoint.send(LaunchExecutor(masterUrl, exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory)) // 然后向driver发送executors的信息 exec.application.driver.send( ExecutorAdded(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory)) &#125; worker在接收到启动executor的消息后执行具体的启动操作，并向Master汇报。 然后也要向driver发送executors的资源信息，driver收到信息后执行application，至此分配并启动executors的大致流程也就执行完毕。 最后用一张图总结一下启动Driver和Worker的简易流程： 本文只是大致的分析了Master在执行schedule的时候具体为Driver、Executors分配资源并启动它们的流程，以后我们还会分析整个application的运行流程，那时我们会具体进行分析。 本文参考和拓展阅读： Spark-1.6.3源码 Spark-2.1.0源码 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"spark","slug":"spark","permalink":"http://www.sun4lower.cn/tags/spark/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"},{"name":"master","slug":"master","permalink":"http://www.sun4lower.cn/tags/master/"},{"name":"driver","slug":"driver","permalink":"http://www.sun4lower.cn/tags/driver/"},{"name":"executor","slug":"executor","permalink":"http://www.sun4lower.cn/tags/executor/"},{"name":"schedule","slug":"schedule","permalink":"http://www.sun4lower.cn/tags/schedule/"}]},{"title":"Spark-Core源码精读(1)、Spark Deployment & start-all.sh on Standalone mode","slug":"sc-deploy","date":"2017-02-25T05:58:04.000Z","updated":"2017-02-25T14:44:34.000Z","comments":true,"path":"2017/02/25/sc-deploy/","link":"","permalink":"http://www.sun4lower.cn/2017/02/25/sc-deploy/","excerpt":"","text":"本文为精度Spark-core的源码的第一节，主要内容包括Spark Deployment的简介和Standalone模式下启动集群的详细流程精读。 注：本专题的文章皆使用Spark-1.6.3版本的源码为参考，如果Spark-2.1.0版本有重大改进的地方也会进行说明。 Spark DeploymentSpark 的部署主要有三种方式：local、standalone、yarn、mesos 其中local和standalone模式主要用于测试学习，实际生产环境下国内一般都是使用yarn，这是历史原因造成的（考虑到集群中同时有Hadoop）；而国外一般都是使用mesos，而且个人认为mesos也是一种趋势，关于yarn和mesos的部分，以后会单独进行分析，下面我们详细解读standalone模式下的集群启动的具体流程。 Standalone mode下集群启动源码精读我们就从start-all.sh开始，主要代码如下： 12345678# Load the Spark configuration. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"# Start Master\"$&#123;SPARK_HOME&#125;/sbin\"/start-master.sh $TACHYON_STR# Start Workers\"$&#123;SPARK_HOME&#125;/sbin\"/start-slaves.sh $TACHYON_STR 注释说的很明确了，我们继续追踪start-master.sh 123456CLASS=\"org.apache.spark.deploy.master.Master\"...\"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start $CLASS 1 \\ --ip $SPARK_MASTER_IP --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \\ $ORIGINAL_ARGS... 可以看出，是执行了spark-daemon.sh的start方法，即通过动态加载的方式将org.apache.spark.deploy.master.Master作为一个daemon（守护线程）来运行，所以我们直接分析Master的源码： 12345678910111213141516171819202122232425262728293031323334private[deploy] object Master extends Logging &#123; val SYSTEM_NAME = \"sparkMaster\" val ENDPOINT_NAME = \"Master\" def main(argStrings: Array[String]) &#123; //注册log SignalLogger.register(log) //实例化SparkConf，会加载`spark.*`格式的配置信息 val conf = new SparkConf //使用MasterArguments对传入的参数argStrings和默认加载的conf进行封装，并执行一些初始化操作 val args = new MasterArguments(argStrings, conf) val (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf) rpcEnv.awaitTermination() &#125; /** * Start the Master and return a three tuple of: * (1) The Master RpcEnv * (2) The web UI bound port * (3) The REST server bound port, if any */ def startRpcEnvAndEndpoint( host: String, port: Int, webUiPort: Int, conf: SparkConf): (RpcEnv, Int, Option[Int]) = &#123; val securityMgr = new SecurityManager(conf) val rpcEnv = RpcEnv.create(SYSTEM_NAME, host, port, conf, securityMgr) val masterEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME, new Master(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf)) val portsResponse = masterEndpoint.askWithRetry[BoundPortsResponse](BoundPortsRequest) (rpcEnv, portsResponse.webUIPort, portsResponse.restPort) &#125;&#125; 首先注册log，实例化SparkConf并加载spark.*格式的配置信息，然后使用MasterArguments对传入的参数argStrings和默认加载的conf进行封装，并执行一些初始化操作，主要是加载配置信息，这里不做详细说明，我们接着往下看。 下面才是真正意义上的Start Master，startRpcEnvAndEndpoint函数中首先实例化了SecurityManager（Spark中负责安全的类），然后创建了RpcEnv（Spark的Rpc通信有三个抽象：RpcEnv、RpcEndpoint、RpcEndpointRef，这样做屏蔽了底层的实现，方便用户进行扩展，Spark-1.6.3底层的默认实现方式是Netty，而Spark-2.x已经将Akka的依赖移除），接着实例化Master，实际上就是实例化了一个RpcEndpoint（因为Master实现了ThreadSafeRpcEndpoint接口，而ThreadSafeRpcEndpoint又继承了RpcEndpoint），实例化完成后通过RpcEnv的setupEndpoint向RpcEnv进行注册，注册的时候执行了Master的onStart方法，最后返回了一个RpcEndpointRef（实际上是NettyRpcEndpointRef），通过获得的RpcEndpointRef向Master（Endpoint）发送了一条BoundPortsRequest消息，Master通过receiveAndReply方法接受到该消息（实际上是通过NettyRpcEnv中的Dispatcher进行消息的分配），模式匹配到是BoundPortsRequest类型的消息，然后执行reply方法进行回复，源码如下：123case BoundPortsRequest =&gt; &#123; context.reply(BoundPortsResponse(address.port, webUi.boundPort, restServerBoundPort)) &#125; 至此Master启动完成，Rpc部分可以参考另一篇文章：Spark RPC 到底是个什么鬼？，下面贴出Master实例化部分和onStart方法的源码及中文注释： Master实例化部分： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899//默认的情况下，取消的task不会从工作的队列中移除直到延迟时间完成，所以创建一个守护线程来“手动”移除它private val forwardMessageThread = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"master-forward-message-thread\")//用于执行重建UI代码的守护线程private val rebuildUIThread = ThreadUtils.newDaemonSingleThreadExecutor(\"master-rebuild-ui-thread\") //通过rebuildUIThread获得重建UI的执行上下文private val rebuildUIContext = ExecutionContext.fromExecutor(rebuildUIThread)//获取hadoop的配置文件private val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)//时间格式，用于构建application IDprivate def createDateFormat = new SimpleDateFormat(\"yyyyMMddHHmmss\") // For application IDs//如果Master在60s内没有收到Worker发送的heartbeat信息就认为这个Worker timeoutprivate val WORKER_TIMEOUT_MS = conf.getLong(\"spark.worker.timeout\", 60) * 1000//webUI中显示的完成的application的最大个数，超过200个就移除掉(200/10,1)=20个完成的applicationsprivate val RETAINED_APPLICATIONS = conf.getInt(\"spark.deploy.retainedApplications\", 200)//webUI中显示的完成的drivers的最大个数，超过200个就移除掉(200/10,1)=20个完成的driversprivate val RETAINED_DRIVERS = conf.getInt(\"spark.deploy.retainedDrivers\", 200)//如果Master在(REAPER_ITERATIONS + 1) * WORKER_TIMEOUT_MS)秒内仍然没有收到Worker发送的heartbeat信息，就删除这个Workerprivate val REAPER_ITERATIONS = conf.getInt(\"spark.dead.worker.persistence\", 15)//recoveryMode：NONE、ZOOKEEPER、FILESYSTEM、CUSTOM，默认是NONEprivate val RECOVERY_MODE = conf.get(\"spark.deploy.recoveryMode\", \"NONE\")//Executor失败的最大重试次数private val MAX_EXECUTOR_RETRIES = conf.getInt(\"spark.deploy.maxExecutorRetries\", 10)//下面是各种“数据结构”，不再一一说明val workers = new HashSet[WorkerInfo]val idToApp = new HashMap[String, ApplicationInfo]val waitingApps = new ArrayBuffer[ApplicationInfo]val apps = new HashSet[ApplicationInfo]private val idToWorker = new HashMap[String, WorkerInfo]private val addressToWorker = new HashMap[RpcAddress, WorkerInfo]private val endpointToApp = new HashMap[RpcEndpointRef, ApplicationInfo]private val addressToApp = new HashMap[RpcAddress, ApplicationInfo]private val completedApps = new ArrayBuffer[ApplicationInfo]private var nextAppNumber = 0// Using ConcurrentHashMap so that master-rebuild-ui-thread can add a UI after asyncRebuildUIprivate val appIdToUI = new ConcurrentHashMap[String, SparkUI]private val drivers = new HashSet[DriverInfo]private val completedDrivers = new ArrayBuffer[DriverInfo]// Drivers currently spooled for schedulingprivate val waitingDrivers = new ArrayBuffer[DriverInfo]private var nextDriverNumber = 0Utils.checkHost(address.host, \"Expected hostname\")//下面是Metrics系统相关的代码private val masterMetricsSystem = MetricsSystem.createMetricsSystem(\"master\", conf, securityMgr)private val applicationMetricsSystem = MetricsSystem.createMetricsSystem(\"applications\", conf, securityMgr)private val masterSource = new MasterSource(this)// After onStart, webUi will be setprivate var webUi: MasterWebUI = nullprivate val masterPublicAddress = &#123; val envVar = conf.getenv(\"SPARK_PUBLIC_DNS\") if (envVar != null) envVar else address.host&#125;private val masterUrl = address.toSparkURLprivate var masterWebUiUrl: String = _//当前Master的状态：STANDBY, ALIVE, RECOVERING, COMPLETING_RECOVERYprivate var state = RecoveryState.STANDBYprivate var persistenceEngine: PersistenceEngine = _private var leaderElectionAgent: LeaderElectionAgent = _private var recoveryCompletionTask: ScheduledFuture[_] = _private var checkForWorkerTimeOutTask: ScheduledFuture[_] = _// As a temporary workaround before better ways of configuring memory, we allow users to set// a flag that will perform round-robin scheduling across the nodes (spreading out each app// among all the nodes) instead of trying to consolidate each app onto a small # of nodes.// 避免将application的运行限制在固定的几个节点上private val spreadOutApps = conf.getBoolean(\"spark.deploy.spreadOut\", true)// Default maxCores for applications that don't specify it (i.e. pass Int.MaxValue)private val defaultCores = conf.getInt(\"spark.deploy.defaultCores\", Int.MaxValue)if (defaultCores &lt; 1) &#123; throw new SparkException(\"spark.deploy.defaultCores must be positive\")&#125;// Alternative application submission gateway that is stable across Spark versions// 用来接受application提交的restServerprivate val restServerEnabled = conf.getBoolean(\"spark.master.rest.enabled\", true)private var restServer: Option[StandaloneRestServer] = Noneprivate var restServerBoundPort: Option[Int] = None onStart方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859override def onStart(): Unit = &#123; //打日志 logInfo(\"Starting Spark master at \" + masterUrl) logInfo(s\"Running Spark version $&#123;org.apache.spark.SPARK_VERSION&#125;\") //实例化standalone模式下的MasterWebUI并绑定到HTTP Server webUi = new MasterWebUI(this, webUiPort) webUi.bind() //可以通过这个Url地址看到Master的信息 masterWebUiUrl = \"http://\" + masterPublicAddress + \":\" + webUi.boundPort //以固定的时间间隔检查并移除time-out的worker checkForWorkerTimeOutTask = forwardMessageThread.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; self.send(CheckForWorkerTimeOut) &#125; &#125;, 0, WORKER_TIMEOUT_MS, TimeUnit.MILLISECONDS) //实例化并启动restServer用于接受application的提交 if (restServerEnabled) &#123; val port = conf.getInt(\"spark.master.rest.port\", 6066) restServer = Some(new StandaloneRestServer(address.host, port, conf, self, masterUrl)) &#125; restServerBoundPort = restServer.map(_.start()) //启动MetricsSystem masterMetricsSystem.registerSource(masterSource) masterMetricsSystem.start() applicationMetricsSystem.start() // Attach the master and app metrics servlet handler to the web ui after the metrics systems are // started. masterMetricsSystem.getServletHandlers.foreach(webUi.attachHandler) applicationMetricsSystem.getServletHandlers.foreach(webUi.attachHandler) //序列化器 val serializer = new JavaSerializer(conf) //恢复机制，包括持久化引擎和选举机制 val (persistenceEngine_, leaderElectionAgent_) = RECOVERY_MODE match &#123; case \"ZOOKEEPER\" =&gt; logInfo(\"Persisting recovery state to ZooKeeper\") val zkFactory = new ZooKeeperRecoveryModeFactory(conf, serializer) (zkFactory.createPersistenceEngine(), zkFactory.createLeaderElectionAgent(this)) case \"FILESYSTEM\" =&gt; val fsFactory = new FileSystemRecoveryModeFactory(conf, serializer) (fsFactory.createPersistenceEngine(), fsFactory.createLeaderElectionAgent(this)) case \"CUSTOM\" =&gt; val clazz = Utils.classForName(conf.get(\"spark.deploy.recoveryMode.factory\")) val factory = clazz.getConstructor(classOf[SparkConf], classOf[Serializer]) .newInstance(conf, serializer) .asInstanceOf[StandaloneRecoveryModeFactory] (factory.createPersistenceEngine(), factory.createLeaderElectionAgent(this)) case _ =&gt; (new BlackHolePersistenceEngine(), new MonarchyLeaderAgent(this)) &#125; persistenceEngine = persistenceEngine_ leaderElectionAgent = leaderElectionAgent_ &#125; 下面介绍Worker的启动 start-slaves.sh: 12# Launch the slaves\"$&#123;SPARK_HOME&#125;/sbin/slaves.sh\" cd \"$&#123;SPARK_HOME&#125;\" \\; \"$&#123;SPARK_HOME&#125;/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT\" start-slave.sh: 1234CLASS=\"org.apache.spark.deploy.worker.Worker\"... \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start $CLASS $WORKER_NUM \\ --webui-port \"$WEBUI_PORT\" $PORT_FLAG $PORT_NUM $MASTER \"$@\" 和Master的启动类似，我们直接看Worker文件，仍然从main方法开始： 1234567891011121314151617181920212223242526272829def main(argStrings: Array[String]) &#123; SignalLogger.register(log) val conf = new SparkConf val args = new WorkerArguments(argStrings, conf) val rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores, args.memory, args.masters, args.workDir, conf = conf) rpcEnv.awaitTermination() &#125; def startRpcEnvAndEndpoint( host: String, port: Int, webUiPort: Int, cores: Int, memory: Int, masterUrls: Array[String], workDir: String, workerNumber: Option[Int] = None, conf: SparkConf = new SparkConf): RpcEnv = &#123; // The LocalSparkCluster runs multiple local sparkWorkerX RPC Environments val systemName = SYSTEM_NAME + workerNumber.map(_.toString).getOrElse(\"\") val securityMgr = new SecurityManager(conf) val rpcEnv = RpcEnv.create(systemName, host, port, conf, securityMgr) val masterAddresses = masterUrls.map(RpcAddress.fromSparkURL(_)) rpcEnv.setupEndpoint(ENDPOINT_NAME, new Worker(rpcEnv, webUiPort, cores, memory, masterAddresses, systemName, ENDPOINT_NAME, workDir, conf, securityMgr)) rpcEnv &#125; 可以看到前面和Master类似，只不过Worker有可能是多个，所以需要根据workerNumber构造一个systemName，用来创建不同的RpcEnv，然后实例化Worker（即实例化Endpoint），实例化的时候需要传入masterAddresses（注意此处可能有多个Master），以便以后向Master注册，同时由于要向对应的RpcEnv注册，注册的时候同样要执行Worker的onStart方法，我会将Worker实例化和onStart的源码放到后面，这里我们先来看一下Worker向Master注册的代码（onStart方法中调用registerWithMaster）： 12345678910111213141516171819202122private def registerWithMaster() &#123; // onDisconnected may be triggered multiple times, so don't attempt registration // if there are outstanding registration attempts scheduled. registrationRetryTimer match &#123; case None =&gt; registered = false registerMasterFutures = tryRegisterAllMasters() connectionAttemptCount = 0 registrationRetryTimer = Some(forwordMessageScheduler.scheduleAtFixedRate( new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; Option(self).foreach(_.send(ReregisterWithMaster)) &#125; &#125;, INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS, INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS, TimeUnit.SECONDS)) case Some(_) =&gt; logInfo(\"Not spawning another attempt to register with the master, since there is an\" + \" attempt scheduled already.\") &#125; &#125; 可以看到内部调用了tryRegisterAllMasters方法： 1234567891011121314151617private def tryRegisterAllMasters(): Array[JFuture[_]] = &#123; masterRpcAddresses.map &#123; masterAddress =&gt; registerMasterThreadPool.submit(new Runnable &#123; override def run(): Unit = &#123; try &#123; logInfo(\"Connecting to master \" + masterAddress + \"...\") val masterEndpoint = rpcEnv.setupEndpointRef(Master.SYSTEM_NAME, masterAddress, Master.ENDPOINT_NAME) registerWithMaster(masterEndpoint) &#125; catch &#123; case ie: InterruptedException =&gt; // Cancelled case NonFatal(e) =&gt; logWarning(s\"Failed to connect to master $masterAddress\", e) &#125; &#125; &#125;) &#125; &#125; 通过一个名为registerMasterThreadPool的线程池（最大线程数为Worker的个数）来运行run方法中的内容：首先通过setupEndpointRef方法获得其中一个Master的一个引用（RpcEndpointRef），然后执行registerWithMaster(masterEndpoint)方法，刚才得到的Master的引用作为参数传入，下面进入registerWithMaster方法：（注意此处的registerWithMaster方法是有一个RpcEndpointRef作为参数的，和刚开始的那个不一样） 1234567891011121314private def registerWithMaster(masterEndpoint: RpcEndpointRef): Unit = &#123; masterEndpoint.ask[RegisterWorkerResponse](RegisterWorker( workerId, host, port, self, cores, memory, webUi.boundPort, publicAddress)) .onComplete &#123; // This is a very fast action so we can use \"ThreadUtils.sameThread\" case Success(msg) =&gt; Utils.tryLogNonFatalError &#123; handleRegisterResponse(msg) &#125; case Failure(e) =&gt; logError(s\"Cannot register with master: $&#123;masterEndpoint.address&#125;\", e) System.exit(1) &#125;(ThreadUtils.sameThread) &#125; 内部使用masterEndpoint（Master的RpcEndpointRef）的ask方法向Master发送一条RegisterWorker的消息，并使用onComplete方法接受Master的处理结果，下面我们先来看一下消息到达Master端进行怎样的处理： 12345678910111213141516171819202122232425override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = &#123; case RegisterWorker( id, workerHost, workerPort, workerRef, cores, memory, workerUiPort, publicAddress) =&gt; &#123; logInfo(\"Registering worker %s:%d with %d cores, %s RAM\".format( workerHost, workerPort, cores, Utils.megabytesToString(memory))) if (state == RecoveryState.STANDBY) &#123; context.reply(MasterInStandby) &#125; else if (idToWorker.contains(id)) &#123; context.reply(RegisterWorkerFailed(\"Duplicate worker ID\")) &#125; else &#123; val worker = new WorkerInfo(id, workerHost, workerPort, cores, memory, workerRef, workerUiPort, publicAddress) if (registerWorker(worker)) &#123; persistenceEngine.addWorker(worker) context.reply(RegisteredWorker(self, masterWebUiUrl)) schedule() &#125; else &#123; val workerAddress = worker.endpoint.address logWarning(\"Worker registration failed. Attempted to re-register worker at same \" + \"address: \" + workerAddress) context.reply(RegisterWorkerFailed(\"Attempted to re-register worker at same address: \" + workerAddress)) &#125; &#125; &#125; 首先receiveAndReply方法匹配到Worker发过来的RegisterWorker消息，然后执行具体的操作：打了一个日志，判断Master现在的状态，如果是STANDBY就reply一个MasterInStandby的消息，如果idToWorker中已经存在该Worker的ID就回复重复的worker ID的失败信息，如果都不是，将获得的Worker信息用WorkerInfo进行封装，然后执行registerWorker(worker)操作注册该Worker，如果成功就向persistenceEngine中添加该Worker并reply给Worker RegisteredWorker(self, masterWebUiUrl)消息并执行schedule方法，如果注册失败就reply RegisterWorkerFailed消息，下面我们具体看一下Master端是如何注册Worker的，即registerWorker(worker)方法： 123456789101112131415161718192021222324252627private def registerWorker(worker: WorkerInfo): Boolean = &#123; // There may be one or more refs to dead workers on this same node (w/ different ID's), // remove them. workers.filter &#123; w =&gt; (w.host == worker.host &amp;&amp; w.port == worker.port) &amp;&amp; (w.state == WorkerState.DEAD) &#125;.foreach &#123; w =&gt; workers -= w &#125; val workerAddress = worker.endpoint.address if (addressToWorker.contains(workerAddress)) &#123; val oldWorker = addressToWorker(workerAddress) if (oldWorker.state == WorkerState.UNKNOWN) &#123; // A worker registering from UNKNOWN implies that the worker was restarted during recovery. // The old worker must thus be dead, so we will remove it and accept the new worker. removeWorker(oldWorker) &#125; else &#123; logInfo(\"Attempted to re-register worker at same address: \" + workerAddress) return false &#125; &#125; workers += worker idToWorker(worker.id) = worker addressToWorker(workerAddress) = worker true &#125; 首先判断是否有和该Worker的host和port相同且状态为DEAD的Worker，如果有就remove掉，然后获得该Worker的RpcAddress，然后根据RpcAddress判断addressToWorker中是否有相同地址的记录，如果有记录且老的Worker的状态为UNKNOWN就remove掉老的Worker，如果没有记录就打日志并返回false（导致上一步reply：RegisterWorkerFailed）然后分别在workers、idToWorker、addressToWorker中添加该Worker，最后返回true，导致上一步向Worker reply注册成功的消息：context.reply(RegisteredWorker(self, masterWebUiUrl))，并执行schedule()，即向等待的applications分配当前可用的资源（每当新的application加入或者有资源变化时都会调用该方法），这个方法我会用单独的一片文章详细分析，现在我们先来看Worker端是如何进行回复的，回到上面的registerWithMaster方法（有参数的），我们直接看成功后执行的handleRegisterResponse(msg)这个方法： 12345678910111213141516171819202122232425262728293031private def handleRegisterResponse(msg: RegisterWorkerResponse): Unit = synchronized &#123; msg match &#123; case RegisteredWorker(masterRef, masterWebUiUrl) =&gt; logInfo(\"Successfully registered with master \" + masterRef.address.toSparkURL) registered = true changeMaster(masterRef, masterWebUiUrl) forwordMessageScheduler.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; self.send(SendHeartbeat) &#125; &#125;, 0, HEARTBEAT_MILLIS, TimeUnit.MILLISECONDS) if (CLEANUP_ENABLED) &#123; logInfo( s\"Worker cleanup enabled; old application directories will be deleted in: $workDir\") forwordMessageScheduler.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; self.send(WorkDirCleanup) &#125; &#125;, CLEANUP_INTERVAL_MILLIS, CLEANUP_INTERVAL_MILLIS, TimeUnit.MILLISECONDS) &#125; case RegisterWorkerFailed(message) =&gt; if (!registered) &#123; logError(\"Worker registration failed: \" + message) System.exit(1) &#125; case MasterInStandby =&gt; // Ignore. Master not yet ready. &#125; &#125; 依然是模式匹配的方式： 如果接受到的是RegisteredWorker，会执行changeMaster方法，取消最后一次的重试，然后向自己的RpcEnv发送SendHeartBeat消息，使用receive方法接受到该消息后会通过sendToMaster方法向Master发送心跳，最后判断CLEANUP_ENABLED如果开启就向自己的RpcEnv发送WorkDirCleanup消息，接受到消息后将老的application的目录清除 如果接受到的是RegisterWorkerFailed就表明注册失败 changeMaster发送： 123456789private def changeMaster(masterRef: RpcEndpointRef, uiUrl: String) &#123; // activeMasterUrl it's a valid Spark url since we receive it from master. activeMasterUrl = masterRef.address.toSparkURL activeMasterWebUiUrl = uiUrl master = Some(masterRef) connected = true // Cancel any outstanding re-registration attempts because we found a new master cancelLastRegistrationRetry() &#125; cancelLastRegistrationRetry: 12345678private def cancelLastRegistrationRetry(): Unit = &#123; if (registerMasterFutures != null) &#123; registerMasterFutures.foreach(_.cancel(true)) registerMasterFutures = null &#125; registrationRetryTimer.foreach(_.cancel(true)) registrationRetryTimer = None &#125; 如果Worker注册失败同样会通过registrationRetryTimer进行重试： 123456789registrationRetryTimer = Some(forwordMessageScheduler.scheduleAtFixedRate( new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; Option(self).foreach(_.send(ReregisterWithMaster)) &#125; &#125;, INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS, INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS, TimeUnit.SECONDS)) 可以看到向自己发送重新注册的消息：ReregisterWithMaster，receive接收到后会执行reregisterWithMaster()方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586private def reregisterWithMaster(): Unit = &#123; Utils.tryOrExit &#123; //重试次数加1 connectionAttemptCount += 1 if (registered) &#123; //如果已经注册了，就取消重试 cancelLastRegistrationRetry() &#125; else if (connectionAttemptCount &lt;= TOTAL_REGISTRATION_RETRIES) &#123; //判断是否超过最大重试次数 logInfo(s\"Retrying connection to master (attempt # $connectionAttemptCount)\") /** * Re-register with the active master this worker has been communicating with. If there * is none, then it means this worker is still bootstrapping and hasn't established a * connection with a master yet, in which case we should re-register with all masters. * * It is important to re-register only with the active master during failures. Otherwise, * if the worker unconditionally attempts to re-register with all masters, the following * race condition may arise and cause a \"duplicate worker\" error detailed in SPARK-4592: * * (1) Master A fails and Worker attempts to reconnect to all masters * (2) Master B takes over and notifies Worker * (3) Worker responds by registering with Master B * (4) Meanwhile, Worker's previous reconnection attempt reaches Master B, * causing the same Worker to register with Master B twice * * Instead, if we only register with the known active master, we can assume that the * old master must have died because another master has taken over. Note that this is * still not safe if the old master recovers within this interval, but this is a much * less likely scenario. */ master match &#123; case Some(masterRef) =&gt; // registered == false &amp;&amp; master != None means we lost the connection to master, so // masterRef cannot be used and we need to recreate it again. Note: we must not set // master to None due to the above comments. // 这里说的很清楚，如果注册失败了，但是master != None说明我们失去了和master的连接，所以需要重新创建一个masterRef // 先取消原来阻塞的用来等待消息回复的线程 if (registerMasterFutures != null) &#123; registerMasterFutures.foreach(_.cancel(true)) &#125; // 然后创建新的masterRef，然后重新注册 val masterAddress = masterRef.address registerMasterFutures = Array(registerMasterThreadPool.submit(new Runnable &#123; override def run(): Unit = &#123; try &#123; logInfo(\"Connecting to master \" + masterAddress + \"...\") val masterEndpoint = rpcEnv.setupEndpointRef(Master.SYSTEM_NAME, masterAddress, Master.ENDPOINT_NAME) registerWithMaster(masterEndpoint) &#125; catch &#123; case ie: InterruptedException =&gt; // Cancelled case NonFatal(e) =&gt; logWarning(s\"Failed to connect to master $masterAddress\", e) &#125; &#125; &#125;)) case None =&gt; // 如果没有masterRef，先取消原来阻塞的用来等待消息回复的线程 if (registerMasterFutures != null) &#123; registerMasterFutures.foreach(_.cancel(true)) &#125; // 然后执行最初的注册，即tryRegisterAllMasters // We are retrying the initial registration registerMasterFutures = tryRegisterAllMasters() &#125; // We have exceeded the initial registration retry threshold // All retries from now on should use a higher interval // 如果超过刚开始设置的重试注册次数，取消之前的重试，开启新的注册，并改变重试次数和时间间隔 // 刚开始的重试默认为6次，时间间隔在5到15秒之间，接下来的10次重试时间间隔在30到90秒之间 if (connectionAttemptCount == INITIAL_REGISTRATION_RETRIES) &#123; registrationRetryTimer.foreach(_.cancel(true)) registrationRetryTimer = Some( forwordMessageScheduler.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryLogNonFatalError &#123; self.send(ReregisterWithMaster) &#125; &#125;, PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS, PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS, TimeUnit.SECONDS)) &#125; &#125; else &#123; logError(\"All masters are unresponsive! Giving up.\") System.exit(1) &#125; &#125; &#125; 至此Worker的启动和注册完成，即start-all.sh执行完成。 下面是Worker的初始化部分和onStart方法的源码及注释（重要部分）： 初始化部分： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106private val host = rpcEnv.address.hostprivate val port = rpcEnv.address.portUtils.checkHost(host, \"Expected hostname\")assert (port &gt; 0)// A scheduled executor used to send messages at the specified time.private val forwordMessageScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"worker-forward-message-scheduler\")// A separated thread to clean up the workDir. Used to provide the implicit parameter of `Future`// methods.private val cleanupThreadExecutor = ExecutionContext.fromExecutorService( ThreadUtils.newDaemonSingleThreadExecutor(\"worker-cleanup-thread\"))// For worker and executor IDsprivate def createDateFormat = new SimpleDateFormat(\"yyyyMMddHHmmss\")// 发送心跳的时间间隔：timeout的时间 / 4// Send a heartbeat every (heartbeat timeout) / 4 millisecondsprivate val HEARTBEAT_MILLIS = conf.getLong(\"spark.worker.timeout\", 60) * 1000 / 4// 重试的模型及其次数设置// Model retries to connect to the master, after Hadoop's model.// The first six attempts to reconnect are in shorter intervals (between 5 and 15 seconds)// Afterwards, the next 10 attempts are between 30 and 90 seconds.// A bit of randomness is introduced so that not all of the workers attempt to reconnect at// the same time.private val INITIAL_REGISTRATION_RETRIES = 6private val TOTAL_REGISTRATION_RETRIES = INITIAL_REGISTRATION_RETRIES + 10private val FUZZ_MULTIPLIER_INTERVAL_LOWER_BOUND = 0.500private val REGISTRATION_RETRY_FUZZ_MULTIPLIER = &#123; val randomNumberGenerator = new Random(UUID.randomUUID.getMostSignificantBits) randomNumberGenerator.nextDouble + FUZZ_MULTIPLIER_INTERVAL_LOWER_BOUND&#125;private val INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS = (math.round(10 * REGISTRATION_RETRY_FUZZ_MULTIPLIER))private val PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS = (math.round(60 * REGISTRATION_RETRY_FUZZ_MULTIPLIER))//CLEANUP相关的设置private val CLEANUP_ENABLED = conf.getBoolean(\"spark.worker.cleanup.enabled\", false)// How often worker will clean up old app foldersprivate val CLEANUP_INTERVAL_MILLIS = conf.getLong(\"spark.worker.cleanup.interval\", 60 * 30) * 1000// TTL for app folders/data; after TTL expires it will be cleaned upprivate val APP_DATA_RETENTION_SECONDS = conf.getLong(\"spark.worker.cleanup.appDataTtl\", 7 * 24 * 3600)private val testing: Boolean = sys.props.contains(\"spark.testing\")//对master的引用private var master: Option[RpcEndpointRef] = Noneprivate var activeMasterUrl: String = \"\"private[worker] var activeMasterWebUiUrl : String = \"\"private val workerUri = rpcEnv.uriOf(systemName, rpcEnv.address, endpointName)private var registered = falseprivate var connected = falseprivate val workerId = generateWorkerId()private val sparkHome = if (testing) &#123; assert(sys.props.contains(\"spark.test.home\"), \"spark.test.home is not set!\") new File(sys.props(\"spark.test.home\")) &#125; else &#123; new File(sys.env.get(\"SPARK_HOME\").getOrElse(\".\")) &#125;var workDir: File = nullval finishedExecutors = new LinkedHashMap[String, ExecutorRunner]val drivers = new HashMap[String, DriverRunner]val executors = new HashMap[String, ExecutorRunner]val finishedDrivers = new LinkedHashMap[String, DriverRunner]val appDirectories = new HashMap[String, Seq[String]]val finishedApps = new HashSet[String]val retainedExecutors = conf.getInt(\"spark.worker.ui.retainedExecutors\", WorkerWebUI.DEFAULT_RETAINED_EXECUTORS)val retainedDrivers = conf.getInt(\"spark.worker.ui.retainedDrivers\", WorkerWebUI.DEFAULT_RETAINED_DRIVERS)// The shuffle service is not actually started unless configured.private val shuffleService = new ExternalShuffleService(conf, securityMgr)private val publicAddress = &#123; val envVar = conf.getenv(\"SPARK_PUBLIC_DNS\") if (envVar != null) envVar else host&#125;private var webUi: WorkerWebUI = nullprivate var connectionAttemptCount = 0private val metricsSystem = MetricsSystem.createMetricsSystem(\"worker\", conf, securityMgr)private val workerSource = new WorkerSource(this)private var registerMasterFutures: Array[JFuture[_]] = nullprivate var registrationRetryTimer: Option[JScheduledFuture[_]] = None// 用来和Master注册使用的线程池，默认线程的最大个数为Worker的个数// A thread pool for registering with masters. Because registering with a master is a blocking// action, this thread pool must be able to create \"masterRpcAddresses.size\" threads at the same// time so that we can register with all masters.private val registerMasterThreadPool = ThreadUtils.newDaemonCachedThreadPool( \"worker-register-master-threadpool\", masterRpcAddresses.size // Make sure we can register with all masters at the same time)var coresUsed = 0var memoryUsed = 0 onStart()方法： 123456789101112131415161718192021override def onStart() &#123; assert(!registered) logInfo(\"Starting Spark worker %s:%d with %d cores, %s RAM\".format( host, port, cores, Utils.megabytesToString(memory))) logInfo(s\"Running Spark version $&#123;org.apache.spark.SPARK_VERSION&#125;\") logInfo(\"Spark home: \" + sparkHome) // 创建Work的目录 createWorkDir() // 开启 external shuffle service shuffleService.startIfEnabled() webUi = new WorkerWebUI(this, workDir, webUiPort) webUi.bind() // 向Master注册自己 registerWithMaster() // metrics系统 metricsSystem.registerSource(workerSource) metricsSystem.start() // Attach the worker metrics servlet handler to the web ui after the metrics system is started. metricsSystem.getServletHandlers.foreach(webUi.attachHandler) &#125; 本文简单介绍了Spark的几种部署模式，并详细的分析了start-all.sh所执行源码（Master的启动和注册、Worker的启动和向Master的注册）的具体流程，当然Master的schedule方法并没有详细说明，我们会单独用一篇文章进行详细的分析。 本文参考和拓展阅读： Spark-1.6.3源码 Spark-2.1.0源码 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"RPC","slug":"prc","permalink":"http://www.sun4lower.cn/tags/prc/"},{"name":"spark","slug":"spark","permalink":"http://www.sun4lower.cn/tags/spark/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"},{"name":"master","slug":"master","permalink":"http://www.sun4lower.cn/tags/master/"},{"name":"worker","slug":"worker","permalink":"http://www.sun4lower.cn/tags/worker/"},{"name":"deployment","slug":"deployment","permalink":"http://www.sun4lower.cn/tags/deployment/"},{"name":"standalone","slug":"standalone","permalink":"http://www.sun4lower.cn/tags/standalone/"},{"name":"yarn","slug":"yarn","permalink":"http://www.sun4lower.cn/tags/yarn/"},{"name":"mesos","slug":"mesos","permalink":"http://www.sun4lower.cn/tags/mesos/"}]},{"title":"Spark RPC 到底是个什么鬼？","slug":"SparkRpcBasic","date":"2017-02-22T09:49:24.000Z","updated":"2017-02-24T07:49:21.000Z","comments":true,"path":"2017/02/22/SparkRpcBasic/","link":"","permalink":"http://www.sun4lower.cn/2017/02/22/SparkRpcBasic/","excerpt":"","text":"本文会为大家介绍Spark中的RPC通信机制，详细阐述“Spark RPC到底是个什么鬼？”，闲话少叙，让我们来进入Spark RPC的世界！ Spark RPC三剑客Spark RPC中最为重要的三个抽象（“三剑客”）为：RpcEnv、RpcEndpoint、RpcEndpointRef，这样做的好处有： 对上层的API来说，屏蔽了底层的具体实现，使用方便 可以通过不同的实现来完成指定的功能，方便扩展 促进了底层实现层的良性竞争，Spark 1.6.3中默认使用了Netty作为底层的实现，但Akka的依赖依然存在；而Spark 2.1.0中的底层实现只有Netty，这样用户可以方便的使用不同版本的Akka或者将来某种更好的底层实现 下面我们就结合Netty和“三剑客”来具体分析他们是如何来协同工作的。 Send a message locally我们通过Spark源码中的一个Test（RpcEnvSuite.scala）来分析一下发送本地消息的具体流程，源码如下（对源码做了一些修改）： 12345678910111213141516test(\"send a message locally\") &#123; @volatile var message: String = null val rpcEndpointRef = env.setupEndpoint(\"send-locally\", new RpcEndpoint &#123; override val rpcEnv = env override def receive = &#123; //case msg: String =&gt; message = msg case msg: String =&gt; println(message) //我们直接将接收到的消息打印出来 &#125; &#125;) rpcEndpointRef.send(\"hello\") //下面是原来的代码 //eventually(timeout(5 seconds), interval(10 millis)) &#123; // assert(\"hello\" === message) //&#125;&#125; 为了方便理解，先把流程图贴出来，然后详细进行阐述： 下面我们来详细阐述上例的具体过程： 首先是RpcEndpoint创建并注册的流程：（图中的蓝色线条部分） 1、创建RpcEndpoint，并初始化rpcEnv的引用（RpcEnv已经创建好，底层实际上是实例化了一个NettyRpcEnv，而NettyRpcEnv是通过工厂方法NettyRpcEnvFactory创建的） 2、实例化RpcEndpoint之后需要向RpcEnv注册该RpcEndpoint，底层实现是向NettyRpcEnv进行注册，而实际上是通过调用Dispatcher的registerRpcEndpoint方法向Dispatcher进行注册 3、具体的注册就是向endpoints、endpointRefs、receivers中插入记录：而receivers中插入的信息会被Dispatcher中的线程池中的线程执行：会将记录take出来然后调用Inbox的process方法通过模式匹配的方法进行处理，注册的时候通过匹配到OnStart类型的message，去执行RpcEndpoint的onStart方法（例如Master、Worker注册时，就要执行各自的onStart方法），本例中未做任何操作 4、注册完成后返回RpcEndpointRef，我们通过RpcEndpointRef就可以向其代表的RpcEndpoint发送消息 下面就是通过RpcEndpointRef向其代表的RpcEndpoint发送消息的具体流程：（图中的红色线条部分） 1、2、调用RpcEndpointRef的send方法，底层实现是调用Netty的NettyRpcEndpointRef的send方法，而实际上又是调用的NettyRpcEnv的send方法，发送的消息使用RequestMessage进行封装： 1nettyEnv.send(RequestMessage(nettyEnv.address, this, message)) 3、4、NettyRpcEnv的send方法首先会根据RpcAddress判断是本地还是远程调用，此处是同一个RpcEnv，所以是本地调用，即调用Dispatcher的postOneWayMessage方法 5、postOneWayMessage方法内部调用Dispatcher的postMessage方法 6、postMessage会向具体的RpcEndpoint发送消息，首先通过endpointName从endpoints中获得注册时的EndpointData，如果不为空就执行EndpointData中Inbox的post(message)方法，向Inbox的mesages中插入一条InboxMessage，同时向receivers中插入一条记录，此处将Inbox单独画出来是为了方便大家理解 7、Dispatcher中的线程池会拿出一条线程用来循环receivers中的消息，首先使用take方法获得receivers中的一条记录，然后调用Inbox的process方法来执行这条记录，而process将messages中的一条InboxMessage（第6步中插入的）拿出来进行处理，具体的处理方法就是通过模式匹配的方法，匹配到消息的类型（此处是OneWayMessage），然后来执行RpcEndpoint中对应的receive方法，在此例中我们只打印出这条消息（步骤8） 至此，一个简单的发送本地消息的流程执行完成。 什么，上面的图太复杂了？我也觉得，下面给出一张简洁的图： 我们通过NettyRpcEndpointRef来发出一个消息，消息经过NettyRpcEnv、Dispatcher、Inbox的共同处理最终将消息发送到NettyRpcEndpoint，NettyRpcEndpoint收到消息后进行处理（一般是通过模式匹配的方式进行不同的处理） 如果进一步的进行抽象就得到了我们刚开始所讲的“三剑客”：RpcEnv、RpcEndpoint、RpcEndpointRef RpcEndpointRef发送消息给RpcEnv，RpcEnv查询注册信息将消息路由到指定的RpcEndpoint，RpcEndpoint接收到消息后进行处理（模式匹配的方式） RpcEndpoint的声明周期：constructor -&gt; onStart -&gt; receive* -&gt; onStop 其中receive*包括receive和receiveAndReply 本文我们只是通过一个简单的测试程序分析了Spark Rpc底层的实现，集群中的其它通信（比如Master和Woker的通信）的原理和这个测试类似，只不过具体的发送方式有所不同（包括ask、askWithRetry等），而且远程发消息的时候使用了OutBox和NIO等相关的内容，感兴趣的朋友可以对源码进行详细的阅读，本文不一一说明，目的就是通过简单的测试理解大致流程，不再为“Spark Rpc到底是什么”而纠结，一句话总结：Spark Rpc就是Spark中对分布式消息通信系统的高度抽象。 本文参考和拓展阅读： spark源码 Netty官方网站 Java NIO Tutorial 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"通信机制","slug":"event","permalink":"http://www.sun4lower.cn/tags/event/"},{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"Netty","slug":"netty","permalink":"http://www.sun4lower.cn/tags/netty/"},{"name":"RPC","slug":"prc","permalink":"http://www.sun4lower.cn/tags/prc/"},{"name":"spark","slug":"spark","permalink":"http://www.sun4lower.cn/tags/spark/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"}]},{"title":"浅显易懂之线程模型的演变","slug":"event-threadmodel","date":"2017-02-22T09:49:24.000Z","updated":"2017-02-22T13:36:07.000Z","comments":true,"path":"2017/02/22/event-threadmodel/","link":"","permalink":"http://www.sun4lower.cn/2017/02/22/event-threadmodel/","excerpt":"","text":"本文旨在通过一个实际工作中例子让大家理解线程模型的演变，从而对 Netty 的模型有一个粗略的印象，面向的是想初步了解 Netty 原理的读者，而并不关心具体的执行细节和相关术语的描述。 单线程单线程映射到我们的实际工作中就是由一个人完成所有的工作，如下图所示： A 自己独立的完成全部的Job 多线程多线程映射到我们的工作中就是多个人共同协作完成工作，如下图所示： A、B、C 三人工作完成工作，每个人会分到具体的工作去执行，较之单线程，执行效率提高。 线程池多个 Worker（A、B、C等）已经准备好去完成即将到来的Job，当Job 1过来的时候，会从线程池中选择一个线程也就是Worker（A）来完成这项任务，完成之后A仍然要回到线程池中，等待下一个工作的到来。 Reactor单线程模型Reactor 的单线程模型映射到实际工作中如下图所示： 我们有一个Boss来接收具体的Job，然后Boss将具体接收到的Job分配给已经准备好的Worker，交给具体的Worker来完成最终的Job，但是如果Job非常多的时候，Boss的压力就会越来越大，所以Boss成为了性能的瓶颈，而下面提到的Reactor多线程模型就是为了解决这个问题而产生的。 Reactor多线程模型为了完善Reactor单线程模型，Reactor多线程模型进行了优化： 和Reactor单线程模型相比，Reactor多线程模型增加了一个从Manager pool选择Manager(相当于项目经理)的过程，即通过Manager来帮助Boss分配具体的Job给Worker，例如图中选择了M1，那么M1就会去完成选择Worker的工作，虚线代表选择了M2或者M3。 Netty的具体实现就类似于Reactor的多线程模型，而Spark现在Rpc的底层就是通过Netty来实现的。 至此本文的目的已经达到，如果想要更加详细的了解Netty的具体实现细节和NIO相关的知识可以参考如下文章： Netty系列之Netty线程模型 并发编程网有关Netty的部分 Netty官方网站 Java NIO Tutorial 站内博客未经特殊说明皆为原创，欢迎转载，转载请注明出处、作者，谢谢！","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"通信机制","slug":"bigdata/event","permalink":"http://www.sun4lower.cn/categories/bigdata/event/"}],"tags":[{"name":"通信机制","slug":"event","permalink":"http://www.sun4lower.cn/tags/event/"},{"name":"Netty","slug":"netty","permalink":"http://www.sun4lower.cn/tags/netty/"},{"name":"多线程","slug":"多线程","permalink":"http://www.sun4lower.cn/tags/多线程/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-02-19T04:52:30.000Z","updated":"2017-02-20T03:19:14.000Z","comments":true,"path":"2017/02/19/hello-world/","link":"","permalink":"http://www.sun4lower.cn/2017/02/19/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server 123public static void main(String[] args)&#123; System.out.print(\"hello world\")&#125; 12def sum(a: Int, b: Int): Int = a + bprintln(\"hello world\") More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment hello world","categories":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/categories/bigdata/"},{"name":"spark","slug":"bigdata/spark","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/"},{"name":"spark-core","slug":"bigdata/spark/sparkc","permalink":"http://www.sun4lower.cn/categories/bigdata/spark/sparkc/"}],"tags":[{"name":"大数据","slug":"bigdata","permalink":"http://www.sun4lower.cn/tags/bigdata/"},{"name":"spark-core","slug":"sparkc","permalink":"http://www.sun4lower.cn/tags/sparkc/"},{"name":"性能调优","slug":"perf","permalink":"http://www.sun4lower.cn/tags/perf/"}]}]}